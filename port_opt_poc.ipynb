{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6528bf7d-6352-48a7-9e9d-a40af3edc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "# plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a814ec-5b4c-45f5-9150-949957c778cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '~/deepdow_poc'\n",
    "raw_data = root_path + '/raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9a2bcb-32ae-4984-8feb-a53add39d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will load the dataset for each assets, filter only the adj close column to use as our feature and concat the assets into same dataframe\n",
    "\"\"\"\n",
    "\n",
    "def load_prepare_data(assets_list, suffix, col=['Adj Close']): #, 'Volume']):\n",
    "    assets_dict = {}\n",
    "    temp_list = []\n",
    "    \n",
    "    if suffix != None:\n",
    "        for asset in assets_list:\n",
    "            df_temp = pd.read_csv(raw_data + asset + suffix +'.csv')\n",
    "            df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "            df_temp.set_index('Date', inplace=True)\n",
    "            df_temp = pd.DataFrame(df_temp[col])\n",
    "\n",
    "            assets_dict[asset] = df_temp\n",
    "            temp_list.append(df_temp)\n",
    "    else:\n",
    "        for asset in assets_list:\n",
    "            df_temp = pd.read_csv(raw_data + asset +'.csv')\n",
    "            df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "            df_temp.set_index('Date', inplace=True)\n",
    "            df_temp = pd.DataFrame(df_temp[col])\n",
    "\n",
    "            assets_dict[asset] = df_temp\n",
    "            temp_list.append(df_temp)\n",
    "            \n",
    "    df = pd.concat(temp_list, keys=assets_list, axis=1)\n",
    "    return assets_dict, df\n",
    "\n",
    "\n",
    "ASSETS = ['AAPL', 'TSLA', 'SHOP', 'HLFNX','VGT']\n",
    "_,df_1 = load_prepare_data(ASSETS, '_train')\n",
    "_,df_2 = load_prepare_data(ASSETS, '_test')\n",
    "df = pd.concat([df_1, df_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623c0df0-2291-431e-9ed9-457d125f281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the data of the asset price to daily returns\n",
    "\"\"\"\n",
    "\n",
    "def to_return(data, col, log=True):\n",
    "    df = data.copy()\n",
    "    for asset in df.columns.levels[0]:\n",
    "        if log:\n",
    "            df[asset, 'Adj Close'] = df[asset, 'Adj Close'].pct_change().apply(lambda x: np.log(1+x))\n",
    "        else:\n",
    "            df[asset, 'Adj Close'] = df[asset, 'Adj Close'].pct_change()\n",
    "            \n",
    "    return df.iloc[1:]\n",
    "\n",
    "df_1 = to_return(df_1, 'Adj Close')\n",
    "df_2 = to_return(df_2, 'Adj Close')\n",
    "df = to_return(df, 'Adj Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b53b4f4-7227-4a46-b06a-945bfdbb7294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>SHOP</th>\n",
       "      <th>HLFNX</th>\n",
       "      <th>VGT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>-0.104924</td>\n",
       "      <td>-0.031978</td>\n",
       "      <td>-0.058433</td>\n",
       "      <td>-0.020795</td>\n",
       "      <td>-0.050685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>0.041803</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>0.061771</td>\n",
       "      <td>0.035463</td>\n",
       "      <td>0.042565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.009570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>0.016839</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.021629</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.012405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>-0.007000</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>-0.063956</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>-0.009144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-24</th>\n",
       "      <td>0.007683</td>\n",
       "      <td>0.024150</td>\n",
       "      <td>0.022745</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.006625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>0.035141</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>-0.066163</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.007003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>-0.013404</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.021177</td>\n",
       "      <td>-0.006415</td>\n",
       "      <td>-0.006635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>-0.008563</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      TSLA      SHOP     HLFNX       VGT\n",
       "           Adj Close Adj Close Adj Close Adj Close Adj Close\n",
       "Date                                                        \n",
       "2019-01-03 -0.104924 -0.031978 -0.058433 -0.020795 -0.050685\n",
       "2019-01-04  0.041803  0.056094  0.061771  0.035463  0.042565\n",
       "2019-01-07 -0.002228  0.052935  0.044830  0.006221  0.011111\n",
       "2019-01-08  0.018884  0.001164  0.007246  0.003096  0.009570\n",
       "2019-01-09  0.016839  0.009438  0.021629  0.008209  0.012405\n",
       "...              ...       ...       ...       ...       ...\n",
       "2020-12-23 -0.007000  0.008769 -0.063956  0.002863 -0.009144\n",
       "2020-12-24  0.007683  0.024150  0.022745  0.004635  0.006625\n",
       "2020-12-28  0.035141  0.002897 -0.066163  0.001422  0.007003\n",
       "2020-12-29 -0.013404  0.003459  0.021177 -0.006415 -0.006635\n",
       "2020-12-30 -0.008563  0.042321 -0.007376  0.004637  0.001331\n",
       "\n",
       "[502 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13299300-c31c-48f7-ac16-a9477159637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Establish constants\n",
    "1. Rebalance freq = BMS first business day of month\n",
    "2. Lookback = number of historical data in unit of days that \n",
    "   deep learning use to learn to predict weight for the horizon\n",
    "3. Horizon = days that user hold before rebalance. Corresponds to the Rebalance freq. \n",
    "    20 days = n_business_day in 1 month\n",
    "\"\"\"\n",
    "\n",
    "LOOKBACK = 40\n",
    "HORIZON = 20\n",
    "WINDOW = 250\n",
    "N_SAMPLES = WINDOW - LOOKBACK - HORIZON\n",
    "N_ASSETS = len(df.columns.levels[0])\n",
    "N_FEATURES = LOOKBACK * N_ASSETS\n",
    "\n",
    "\"\"\"\n",
    "first day of the month in the historical data\n",
    "go through each element in list, if month changes, that is the first day of month, assuming time is ordered in the list\n",
    "\"\"\"\n",
    "def get_first_date(datelist):\n",
    "    rebal_dates = []\n",
    "    for index,d in enumerate(df.index):\n",
    "        if index==0:\n",
    "            rebal_dates.append(d)\n",
    "            current_month = d.month\n",
    "        else:\n",
    "            if d.month != current_month:\n",
    "                rebal_dates.append(d)\n",
    "                current_month = d.month\n",
    "            else:\n",
    "                continue\n",
    "    return rebal_dates\n",
    "\n",
    "REBAL_DATES = get_first_date(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a8125c-6159-4e1a-a3e1-5c48ae96acce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns.levels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4faf5ea9-504a-48d3-b1eb-451d51ce7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train and optimize.\n",
    "Training and optimize will be done to 1 rolling window to predict the weight to use for the horizon.\n",
    "Rolling window will start from T(rebalance date) and go backwards\n",
    "\"\"\"\n",
    "\n",
    "# first will define the loss functions\n",
    "import tensorflow as tf\n",
    "\n",
    "def portfolio_returns(weights, y):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    --------\n",
    "    weights = the predicted values output from neural network. (n_samples, n_asset)\n",
    "    y = test data (n_samples, horizon, n_asset) use for loss calculation\n",
    "    \"\"\"\n",
    "    # exp to reverse log transform\n",
    "    y = tf.math.exp(y) - 1\n",
    "    n_samples = tf.shape(y)[0]\n",
    "    horizon = tf.shape(y)[1]\n",
    "    n_assets = tf.shape(y)[2]\n",
    "    \n",
    "    \"\"\" \n",
    "    reshape the weight into (n_samples, horizon, n_asset) dimension\n",
    "    first we have (n_samples, n_assets) weight output from neural network output\n",
    "    we want to repeat the vector horizon amount of time\n",
    "    \"\"\"\n",
    "    weights_ = tf.tile(tf.reshape(weights, [n_samples, 1, n_assets]), (1, horizon, 1))\n",
    "    \n",
    "    \"\"\"\n",
    "    rebalance : now will always be false (same for all time steps in the horizon)\n",
    "        If True, each timestep the weights are adjusted to be equal to be equal to the original ones. Note that\n",
    "        this assumes that we tinker with the portfolio. If False, the portfolio evolves untouched.\n",
    "    \"\"\"\n",
    "    rebalance=False\n",
    "    \n",
    "    if not rebalance:\n",
    "        weights_unscaled = tf.math.cumprod((1 + y),1)[:, :-1, :] * weights_[:, 1:, :]\n",
    "        \n",
    "        output_list = []\n",
    "        output_list.append(weights_[:, 0, :])\n",
    "        \n",
    "        remaining = weights_unscaled / tf.math.reduce_sum(weights_unscaled, 2, keepdims=True)\n",
    "        rows = remaining.get_shape()\n",
    "        for row in range(rows[1]):\n",
    "            output_list.append(remaining[:, row,:])\n",
    "        \n",
    "        new_weights = tf.stack(output_list, axis=1)\n",
    "\n",
    "    out = tf.math.reduce_sum((y * new_weights),-1)\n",
    "    \n",
    "    # shape (n_samples, horizon) representing per timestep portfolio returns\n",
    "    return out\n",
    "\n",
    "def MeanReturns(y, weights):#, rebalance=False):\n",
    "    \"\"\"Negative mean returns\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])#, rebalance=rebalance)\n",
    "    \n",
    "    \"\"\"Tensor of shape `(n_samples,)` representing the per sample negative mean returns.\"\"\"\n",
    "    # neg_mean = tf.math.exp(prets)\n",
    "    mean = tf.math.reduce_mean(prets, 1)\n",
    "    neg_mean = mean * -1\n",
    "    return neg_mean\n",
    "\n",
    "def SharpeRatio(y, weights, rf=0.01, eps=1e-4):\n",
    "    \"\"\"Negative Sharpe ratio\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])#, rebalance=rebalance)\n",
    "    \n",
    "    \"\"\"Tensor of shape `(n_samples,)` representing the per sample negative mean returns.\"\"\"\n",
    "    sharpe = ((tf.math.reduce_mean(prets, 1) - rf)/(tf.math.reduce_std(prets, 1) + eps))\n",
    "    neg_sharpe = sharpe * -1\n",
    "    return neg_sharpe\n",
    "\n",
    "def LargestWeight(y, weights):\n",
    "    \"\"\"Penalize low diversity, or over powering weight in 1 asset, lower = better\"\"\"\n",
    "    return tf.reduce_max(weights, 1)[0]\n",
    "\n",
    "def StandardDeviation(y, weights):\n",
    "    \"\"\"Returns std dev of portfolio return of the horizon, lower = better\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])\n",
    "    return tf.math.reduce_std(prets, 1)\n",
    "\n",
    "def make_my_loss(alpha):\n",
    "    def my_loss(y, weights):\n",
    "        \"\"\"Combination of different loss functions for the final loss calculation\"\"\"\n",
    "        return (alpha[0] * MeanReturns(y, weights)\n",
    "                + alpha[1] * SharpeRatio(y, weights)\n",
    "                + alpha[2] * StandardDeviation(y, weights)\n",
    "               )\n",
    "    return my_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b46bf7b-9893-43e5-81bf-d42420250a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the model archtecture\n",
    "-----------------------------\n",
    "This is a simple feedfoward model that the input is the flattened array of (n_samples, n_features)\n",
    "n_features = n_assets * horizon\n",
    "\n",
    "The output is softmax of dimension n_assets. Softmax --> sum of all weight = 1\n",
    "\"\"\"\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Softmax\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "def build_model(n_assets, input_shape, dropout):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    x = Dropout(dropout)(x)\n",
    "    weights = Dense(n_assets, activation=\"linear\")(x)\n",
    "    outputs = Softmax()(weights)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7921a069-5130-48b4-b2ea-a01332699e20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 15:53:59.691868: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-01 15:53:59.764013: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 3s 598ms/step - loss: -0.6080 - val_loss: -0.9911\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.6143 - val_loss: -0.9933\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -0.6011 - val_loss: -0.9955\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -0.5714 - val_loss: -0.9978\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -0.6165 - val_loss: -1.0000\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 154ms/step - loss: -0.5892 - val_loss: -1.0020\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -0.6028 - val_loss: -1.0042\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 132ms/step - loss: -0.6249 - val_loss: -1.0064\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -0.6208 - val_loss: -1.0087\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 179ms/step - loss: -0.6236 - val_loss: -1.0109\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6061 - val_loss: -1.0132\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5959 - val_loss: -1.0156\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6399 - val_loss: -1.0179\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6208 - val_loss: -1.0202\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.6281 - val_loss: -1.0225\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.6444 - val_loss: -1.0247\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.6309 - val_loss: -1.0270\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.6411 - val_loss: -1.0293\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6312 - val_loss: -1.0318\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6468 - val_loss: -1.0343\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6523 - val_loss: -1.0368\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6638 - val_loss: -1.0393\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6546 - val_loss: -1.0416\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6441 - val_loss: -1.0439\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.6727 - val_loss: -1.0461\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.6663 - val_loss: -1.0482\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6697 - val_loss: -1.0500\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.6814 - val_loss: -1.0518\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.6574 - val_loss: -1.0536\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.6596 - val_loss: -1.0553\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 156ms/step - loss: -0.6852 - val_loss: -1.0570\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.6909 - val_loss: -1.0587\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.6920 - val_loss: -1.0603\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -0.6950 - val_loss: -1.0619\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.6984 - val_loss: -1.0632\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.6914 - val_loss: -1.0644\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.6845 - val_loss: -1.0655\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6906 - val_loss: -1.0667\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.7214 - val_loss: -1.0677\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6954 - val_loss: -1.0685\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.7195 - val_loss: -1.0693\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.6901 - val_loss: -1.0699\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7223 - val_loss: -1.0704\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.7223 - val_loss: -1.0708\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.7133 - val_loss: -1.0712\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.7381 - val_loss: -1.0713\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.7447 - val_loss: -1.0712\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.7282 - val_loss: -1.0708\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.7500 - val_loss: -1.0704\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.7574 - val_loss: -1.0701\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.7544 - val_loss: -1.0697\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.7484 - val_loss: -1.0689\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.7659 - val_loss: -1.0680\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.7448 - val_loss: -1.0669\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7811 - val_loss: -1.0657\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.7529 - val_loss: -1.0647\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.7877 - val_loss: -1.0634\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.7915 - val_loss: -1.0619\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.7543 - val_loss: -1.0604\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.7852 - val_loss: -1.0590\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.7846 - val_loss: -1.0574\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7970 - val_loss: -1.0557\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.7986 - val_loss: -1.0539\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.7590 - val_loss: -1.0522\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.8048 - val_loss: -1.0504\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.8110 - val_loss: -1.0486\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.8023 - val_loss: -1.0467\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 156ms/step - loss: -0.8331 - val_loss: -1.0449\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.8435 - val_loss: -1.0429\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.8181 - val_loss: -1.0408\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.8265 - val_loss: -1.0387\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.8320 - val_loss: -1.0365\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.8492 - val_loss: -1.0343\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.8673 - val_loss: -1.0321\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.8445 - val_loss: -1.0298\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.8398 - val_loss: -1.0272\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.8527 - val_loss: -1.0246\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.8574 - val_loss: -1.0219\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.8595 - val_loss: -1.0193\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.8542 - val_loss: -1.0169\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -0.8502 - val_loss: -1.0145\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.8568 - val_loss: -1.0120\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.8471 - val_loss: -1.0096\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 147ms/step - loss: -0.8693 - val_loss: -1.0072\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.8734 - val_loss: -1.0047\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.8800 - val_loss: -1.0023\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.8748 - val_loss: -1.0000\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.8742 - val_loss: -0.9977\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.8709 - val_loss: -0.9954\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8788 - val_loss: -0.9931\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.8587 - val_loss: -0.9907\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.8777 - val_loss: -0.9883\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.8795 - val_loss: -0.9860\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.8572 - val_loss: -0.9835\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.8828 - val_loss: -0.9812\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00095: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 518ms/step - loss: -0.6277 - val_loss: -0.6993\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.6248 - val_loss: -0.7006\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.6530 - val_loss: -0.7018\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6387 - val_loss: -0.7030\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6692 - val_loss: -0.7042\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6324 - val_loss: -0.7053\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.6483 - val_loss: -0.7064\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.6634 - val_loss: -0.7074\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6459 - val_loss: -0.7084\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6525 - val_loss: -0.7092\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.6348 - val_loss: -0.7100\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6724 - val_loss: -0.7108\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6535 - val_loss: -0.7115\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6777 - val_loss: -0.7122\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.6834 - val_loss: -0.7130\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.6459 - val_loss: -0.7138\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6679 - val_loss: -0.7145\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.6752 - val_loss: -0.7153\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6951 - val_loss: -0.7160\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.6758 - val_loss: -0.7168\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6786 - val_loss: -0.7175\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.6919 - val_loss: -0.7183\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.7138 - val_loss: -0.7192\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6990 - val_loss: -0.7202\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.7124 - val_loss: -0.7212\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6906 - val_loss: -0.7223\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.7072 - val_loss: -0.7234\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7081 - val_loss: -0.7245\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.6946 - val_loss: -0.7256\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.7248 - val_loss: -0.7267\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.7378 - val_loss: -0.7279\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -0.7220 - val_loss: -0.7292\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.7411 - val_loss: -0.7306\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -0.7169 - val_loss: -0.7321\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.7187 - val_loss: -0.7336\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.7403 - val_loss: -0.7352\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7298 - val_loss: -0.7368\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.7295 - val_loss: -0.7383\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.7484 - val_loss: -0.7398\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.7846 - val_loss: -0.7415\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.7803 - val_loss: -0.7432\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.7450 - val_loss: -0.7448\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7440 - val_loss: -0.7464\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.7362 - val_loss: -0.7478\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.7678 - val_loss: -0.7492\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.7649 - val_loss: -0.7506\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.7645 - val_loss: -0.7519\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.7675 - val_loss: -0.7532\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.8065 - val_loss: -0.7544\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.7839 - val_loss: -0.7555\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.7665 - val_loss: -0.7565\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.7534 - val_loss: -0.7574\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.7620 - val_loss: -0.7582\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.7842 - val_loss: -0.7588\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.7746 - val_loss: -0.7594\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.8002 - val_loss: -0.7599\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.7891 - val_loss: -0.7602\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.8067 - val_loss: -0.7605\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7930 - val_loss: -0.7607\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.8252 - val_loss: -0.7608\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.7871 - val_loss: -0.7608\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.8178 - val_loss: -0.7607\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.8083 - val_loss: -0.7605\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.8431 - val_loss: -0.7603\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.8415 - val_loss: -0.7600\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.8265 - val_loss: -0.7595\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.8509 - val_loss: -0.7591\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.8258 - val_loss: -0.7585\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.7823 - val_loss: -0.7579\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.7808 - val_loss: -0.7573\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.8058 - val_loss: -0.7564\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -0.8142 - val_loss: -0.7555\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: -0.8385 - val_loss: -0.7545\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.8204 - val_loss: -0.7534\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.8249 - val_loss: -0.7522\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -0.8090 - val_loss: -0.7508\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -0.8250 - val_loss: -0.7495\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.8449 - val_loss: -0.7482\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.8298 - val_loss: -0.7468\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.8570 - val_loss: -0.7453\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.8468 - val_loss: -0.7437\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.8370 - val_loss: -0.7419\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.8431 - val_loss: -0.7403\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.8469 - val_loss: -0.7389\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.8218 - val_loss: -0.7376\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.8607 - val_loss: -0.7365\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.8537 - val_loss: -0.7356\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.8436 - val_loss: -0.7348\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.8267 - val_loss: -0.7339\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.8337 - val_loss: -0.7328\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.8485 - val_loss: -0.7315\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8902 - val_loss: -0.7302\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.8629 - val_loss: -0.7290\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.8564 - val_loss: -0.7277\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.8533 - val_loss: -0.7267\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.8726 - val_loss: -0.7257\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -0.8486 - val_loss: -0.7249\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.8583 - val_loss: -0.7241\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.8360 - val_loss: -0.7233\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.8759 - val_loss: -0.7225\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.8604 - val_loss: -0.7216\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.8684 - val_loss: -0.7205\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.8374 - val_loss: -0.7196\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.8360 - val_loss: -0.7188\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.8524 - val_loss: -0.7180\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.8248 - val_loss: -0.7173\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.8613 - val_loss: -0.7167\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.8592 - val_loss: -0.7163\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00108: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 503ms/step - loss: -0.9993 - val_loss: -0.9535\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.9282 - val_loss: -0.9630\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.9610 - val_loss: -0.9722\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.0567 - val_loss: -0.9810\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.0027 - val_loss: -0.9896\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.0069 - val_loss: -0.9982\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.0434 - val_loss: -1.0065\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 153ms/step - loss: -0.9690 - val_loss: -1.0141\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.0286 - val_loss: -1.0210\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.0356 - val_loss: -1.0280\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.9933 - val_loss: -1.0352\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.0614 - val_loss: -1.0422\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.0550 - val_loss: -1.0491\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.0864 - val_loss: -1.0555\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.0157 - val_loss: -1.0614\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.1045 - val_loss: -1.0674\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.1185 - val_loss: -1.0732\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.0805 - val_loss: -1.0790\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.1596 - val_loss: -1.0845\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.1132 - val_loss: -1.0897\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.0682 - val_loss: -1.0947\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.1392 - val_loss: -1.0995\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.1105 - val_loss: -1.1039\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.0893 - val_loss: -1.1079\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.1082 - val_loss: -1.1119\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.0856 - val_loss: -1.1160\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.1309 - val_loss: -1.1200\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.1230 - val_loss: -1.1239\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.1456 - val_loss: -1.1275\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.1612 - val_loss: -1.1308\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -1.1292 - val_loss: -1.1343\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.1540 - val_loss: -1.1377\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.1494 - val_loss: -1.1409\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.1916 - val_loss: -1.1436\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.1079 - val_loss: -1.1458\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.1352 - val_loss: -1.1480\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.1942 - val_loss: -1.1502\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.1952 - val_loss: -1.1523\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.1466 - val_loss: -1.1541\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.1885 - val_loss: -1.1558\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -1.1691 - val_loss: -1.1575\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.2074 - val_loss: -1.1592\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2089 - val_loss: -1.1604\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.2080 - val_loss: -1.1615\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.1535 - val_loss: -1.1622\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.1978 - val_loss: -1.1627\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.2372 - val_loss: -1.1628\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.2101 - val_loss: -1.1630\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2087 - val_loss: -1.1635\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.2213 - val_loss: -1.1640\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.2229 - val_loss: -1.1646\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.2385 - val_loss: -1.1652\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.2032 - val_loss: -1.1657\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.2033 - val_loss: -1.1659\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.2360 - val_loss: -1.1659\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.1919 - val_loss: -1.1660\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.2491 - val_loss: -1.1659\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.2535 - val_loss: -1.1656\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.2451 - val_loss: -1.1653\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.2018 - val_loss: -1.1651\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.2530 - val_loss: -1.1647\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2345 - val_loss: -1.1643\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.2410 - val_loss: -1.1639\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.2014 - val_loss: -1.1634\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2434 - val_loss: -1.1629\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.1921 - val_loss: -1.1624\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2332 - val_loss: -1.1621\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.1872 - val_loss: -1.1622\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.2583 - val_loss: -1.1623\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.2316 - val_loss: -1.1623\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2542 - val_loss: -1.1620\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.2369 - val_loss: -1.1615\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.2234 - val_loss: -1.1610\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.2560 - val_loss: -1.1604\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 150ms/step - loss: -1.2025 - val_loss: -1.1599\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.2612 - val_loss: -1.1594\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.2615 - val_loss: -1.1591\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2527 - val_loss: -1.1590\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.2689 - val_loss: -1.1588\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.2698 - val_loss: -1.1585\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.2653 - val_loss: -1.1582\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.2743 - val_loss: -1.1577\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.2635 - val_loss: -1.1568\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.2696 - val_loss: -1.1557\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.2892 - val_loss: -1.1547\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.2396 - val_loss: -1.1537\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.2929 - val_loss: -1.1527\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.3012 - val_loss: -1.1513\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 142ms/step - loss: -1.2552 - val_loss: -1.1499\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.2778 - val_loss: -1.1486\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.2422 - val_loss: -1.1473\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2279 - val_loss: -1.1459\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 139ms/step - loss: -1.2613 - val_loss: -1.1443\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.2791 - val_loss: -1.1427\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.2880 - val_loss: -1.1415\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.2270 - val_loss: -1.1403\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2636 - val_loss: -1.1392\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.3005 - val_loss: -1.1383\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.2911 - val_loss: -1.1376\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.3020 - val_loss: -1.1370\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3060 - val_loss: -1.1362\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2774 - val_loss: -1.1356\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00102: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_3 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 482ms/step - loss: -2.5183 - val_loss: -2.0499\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 136ms/step - loss: -2.6053 - val_loss: -2.0673\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.5419 - val_loss: -2.0838\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.6983 - val_loss: -2.0995\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.7358 - val_loss: -2.1144\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.7852 - val_loss: -2.1291\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -2.9218 - val_loss: -2.1433\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.5985 - val_loss: -2.1580\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.7608 - val_loss: -2.1722\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.8060 - val_loss: -2.1866\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.7873 - val_loss: -2.2010\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.8821 - val_loss: -2.2156\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.9564 - val_loss: -2.2303\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.8773 - val_loss: -2.2447\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.9414 - val_loss: -2.2591\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.9797 - val_loss: -2.2732\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.0173 - val_loss: -2.2875\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -2.9611 - val_loss: -2.3021\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.9880 - val_loss: -2.3163\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -3.0453 - val_loss: -2.3303\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -2.9576 - val_loss: -2.3441\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.0042 - val_loss: -2.3578\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -3.0965 - val_loss: -2.3714\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -3.0083 - val_loss: -2.3847\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.9469 - val_loss: -2.3982\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.0279 - val_loss: -2.4118\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -3.1307 - val_loss: -2.4252\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.3104 - val_loss: -2.4386\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.1634 - val_loss: -2.4518\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -3.1481 - val_loss: -2.4647\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -3.0702 - val_loss: -2.4773\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.1861 - val_loss: -2.4895\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.1375 - val_loss: -2.5013\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.3032 - val_loss: -2.5131\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.3349 - val_loss: -2.5247\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.2979 - val_loss: -2.5360\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.4159 - val_loss: -2.5473\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.3734 - val_loss: -2.5586\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -3.4268 - val_loss: -2.5699\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.3435 - val_loss: -2.5809\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -3.4931 - val_loss: -2.5914\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.4057 - val_loss: -2.6015\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.4339 - val_loss: -2.6111\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -3.6824 - val_loss: -2.6207\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -3.4571 - val_loss: -2.6302\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -3.4083 - val_loss: -2.6394\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -3.4331 - val_loss: -2.6484\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6460 - val_loss: -2.6573\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.4241 - val_loss: -2.6663\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.4988 - val_loss: -2.6753\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.6127 - val_loss: -2.6844\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.4997 - val_loss: -2.6932\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.5398 - val_loss: -2.7020\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -3.6063 - val_loss: -2.7109\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.7310 - val_loss: -2.7196\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.8792 - val_loss: -2.7282\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -3.8604 - val_loss: -2.7368\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.7453 - val_loss: -2.7451\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.7595 - val_loss: -2.7533\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.7024 - val_loss: -2.7614\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.9741 - val_loss: -2.7694\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -3.7533 - val_loss: -2.7774\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.8538 - val_loss: -2.7855\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.9030 - val_loss: -2.7938\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.8463 - val_loss: -2.8020\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 177ms/step - loss: -3.7662 - val_loss: -2.8102\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.7640 - val_loss: -2.8184\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.7593 - val_loss: -2.8265\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.9005 - val_loss: -2.8344\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.8265 - val_loss: -2.8421\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.8317 - val_loss: -2.8497\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.9432 - val_loss: -2.8572\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -3.9975 - val_loss: -2.8648\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.9933 - val_loss: -2.8725\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.9368 - val_loss: -2.8802\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.9901 - val_loss: -2.8878\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.8471 - val_loss: -2.8953\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -4.0456 - val_loss: -2.9028\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -3.9610 - val_loss: -2.9102\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -4.0547 - val_loss: -2.9173\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -4.1076 - val_loss: -2.9243\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -4.0211 - val_loss: -2.9313\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.0782 - val_loss: -2.9383\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -4.0914 - val_loss: -2.9451\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -4.0461 - val_loss: -2.9520\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -4.1028 - val_loss: -2.9587\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -4.0763 - val_loss: -2.9652\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.1095 - val_loss: -2.9716\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.2387 - val_loss: -2.9778\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.1628 - val_loss: -2.9838\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.2167 - val_loss: -2.9898\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.2279 - val_loss: -2.9956\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -4.1234 - val_loss: -3.0011\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 136ms/step - loss: -4.1599 - val_loss: -3.0064\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -4.2004 - val_loss: -3.0116\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.1845 - val_loss: -3.0167\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.2822 - val_loss: -3.0217\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.2474 - val_loss: -3.0263\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.1975 - val_loss: -3.0308\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.2632 - val_loss: -3.0353\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.2170 - val_loss: -3.0396\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.2338 - val_loss: -3.0438\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.3071 - val_loss: -3.0478\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -4.3382 - val_loss: -3.0516\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -4.3914 - val_loss: -3.0551\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.2773 - val_loss: -3.0583\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.2464 - val_loss: -3.0615\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.2495 - val_loss: -3.0646\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -4.2964 - val_loss: -3.0677\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.3777 - val_loss: -3.0707\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.4653 - val_loss: -3.0736\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.2850 - val_loss: -3.0764\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.4046 - val_loss: -3.0791\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -4.4047 - val_loss: -3.0817\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.4666 - val_loss: -3.0840\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.3130 - val_loss: -3.0863\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -4.3762 - val_loss: -3.0884\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.4438 - val_loss: -3.0904\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.3802 - val_loss: -3.0923\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.4509 - val_loss: -3.0941\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.3407 - val_loss: -3.0959\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: -4.4956 - val_loss: -3.0975\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.3959 - val_loss: -3.0991\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -4.4088 - val_loss: -3.1006\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.4862 - val_loss: -3.1020\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -4.4676 - val_loss: -3.1033\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.4116 - val_loss: -3.1046\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.3982 - val_loss: -3.1058\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.4149 - val_loss: -3.1070\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.4843 - val_loss: -3.1082\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -4.4114 - val_loss: -3.1093\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.5031 - val_loss: -3.1103\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -4.4558 - val_loss: -3.1113\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.5748 - val_loss: -3.1122\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -4.4568 - val_loss: -3.1131\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.4835 - val_loss: -3.1139\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -4.4889 - val_loss: -3.1146\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -4.5144 - val_loss: -3.1154\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.5136 - val_loss: -3.1161\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -4.5043 - val_loss: -3.1168\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -4.4959 - val_loss: -3.1175\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.4751 - val_loss: -3.1181\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.5036 - val_loss: -3.1186\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -4.5128 - val_loss: -3.1192\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.5485 - val_loss: -3.1197\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -4.5791 - val_loss: -3.1201\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -4.5513 - val_loss: -3.1206\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.4991 - val_loss: -3.1211\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -4.5622 - val_loss: -3.1215\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.5202 - val_loss: -3.1219\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.5217 - val_loss: -3.1222\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.5295 - val_loss: -3.1226\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.5351 - val_loss: -3.1229\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.5371 - val_loss: -3.1232\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -4.5788 - val_loss: -3.1236\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -4.5767 - val_loss: -3.1239\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -4.5746 - val_loss: -3.1242\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.5624 - val_loss: -3.1245\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.5456 - val_loss: -3.1247\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -4.6150 - val_loss: -3.1250\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -4.57 - 0s 98ms/step - loss: -4.5214 - val_loss: -3.1253\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.5560 - val_loss: -3.1255\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.5955 - val_loss: -3.1258\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.5987 - val_loss: -3.1260\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.5645 - val_loss: -3.1262\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.6016 - val_loss: -3.1264\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.5964 - val_loss: -3.1266\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.6040 - val_loss: -3.1268\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.5805 - val_loss: -3.1269\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.6352 - val_loss: -3.1271\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.5337 - val_loss: -3.1272\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.4712 - val_loss: -3.1274\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -4.5588 - val_loss: -3.1275\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.5897 - val_loss: -3.1276\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -4.5959 - val_loss: -3.1278\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.6069 - val_loss: -3.1279\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.6293 - val_loss: -3.1280\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.6168 - val_loss: -3.1281\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -4.5815 - val_loss: -3.1282\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -4.6434 - val_loss: -3.1283\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -4.5844 - val_loss: -3.1284\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.6259 - val_loss: -3.1285\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -4.6204 - val_loss: -3.1286\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.5961 - val_loss: -3.1287\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.6121 - val_loss: -3.1288\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.5975 - val_loss: -3.1289\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.5372 - val_loss: -3.1289\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6073 - val_loss: -3.1290\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.6268 - val_loss: -3.1291\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -4.6135 - val_loss: -3.1292\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.5891 - val_loss: -3.1292\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.5998 - val_loss: -3.1293\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.6084 - val_loss: -3.1294\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.6269 - val_loss: -3.1295\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.6089 - val_loss: -3.1295\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.6187 - val_loss: -3.1296\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.6131 - val_loss: -3.1296\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -4.5530 - val_loss: -3.1297\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -4.6058 - val_loss: -3.1298\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.6339 - val_loss: -3.1298\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -4.5564 - val_loss: -3.1299\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.6477 - val_loss: -3.1299\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.6097 - val_loss: -3.1300\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -4.6425 - val_loss: -3.1300\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -4.6062 - val_loss: -3.1300\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.5737 - val_loss: -3.1301\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -4.6283 - val_loss: -3.1301\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -4.6487 - val_loss: -3.1301\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -4.6458 - val_loss: -3.1302\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.5985 - val_loss: -3.1302\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6098 - val_loss: -3.1302\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.6316 - val_loss: -3.1303\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.6466 - val_loss: -3.1303\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -4.6444 - val_loss: -3.1303\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.6466 - val_loss: -3.1303\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -4.6501 - val_loss: -3.1304\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -4.6360 - val_loss: -3.1304\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -4.6456 - val_loss: -3.1304\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -4.6308 - val_loss: -3.1305\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.6203 - val_loss: -3.1305\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6324 - val_loss: -3.1305\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6406 - val_loss: -3.1305\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.6359 - val_loss: -3.1305\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.6492 - val_loss: -3.1305\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.6042 - val_loss: -3.1306\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.6575 - val_loss: -3.1306\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6604 - val_loss: -3.1306\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.6413 - val_loss: -3.1306\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.6523 - val_loss: -3.1306\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.6289 - val_loss: -3.1306\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.6555 - val_loss: -3.1307\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.6402 - val_loss: -3.1307\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.6525 - val_loss: -3.1307\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -4.6721 - val_loss: -3.1307\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.6472 - val_loss: -3.1307\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -4.6456 - val_loss: -3.1307\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -4.6431 - val_loss: -3.1307\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.6404 - val_loss: -3.1308\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.6627 - val_loss: -3.1308\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6329 - val_loss: -3.1308\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.6377 - val_loss: -3.1308\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -4.6443 - val_loss: -3.1308\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.6539 - val_loss: -3.1308\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.6463 - val_loss: -3.1308\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -4.6413 - val_loss: -3.1308\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.6493 - val_loss: -3.1308\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6388 - val_loss: -3.1308\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -4.6320 - val_loss: -3.1308\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -4.6052 - val_loss: -3.1308\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6500 - val_loss: -3.1308\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -4.6278 - val_loss: -3.1308\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.6513 - val_loss: -3.1308\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6504 - val_loss: -3.1308\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.6561 - val_loss: -3.1308\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6433 - val_loss: -3.1308\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -4.6545 - val_loss: -3.1309\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -4.6437 - val_loss: -3.1309\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.6573 - val_loss: -3.1309\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6224 - val_loss: -3.1309\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -4.6390 - val_loss: -3.1309\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.6410 - val_loss: -3.1309\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -4.6487 - val_loss: -3.1309\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.6319 - val_loss: -3.1309\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.6520 - val_loss: -3.1309\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -4.6502 - val_loss: -3.1309\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.6550 - val_loss: -3.1309\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.6336 - val_loss: -3.1309\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -4.6424 - val_loss: -3.1309\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.6487 - val_loss: -3.1309\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -4.6434 - val_loss: -3.1309\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6562 - val_loss: -3.1309\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.6434 - val_loss: -3.1309\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -4.6524 - val_loss: -3.1309\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.6248 - val_loss: -3.1309\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -4.6459 - val_loss: -3.1309\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.6380 - val_loss: -3.1309\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -4.6419 - val_loss: -3.1309\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00277: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 478ms/step - loss: -2.7207 - val_loss: -2.1025\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.7103 - val_loss: -2.1098\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.6658 - val_loss: -2.1167\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -2.7581 - val_loss: -2.1234\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.7797 - val_loss: -2.1297\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.7666 - val_loss: -2.1357\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -2.7176 - val_loss: -2.1414\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.8684 - val_loss: -2.1468\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.9198 - val_loss: -2.1523\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.7725 - val_loss: -2.1575\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.8745 - val_loss: -2.1625\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.9232 - val_loss: -2.1675\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.9965 - val_loss: -2.1723\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -2.9480 - val_loss: -2.1770\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.8968 - val_loss: -2.1814\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -3.0397 - val_loss: -2.1858\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.9739 - val_loss: -2.1901\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.0570 - val_loss: -2.1944\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.9981 - val_loss: -2.1986\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -2.9891 - val_loss: -2.2027\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.1076 - val_loss: -2.2068\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.9936 - val_loss: -2.2106\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -3.0923 - val_loss: -2.2142\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.0349 - val_loss: -2.2177\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.1291 - val_loss: -2.2211\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.2117 - val_loss: -2.2244\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.1507 - val_loss: -2.2276\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.1704 - val_loss: -2.2308\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.2039 - val_loss: -2.2339\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -3.1580 - val_loss: -2.2368\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.1793 - val_loss: -2.2397\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.1743 - val_loss: -2.2424\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.2085 - val_loss: -2.2451\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.2595 - val_loss: -2.2477\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -3.1603 - val_loss: -2.2501\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -3.2243 - val_loss: -2.2523\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -3.1734 - val_loss: -2.2546\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.1759 - val_loss: -2.2567\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.1653 - val_loss: -2.2589\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.1684 - val_loss: -2.2611\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.3093 - val_loss: -2.2631\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.2784 - val_loss: -2.2650\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -3.3065 - val_loss: -2.2668\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.2619 - val_loss: -2.2684\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -3.3721 - val_loss: -2.2700\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 150ms/step - loss: -3.3573 - val_loss: -2.2714\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 154ms/step - loss: -3.3233 - val_loss: -2.2729\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.3435 - val_loss: -2.2742\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -3.3866 - val_loss: -2.2755\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -3.3417 - val_loss: -2.2768\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.3343 - val_loss: -2.2780\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -3.3808 - val_loss: -2.2791\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.3059 - val_loss: -2.2801\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.3459 - val_loss: -2.2812\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.4071 - val_loss: -2.2821\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.4236 - val_loss: -2.2831\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.3946 - val_loss: -2.2840\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.3954 - val_loss: -2.2849\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.3920 - val_loss: -2.2857\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.4392 - val_loss: -2.2865\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -3.4448 - val_loss: -2.2873\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.4260 - val_loss: -2.2880\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.3932 - val_loss: -2.2888\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -3.4721 - val_loss: -2.2895\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.3641 - val_loss: -2.2901\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.4553 - val_loss: -2.2907\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.4438 - val_loss: -2.2912\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.5205 - val_loss: -2.2918\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.5104 - val_loss: -2.2923\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.4516 - val_loss: -2.2927\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.5054 - val_loss: -2.2931\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -3.4353 - val_loss: -2.2935\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.4384 - val_loss: -2.2938\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.4622 - val_loss: -2.2941\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 148ms/step - loss: -3.4926 - val_loss: -2.2944\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.5307 - val_loss: -2.2947\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.5326 - val_loss: -2.2950\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.5214 - val_loss: -2.2953\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -3.5351 - val_loss: -2.2955\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -3.5046 - val_loss: -2.2958\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -3.5088 - val_loss: -2.2960\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -3.5218 - val_loss: -2.2961\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.4506 - val_loss: -2.2963\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.5340 - val_loss: -2.2964\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.5740 - val_loss: -2.2965\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -3.5798 - val_loss: -2.2967\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.5191 - val_loss: -2.2968\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.4405 - val_loss: -2.2969\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -3.5258 - val_loss: -2.2970\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -3.5329 - val_loss: -2.2971\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.5758 - val_loss: -2.2972\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.5526 - val_loss: -2.2973\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.5912 - val_loss: -2.2973\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -3.5290 - val_loss: -2.2974\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -3.5443 - val_loss: -2.2975\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.5218 - val_loss: -2.2976\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.5496 - val_loss: -2.2977\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -3.5644 - val_loss: -2.2978\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.5694 - val_loss: -2.2978\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.5776 - val_loss: -2.2979\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -3.5657 - val_loss: -2.2980\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -3.5372 - val_loss: -2.2981\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.5793 - val_loss: -2.2982\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -3.6066 - val_loss: -2.2983\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -3.5159 - val_loss: -2.2983\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -3.5447 - val_loss: -2.2984\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.6123 - val_loss: -2.2985\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.5844 - val_loss: -2.2985\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.5831 - val_loss: -2.2986\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.5541 - val_loss: -2.2987\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.5957 - val_loss: -2.2987\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -3.6074 - val_loss: -2.2988\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -3.6078 - val_loss: -2.2989\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -3.5960 - val_loss: -2.2989\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -3.5817 - val_loss: -2.2990\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -3.6015 - val_loss: -2.2990\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.5811 - val_loss: -2.2991\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.6118 - val_loss: -2.2991\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.5819 - val_loss: -2.2991\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -3.6276 - val_loss: -2.2992\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.6086 - val_loss: -2.2992\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -3.6278 - val_loss: -2.2992\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.6148 - val_loss: -2.2993\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -3.6284 - val_loss: -2.2993\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.5851 - val_loss: -2.2993\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.6226 - val_loss: -2.2994\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.5932 - val_loss: -2.2994\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -3.6132 - val_loss: -2.2994\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -3.5959 - val_loss: -2.2994\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.5787 - val_loss: -2.2994\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.6035 - val_loss: -2.2994\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -3.6471 - val_loss: -2.2995\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -3.5997 - val_loss: -2.2995\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.5777 - val_loss: -2.2995\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6213 - val_loss: -2.2995\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.5773 - val_loss: -2.2995\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6039 - val_loss: -2.2995\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -3.5987 - val_loss: -2.2995\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.6213 - val_loss: -2.2995\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.6068 - val_loss: -2.2995\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.6202 - val_loss: -2.2995\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.6020 - val_loss: -2.2995\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -3.6375 - val_loss: -2.2995\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.6344 - val_loss: -2.2995\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6175 - val_loss: -2.2995\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6200 - val_loss: -2.2995\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -3.6239 - val_loss: -2.2995\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.5836 - val_loss: -2.2995\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -3.6304 - val_loss: -2.2995\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.6148 - val_loss: -2.2995\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6292 - val_loss: -2.2995\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -3.6318 - val_loss: -2.2995\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -3.6405 - val_loss: -2.2995\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -3.6333 - val_loss: -2.2995\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -3.6379 - val_loss: -2.2995\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -3.5855 - val_loss: -2.2995\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -3.6052 - val_loss: -2.2995\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6143 - val_loss: -2.2995\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.6260 - val_loss: -2.2995\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00159: early stopping\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f74d8677440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_5 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 563ms/step - loss: -1.0610 - val_loss: -1.6841\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.0845 - val_loss: -1.6821\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.1281 - val_loss: -1.6804\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.0832 - val_loss: -1.6788\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.1516 - val_loss: -1.6773\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.0785 - val_loss: -1.6757\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.1241 - val_loss: -1.6743\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.1922 - val_loss: -1.6730\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.1107 - val_loss: -1.6716\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.1753 - val_loss: -1.6702\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.2076 - val_loss: -1.6689\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.1797 - val_loss: -1.6676\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.2097 - val_loss: -1.6667\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.1980 - val_loss: -1.6656\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.2122 - val_loss: -1.6649\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.2149 - val_loss: -1.6648\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.2554 - val_loss: -1.6649\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.2410 - val_loss: -1.6646\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.2051 - val_loss: -1.6642\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.2279 - val_loss: -1.6637\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.2040 - val_loss: -1.6634\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.2194 - val_loss: -1.6638\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.2552 - val_loss: -1.6648\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.2479 - val_loss: -1.6655\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.2487 - val_loss: -1.6663\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.2364 - val_loss: -1.6672\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.3310 - val_loss: -1.6687\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.2886 - val_loss: -1.6705\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.2444 - val_loss: -1.6728\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.3108 - val_loss: -1.6756\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.2559 - val_loss: -1.6779\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.2865 - val_loss: -1.6803\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.3213 - val_loss: -1.6829\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 165ms/step - loss: -1.3427 - val_loss: -1.6857\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 168ms/step - loss: -1.3180 - val_loss: -1.6893\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3212 - val_loss: -1.6930\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.3298 - val_loss: -1.6971\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.3542 - val_loss: -1.7013\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3752 - val_loss: -1.7056\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.3377 - val_loss: -1.7096\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.3552 - val_loss: -1.7137\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3753 - val_loss: -1.7181\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.4165 - val_loss: -1.7230\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3608 - val_loss: -1.7277\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.3102 - val_loss: -1.7325\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.3611 - val_loss: -1.7375\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.3754 - val_loss: -1.7429\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.3685 - val_loss: -1.7483\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.3641 - val_loss: -1.7544\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.3908 - val_loss: -1.7609\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3951 - val_loss: -1.7680\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.4172 - val_loss: -1.7760\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.4394 - val_loss: -1.7841\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.3804 - val_loss: -1.7921\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.3626 - val_loss: -1.8000\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.4324 - val_loss: -1.8078\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.3806 - val_loss: -1.8151\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.4365 - val_loss: -1.8225\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.4381 - val_loss: -1.8302\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.3532 - val_loss: -1.8376\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.3699 - val_loss: -1.8446\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.4780 - val_loss: -1.8515\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.4273 - val_loss: -1.8582\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.4657 - val_loss: -1.8649\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.4251 - val_loss: -1.8720\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.4575 - val_loss: -1.8789\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4542 - val_loss: -1.8859\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.4506 - val_loss: -1.8931\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4292 - val_loss: -1.9004\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.4787 - val_loss: -1.9082\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.5033 - val_loss: -1.9160\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4807 - val_loss: -1.9234\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.5037 - val_loss: -1.9307\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.5307 - val_loss: -1.9384\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.5064 - val_loss: -1.9460\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.5315 - val_loss: -1.9534\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5405 - val_loss: -1.9609\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.4669 - val_loss: -1.9686\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -1.4987 - val_loss: -1.9762\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.5161 - val_loss: -1.9839\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.5377 - val_loss: -1.9918\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4860 - val_loss: -1.9993\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.5072 - val_loss: -2.0066\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.5482 - val_loss: -2.0142\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.5710 - val_loss: -2.0219\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.5506 - val_loss: -2.0294\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.5543 - val_loss: -2.0371\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.5715 - val_loss: -2.0449\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.6101 - val_loss: -2.0527\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -1.6084 - val_loss: -2.0605\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5897 - val_loss: -2.0683\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.5623 - val_loss: -2.0763\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.5873 - val_loss: -2.0845\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.6100 - val_loss: -2.0930\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6093 - val_loss: -2.1018\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.5736 - val_loss: -2.1105\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.5532 - val_loss: -2.1189\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.5821 - val_loss: -2.1272\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6499 - val_loss: -2.1356\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6477 - val_loss: -2.1445\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6422 - val_loss: -2.1536\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6597 - val_loss: -2.1629\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6441 - val_loss: -2.1722\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6516 - val_loss: -2.1817\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6862 - val_loss: -2.1911\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6640 - val_loss: -2.2005\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6760 - val_loss: -2.2099\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.6815 - val_loss: -2.2191\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6863 - val_loss: -2.2283\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6566 - val_loss: -2.2373\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7004 - val_loss: -2.2461\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7725 - val_loss: -2.2551\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7341 - val_loss: -2.2641\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7139 - val_loss: -2.2730\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7715 - val_loss: -2.2817\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.7520 - val_loss: -2.2902\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7240 - val_loss: -2.2986\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7786 - val_loss: -2.3068\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8542 - val_loss: -2.3146\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8072 - val_loss: -2.3221\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.7812 - val_loss: -2.3293\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.8134 - val_loss: -2.3364\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8608 - val_loss: -2.3432\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8421 - val_loss: -2.3496\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8360 - val_loss: -2.3558\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8102 - val_loss: -2.3618\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8365 - val_loss: -2.3676\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8220 - val_loss: -2.3731\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8389 - val_loss: -2.3784\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8567 - val_loss: -2.3836\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 160ms/step - loss: -1.8658 - val_loss: -2.3885\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.8631 - val_loss: -2.3932\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.8637 - val_loss: -2.3977\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8570 - val_loss: -2.4019\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8564 - val_loss: -2.4059\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8914 - val_loss: -2.4096\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8897 - val_loss: -2.4131\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8846 - val_loss: -2.4164\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8954 - val_loss: -2.4194\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8706 - val_loss: -2.4224\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9342 - val_loss: -2.4252\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.9080 - val_loss: -2.4278\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.9174 - val_loss: -2.4304\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.9191 - val_loss: -2.4328\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.9423 - val_loss: -2.4350\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8703 - val_loss: -2.4372\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8981 - val_loss: -2.4393\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.9127 - val_loss: -2.4413\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.9515 - val_loss: -2.4432\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8801 - val_loss: -2.4451\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.9487 - val_loss: -2.4468\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.9579 - val_loss: -2.4484\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.9317 - val_loss: -2.4499\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.9574 - val_loss: -2.4514\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9615 - val_loss: -2.4528\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.9758 - val_loss: -2.4541\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.9626 - val_loss: -2.4554\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.9846 - val_loss: -2.4565\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.9615 - val_loss: -2.4576\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: -1.9617 - val_loss: -2.4587\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.9734 - val_loss: -2.4598\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.9767 - val_loss: -2.4608\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.9874 - val_loss: -2.4618\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0029 - val_loss: -2.4627\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9929 - val_loss: -2.4635\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9828 - val_loss: -2.4643\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.9700 - val_loss: -2.4651\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.9966 - val_loss: -2.4659\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.9880 - val_loss: -2.4666\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.9854 - val_loss: -2.4673\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -2.0009 - val_loss: -2.4679\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.9812 - val_loss: -2.4685\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.9949 - val_loss: -2.4691\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.9844 - val_loss: -2.4697\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.9806 - val_loss: -2.4703\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.9923 - val_loss: -2.4709\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.9945 - val_loss: -2.4714\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.9859 - val_loss: -2.4720\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.9812 - val_loss: -2.4725\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.9910 - val_loss: -2.4730\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.9919 - val_loss: -2.4736\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0139 - val_loss: -2.4740\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0120 - val_loss: -2.4745\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0065 - val_loss: -2.4749\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.0247 - val_loss: -2.4753\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0096 - val_loss: -2.4757\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0091 - val_loss: -2.4761\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.9812 - val_loss: -2.4764\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -2.0214 - val_loss: -2.4768\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.9937 - val_loss: -2.4771\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0098 - val_loss: -2.4774\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.9972 - val_loss: -2.4777\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -2.0097 - val_loss: -2.4781\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0077 - val_loss: -2.4784\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -2.0117 - val_loss: -2.4788\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0230 - val_loss: -2.4791\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0091 - val_loss: -2.4794\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0168 - val_loss: -2.4797\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0106 - val_loss: -2.4800\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0271 - val_loss: -2.4802\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0231 - val_loss: -2.4805\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -2.0251 - val_loss: -2.4807\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -2.0258 - val_loss: -2.4809\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -2.0173 - val_loss: -2.4812\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0129 - val_loss: -2.4814\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0271 - val_loss: -2.4816\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -2.0215 - val_loss: -2.4818\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0238 - val_loss: -2.4820\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0343 - val_loss: -2.4822\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.0351 - val_loss: -2.4824\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -2.0253 - val_loss: -2.4826\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.0196 - val_loss: -2.4828\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0399 - val_loss: -2.4830\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -2.0344 - val_loss: -2.4831\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0121 - val_loss: -2.4833\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0370 - val_loss: -2.4835\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0267 - val_loss: -2.4836\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -2.0136 - val_loss: -2.4838\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0328 - val_loss: -2.4839\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -2.0276 - val_loss: -2.4841\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0254 - val_loss: -2.4842\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -2.0284 - val_loss: -2.4844\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -2.0195 - val_loss: -2.4845\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -2.0269 - val_loss: -2.4847\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0291 - val_loss: -2.4848\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0362 - val_loss: -2.4849\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -2.0346 - val_loss: -2.4851\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.0291 - val_loss: -2.4852\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -2.0436 - val_loss: -2.4853\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0253 - val_loss: -2.4854\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0101 - val_loss: -2.4855\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0365 - val_loss: -2.4857\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0364 - val_loss: -2.4858\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -2.0371 - val_loss: -2.4859\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0423 - val_loss: -2.4860\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -2.0469 - val_loss: -2.4861\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.0373 - val_loss: -2.4862\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.0431 - val_loss: -2.4863\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0257 - val_loss: -2.4864\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0379 - val_loss: -2.4865\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -2.0393 - val_loss: -2.4866\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0398 - val_loss: -2.4867\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0452 - val_loss: -2.4868\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0326 - val_loss: -2.4869\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0286 - val_loss: -2.4870\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0330 - val_loss: -2.4871\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0287 - val_loss: -2.4872\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 152ms/step - loss: -2.0452 - val_loss: -2.4873\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -2.0408 - val_loss: -2.4874\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -2.0465 - val_loss: -2.4875\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0377 - val_loss: -2.4876\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0467 - val_loss: -2.4877\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0364 - val_loss: -2.4878\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0431 - val_loss: -2.4878\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0482 - val_loss: -2.4879\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 139ms/step - loss: -2.0465 - val_loss: -2.4880\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.0477 - val_loss: -2.4880\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0414 - val_loss: -2.4881\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0333 - val_loss: -2.4882\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0481 - val_loss: -2.4882\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0523 - val_loss: -2.4883\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0449 - val_loss: -2.4884\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0430 - val_loss: -2.4884\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0475 - val_loss: -2.4885\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0489 - val_loss: -2.4886\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0459 - val_loss: -2.4886\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0218 - val_loss: -2.4887\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0418 - val_loss: -2.4887\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0449 - val_loss: -2.4888\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0533 - val_loss: -2.4888\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -2.0502 - val_loss: -2.4889\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -2.0505 - val_loss: -2.4889\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0417 - val_loss: -2.4890\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0376 - val_loss: -2.4891\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0282 - val_loss: -2.4891\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -2.0452 - val_loss: -2.4892\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0500 - val_loss: -2.4892\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0529 - val_loss: -2.4893\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.0520 - val_loss: -2.4893\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -2.0458 - val_loss: -2.4894\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0499 - val_loss: -2.4894\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -2.0398 - val_loss: -2.4895\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0452 - val_loss: -2.4895\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0506 - val_loss: -2.4896\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -2.0443 - val_loss: -2.4896\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0509 - val_loss: -2.4897\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -2.0467 - val_loss: -2.4897\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0471 - val_loss: -2.4898\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0406 - val_loss: -2.4898\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0432 - val_loss: -2.4898\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0509 - val_loss: -2.4899\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0508 - val_loss: -2.4899\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0515 - val_loss: -2.4900\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0527 - val_loss: -2.4900\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0408 - val_loss: -2.4900\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -2.0518 - val_loss: -2.4901\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0496 - val_loss: -2.4901\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0418 - val_loss: -2.4902\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0530 - val_loss: -2.4902\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0462 - val_loss: -2.4902\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0566 - val_loss: -2.4903\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0475 - val_loss: -2.4903\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0494 - val_loss: -2.4904\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0514 - val_loss: -2.4904\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0558 - val_loss: -2.4904\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0545 - val_loss: -2.4904\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0462 - val_loss: -2.4905\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0486 - val_loss: -2.4905\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0544 - val_loss: -2.4905\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0421 - val_loss: -2.4906\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.0422 - val_loss: -2.4906\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -2.0507 - val_loss: -2.4906\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0518 - val_loss: -2.4907\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0512 - val_loss: -2.4907\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0507 - val_loss: -2.4907\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0538 - val_loss: -2.4908\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.0527 - val_loss: -2.4908\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0425 - val_loss: -2.4908\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.0483 - val_loss: -2.4908\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0505 - val_loss: -2.4909\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0529 - val_loss: -2.4909\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0597 - val_loss: -2.4909\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -2.0520 - val_loss: -2.4909\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0502 - val_loss: -2.4910\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -2.0551 - val_loss: -2.4910\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -2.0523 - val_loss: -2.4910\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -2.0485 - val_loss: -2.4911\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -2.0446 - val_loss: -2.4911\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -2.0478 - val_loss: -2.4911\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -2.0507 - val_loss: -2.4911\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -2.0534 - val_loss: -2.4912\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0565 - val_loss: -2.4912\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0550 - val_loss: -2.4912\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0521 - val_loss: -2.4913\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.0546 - val_loss: -2.4913\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0520 - val_loss: -2.4913\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0538 - val_loss: -2.4913\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0537 - val_loss: -2.4913\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 137ms/step - loss: -2.0520 - val_loss: -2.4914\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -2.0558 - val_loss: -2.4914\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -2.0535 - val_loss: -2.4914\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -2.0460 - val_loss: -2.4914\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0534 - val_loss: -2.4915\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0549 - val_loss: -2.4915\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0513 - val_loss: -2.4915\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0548 - val_loss: -2.4915\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.0590 - val_loss: -2.4916\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -2.0519 - val_loss: -2.4916\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0530 - val_loss: -2.4916\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0537 - val_loss: -2.4916\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0553 - val_loss: -2.4917\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0467 - val_loss: -2.4917\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.0525 - val_loss: -2.4917\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -2.0531 - val_loss: -2.4917\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 145ms/step - loss: -2.0545 - val_loss: -2.4917\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -2.0509 - val_loss: -2.4918\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 140ms/step - loss: -2.0553 - val_loss: -2.4918\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0531 - val_loss: -2.4918\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -2.0532 - val_loss: -2.4918\n",
      "Epoch 360/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0481 - val_loss: -2.4918\n",
      "Epoch 361/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -2.0418 - val_loss: -2.4919\n",
      "Epoch 362/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -2.0585 - val_loss: -2.4919\n",
      "Epoch 363/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0567 - val_loss: -2.4919\n",
      "Epoch 364/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0551 - val_loss: -2.4919\n",
      "Epoch 365/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0570 - val_loss: -2.4919\n",
      "Epoch 366/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0566 - val_loss: -2.4920\n",
      "Epoch 367/1000\n",
      "2/2 [==============================] - 0s 166ms/step - loss: -2.0539 - val_loss: -2.4920\n",
      "Epoch 368/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -2.0572 - val_loss: -2.4920\n",
      "Epoch 369/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0513 - val_loss: -2.4920\n",
      "Epoch 370/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0578 - val_loss: -2.4920\n",
      "Epoch 371/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.0527 - val_loss: -2.4921\n",
      "Epoch 372/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0590 - val_loss: -2.4921\n",
      "Epoch 373/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -2.0536 - val_loss: -2.4921\n",
      "Epoch 374/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -2.0523 - val_loss: -2.4921\n",
      "Epoch 375/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0556 - val_loss: -2.4921\n",
      "Epoch 376/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0571 - val_loss: -2.4921\n",
      "Epoch 377/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0556 - val_loss: -2.4921\n",
      "Epoch 378/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -2.0533 - val_loss: -2.4922\n",
      "Epoch 379/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0545 - val_loss: -2.4922\n",
      "Epoch 380/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0533 - val_loss: -2.4922\n",
      "Epoch 381/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0572 - val_loss: -2.4922\n",
      "Epoch 382/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -2.0596 - val_loss: -2.4922\n",
      "Epoch 383/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -2.0548 - val_loss: -2.4922\n",
      "Epoch 384/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -2.0561 - val_loss: -2.4922\n",
      "Epoch 385/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0582 - val_loss: -2.4923\n",
      "Epoch 386/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -2.0544 - val_loss: -2.4923\n",
      "Epoch 387/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0581 - val_loss: -2.4923\n",
      "Epoch 388/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -2.0603 - val_loss: -2.4923\n",
      "Epoch 389/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.0556 - val_loss: -2.4923\n",
      "Epoch 390/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0571 - val_loss: -2.4923\n",
      "Epoch 391/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -2.0500 - val_loss: -2.4923\n",
      "Epoch 392/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0570 - val_loss: -2.4924\n",
      "Epoch 393/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0557 - val_loss: -2.4924\n",
      "Epoch 394/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0525 - val_loss: -2.4924\n",
      "Epoch 395/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0543 - val_loss: -2.4924\n",
      "Epoch 396/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0540 - val_loss: -2.4924\n",
      "Epoch 397/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0548 - val_loss: -2.4924\n",
      "Epoch 398/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0562 - val_loss: -2.4924\n",
      "Epoch 399/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.0590 - val_loss: -2.4925\n",
      "Epoch 400/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0548 - val_loss: -2.4925\n",
      "Epoch 401/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0550 - val_loss: -2.4925\n",
      "Epoch 402/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0581 - val_loss: -2.4925\n",
      "Epoch 403/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.0535 - val_loss: -2.4925\n",
      "Epoch 404/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0572 - val_loss: -2.4925\n",
      "Epoch 405/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -2.0580 - val_loss: -2.4925\n",
      "Epoch 406/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -2.0592 - val_loss: -2.4926\n",
      "Epoch 407/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -2.0568 - val_loss: -2.4926\n",
      "Epoch 408/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -2.0538 - val_loss: -2.4926\n",
      "Epoch 409/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -2.0580 - val_loss: -2.4926\n",
      "Epoch 410/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -2.0545 - val_loss: -2.4926\n",
      "Epoch 411/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -2.0566 - val_loss: -2.4926\n",
      "Epoch 412/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -2.0576 - val_loss: -2.4926\n",
      "Epoch 413/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0560 - val_loss: -2.4926\n",
      "Epoch 414/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0573 - val_loss: -2.4927\n",
      "Epoch 415/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0538 - val_loss: -2.4927\n",
      "Epoch 416/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0585 - val_loss: -2.4927\n",
      "Epoch 417/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0533 - val_loss: -2.4927\n",
      "Epoch 418/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0506 - val_loss: -2.4927\n",
      "Epoch 419/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0580 - val_loss: -2.4927\n",
      "Epoch 420/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0572 - val_loss: -2.4927\n",
      "Epoch 421/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -2.0580 - val_loss: -2.4927\n",
      "Epoch 422/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -2.0583 - val_loss: -2.4928\n",
      "Epoch 423/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0577 - val_loss: -2.4928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00423: early stopping\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f74d8e915f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_6 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 483ms/step - loss: -0.3936 - val_loss: -2.1437\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.3896 - val_loss: -2.1460\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.3899 - val_loss: -2.1486\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.4149 - val_loss: -2.1508\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4209 - val_loss: -2.1529\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.4006 - val_loss: -2.1547\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.4102 - val_loss: -2.1565\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.4123 - val_loss: -2.1584\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.3962 - val_loss: -2.1604\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.3937 - val_loss: -2.1621\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.4029 - val_loss: -2.1638\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.4184 - val_loss: -2.1653\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.3974 - val_loss: -2.1665\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.4093 - val_loss: -2.1678\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.4338 - val_loss: -2.1692\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.4082 - val_loss: -2.1706\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.3977 - val_loss: -2.1716\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.4381 - val_loss: -2.1722\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.4172 - val_loss: -2.1731\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.4262 - val_loss: -2.1739\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.4404 - val_loss: -2.1746\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.4154 - val_loss: -2.1752\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.4292 - val_loss: -2.1754\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.4251 - val_loss: -2.1756\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.4393 - val_loss: -2.1755\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.4192 - val_loss: -2.1754\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.4156 - val_loss: -2.1756\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.4480 - val_loss: -2.1758\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.4486 - val_loss: -2.1764\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.4377 - val_loss: -2.1778\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.4404 - val_loss: -2.1793\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -0.4396 - val_loss: -2.1816\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.4268 - val_loss: -2.1838\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.4317 - val_loss: -2.1858\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.4441 - val_loss: -2.1889\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.4186 - val_loss: -2.1921\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.4436 - val_loss: -2.1947\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.4272 - val_loss: -2.1973\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.4409 - val_loss: -2.1999\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.4258 - val_loss: -2.2019\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.4566 - val_loss: -2.2035\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.4569 - val_loss: -2.2059\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4545 - val_loss: -2.2094\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4301 - val_loss: -2.2126\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.4477 - val_loss: -2.2156\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.4347 - val_loss: -2.2191\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.4625 - val_loss: -2.2225\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.4672 - val_loss: -2.2259\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4488 - val_loss: -2.2299\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.4591 - val_loss: -2.2338\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4665 - val_loss: -2.2375\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.4550 - val_loss: -2.2404\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.4755 - val_loss: -2.2436\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.4441 - val_loss: -2.2476\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.4629 - val_loss: -2.2527\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.4702 - val_loss: -2.2574\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4678 - val_loss: -2.2627\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.4473 - val_loss: -2.2692\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.4706 - val_loss: -2.2758\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.4687 - val_loss: -2.2825\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.4686 - val_loss: -2.2891\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.4837 - val_loss: -2.2950\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4745 - val_loss: -2.2998\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.4731 - val_loss: -2.3047\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.4750 - val_loss: -2.3107\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.4506 - val_loss: -2.3171\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.4881 - val_loss: -2.3242\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.4701 - val_loss: -2.3311\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.4606 - val_loss: -2.3374\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.4719 - val_loss: -2.3433\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.4685 - val_loss: -2.3499\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.4648 - val_loss: -2.3564\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.4784 - val_loss: -2.3627\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.4729 - val_loss: -2.3697\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.4804 - val_loss: -2.3778\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.4923 - val_loss: -2.3873\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.4436 - val_loss: -2.3971\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -0.4949 - val_loss: -2.4066\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.4922 - val_loss: -2.4163\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.4960 - val_loss: -2.4263\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.4808 - val_loss: -2.4360\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.4849 - val_loss: -2.4459\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.4902 - val_loss: -2.4574\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.4868 - val_loss: -2.4689\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5062 - val_loss: -2.4819\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5032 - val_loss: -2.4973\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.4946 - val_loss: -2.5121\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.4982 - val_loss: -2.5266\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5176 - val_loss: -2.5395\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5020 - val_loss: -2.5517\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.4905 - val_loss: -2.5632\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5185 - val_loss: -2.5739\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5068 - val_loss: -2.5833\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5198 - val_loss: -2.5926\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.4949 - val_loss: -2.6013\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5026 - val_loss: -2.6098\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5137 - val_loss: -2.6185\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.4885 - val_loss: -2.6277\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5113 - val_loss: -2.6370\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.4911 - val_loss: -2.6462\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5054 - val_loss: -2.6554\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.4973 - val_loss: -2.6646\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5140 - val_loss: -2.6730\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5104 - val_loss: -2.6813\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -0.5073 - val_loss: -2.6905\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -0.5036 - val_loss: -2.7005\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.5070 - val_loss: -2.7113\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5304 - val_loss: -2.7213\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5202 - val_loss: -2.7305\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5236 - val_loss: -2.7388\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -0.5264 - val_loss: -2.7464\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5288 - val_loss: -2.7531\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5202 - val_loss: -2.7590\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5150 - val_loss: -2.7647\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.5231 - val_loss: -2.7703\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.4947 - val_loss: -2.7756\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -0.5226 - val_loss: -2.7804\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5231 - val_loss: -2.7851\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5309 - val_loss: -2.7892\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5192 - val_loss: -2.7937\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5281 - val_loss: -2.7982\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5079 - val_loss: -2.8030\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5309 - val_loss: -2.8084\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5304 - val_loss: -2.8143\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5259 - val_loss: -2.8201\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.5207 - val_loss: -2.8263\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5320 - val_loss: -2.8321\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5350 - val_loss: -2.8370\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5290 - val_loss: -2.8411\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5409 - val_loss: -2.8450\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5469 - val_loss: -2.8491\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5226 - val_loss: -2.8534\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5371 - val_loss: -2.8577\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5282 - val_loss: -2.8615\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5328 - val_loss: -2.8650\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5244 - val_loss: -2.8687\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5226 - val_loss: -2.8725\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5141 - val_loss: -2.8762\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5401 - val_loss: -2.8792\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5166 - val_loss: -2.8821\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5260 - val_loss: -2.8854\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5432 - val_loss: -2.8889\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5125 - val_loss: -2.8922\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5223 - val_loss: -2.8957\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5257 - val_loss: -2.8993\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5493 - val_loss: -2.9028\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5258 - val_loss: -2.9067\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5281 - val_loss: -2.9107\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: -0.5372 - val_loss: -2.9146\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5331 - val_loss: -2.9185\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5499 - val_loss: -2.9218\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5472 - val_loss: -2.9246\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5430 - val_loss: -2.9273\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5507 - val_loss: -2.9293\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5314 - val_loss: -2.9315\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5363 - val_loss: -2.9340\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5371 - val_loss: -2.9367\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5524 - val_loss: -2.9387\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5418 - val_loss: -2.9405\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5404 - val_loss: -2.9423\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5416 - val_loss: -2.9440\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5536 - val_loss: -2.9455\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5446 - val_loss: -2.9467\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.5292 - val_loss: -2.9483\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5390 - val_loss: -2.9500\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5415 - val_loss: -2.9516\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: -0.5358 - val_loss: -2.9530\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 139ms/step - loss: -0.5520 - val_loss: -2.9544\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -0.5497 - val_loss: -2.9562\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5542 - val_loss: -2.9580\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5497 - val_loss: -2.9596\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: -0.5397 - val_loss: -2.9610\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5491 - val_loss: -2.9623\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5479 - val_loss: -2.9634\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5460 - val_loss: -2.9647\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5267 - val_loss: -2.9662\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5568 - val_loss: -2.9676\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5442 - val_loss: -2.9689\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5485 - val_loss: -2.9702\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5406 - val_loss: -2.9712\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5420 - val_loss: -2.9721\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5494 - val_loss: -2.9730\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5490 - val_loss: -2.9739\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5443 - val_loss: -2.9748\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5332 - val_loss: -2.9759\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5329 - val_loss: -2.9774\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: -0.5333 - val_loss: -2.9789\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5562 - val_loss: -2.9801\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5393 - val_loss: -2.9814\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5537 - val_loss: -2.9827\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5408 - val_loss: -2.9838\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5539 - val_loss: -2.9847\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5513 - val_loss: -2.9855\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5412 - val_loss: -2.9865\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5439 - val_loss: -2.9876\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5505 - val_loss: -2.9885\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5490 - val_loss: -2.9892\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5508 - val_loss: -2.9897\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5447 - val_loss: -2.9903\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5442 - val_loss: -2.9910\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5352 - val_loss: -2.9918\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5519 - val_loss: -2.9927\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5443 - val_loss: -2.9936\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5385 - val_loss: -2.9945\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5576 - val_loss: -2.9953\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.5420 - val_loss: -2.9960\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5456 - val_loss: -2.9968\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5469 - val_loss: -2.9975\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5488 - val_loss: -2.9981\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -0.5492 - val_loss: -2.9987\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5561 - val_loss: -2.9994\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5482 - val_loss: -3.0001\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5547 - val_loss: -3.0008\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5480 - val_loss: -3.0013\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5465 - val_loss: -3.0019\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5514 - val_loss: -3.0026\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5443 - val_loss: -3.0033\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5564 - val_loss: -3.0038\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5465 - val_loss: -3.0042\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5538 - val_loss: -3.0046\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5362 - val_loss: -3.0050\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5416 - val_loss: -3.0056\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5468 - val_loss: -3.0062\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5507 - val_loss: -3.0068\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5529 - val_loss: -3.0072\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5538 - val_loss: -3.0076\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5495 - val_loss: -3.0080\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5542 - val_loss: -3.0085\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5509 - val_loss: -3.0090\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.5474 - val_loss: -3.0094\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5536 - val_loss: -3.0098\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.5405 - val_loss: -3.0103\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5544 - val_loss: -3.0107\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5331 - val_loss: -3.0112\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5489 - val_loss: -3.0118\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5590 - val_loss: -3.0123\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5508 - val_loss: -3.0128\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5522 - val_loss: -3.0133\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5485 - val_loss: -3.0138\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5472 - val_loss: -3.0144\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.5515 - val_loss: -3.0148\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5525 - val_loss: -3.0153\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5508 - val_loss: -3.0158\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5518 - val_loss: -3.0162\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5557 - val_loss: -3.0164\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -0.5455 - val_loss: -3.0167\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5574 - val_loss: -3.0169\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5490 - val_loss: -3.0172\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5567 - val_loss: -3.0174\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5569 - val_loss: -3.0176\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5546 - val_loss: -3.0177\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 136ms/step - loss: -0.5504 - val_loss: -3.0178\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.5559 - val_loss: -3.0177\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5535 - val_loss: -3.0177\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.5549 - val_loss: -3.0177\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5556 - val_loss: -3.0178\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5542 - val_loss: -3.0178\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5511 - val_loss: -3.0179\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5518 - val_loss: -3.0182\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5494 - val_loss: -3.0186\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5514 - val_loss: -3.0190\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5525 - val_loss: -3.0195\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5525 - val_loss: -3.0198\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5553 - val_loss: -3.0201\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5567 - val_loss: -3.0203\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5486 - val_loss: -3.0206\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5579 - val_loss: -3.0211\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.5582 - val_loss: -3.0214\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.5529 - val_loss: -3.0217\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.5573 - val_loss: -3.0219\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.5466 - val_loss: -3.0222\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5545 - val_loss: -3.0225\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5436 - val_loss: -3.0229\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5475 - val_loss: -3.0233\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5534 - val_loss: -3.0236\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5515 - val_loss: -3.0241\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -0.5526 - val_loss: -3.0245\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5542 - val_loss: -3.0248\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5566 - val_loss: -3.0250\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5516 - val_loss: -3.0252\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5425 - val_loss: -3.0257\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5550 - val_loss: -3.0261\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5510 - val_loss: -3.0266\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5533 - val_loss: -3.0270\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5534 - val_loss: -3.0275\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5568 - val_loss: -3.0279\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5498 - val_loss: -3.0283\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5565 - val_loss: -3.0287\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -0.5526 - val_loss: -3.0291\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5557 - val_loss: -3.0295\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5596 - val_loss: -3.0298\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5553 - val_loss: -3.0301\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5533 - val_loss: -3.0305\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5513 - val_loss: -3.0308\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5577 - val_loss: -3.0311\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5580 - val_loss: -3.0313\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5562 - val_loss: -3.0316\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5578 - val_loss: -3.0317\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5516 - val_loss: -3.0319\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5581 - val_loss: -3.0320\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 142ms/step - loss: -0.5562 - val_loss: -3.0321\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5571 - val_loss: -3.0322\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5523 - val_loss: -3.0322\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5548 - val_loss: -3.0323\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5535 - val_loss: -3.0324\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.5571 - val_loss: -3.0324\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5561 - val_loss: -3.0323\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5520 - val_loss: -3.0323\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5553 - val_loss: -3.0323\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5571 - val_loss: -3.0323\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5472 - val_loss: -3.0325\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5517 - val_loss: -3.0328\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.5553 - val_loss: -3.0330\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -0.5567 - val_loss: -3.0332\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.5543 - val_loss: -3.0332\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5532 - val_loss: -3.0333\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5558 - val_loss: -3.0333\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.5525 - val_loss: -3.0333\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5508 - val_loss: -3.0334\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5559 - val_loss: -3.0334\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5527 - val_loss: -3.0335\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5527 - val_loss: -3.0336\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5555 - val_loss: -3.0338\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5583 - val_loss: -3.0338\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5530 - val_loss: -3.0339\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5559 - val_loss: -3.0340\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5550 - val_loss: -3.0341\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5532 - val_loss: -3.0341\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5524 - val_loss: -3.0342\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5556 - val_loss: -3.0342\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5589 - val_loss: -3.0343\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5553 - val_loss: -3.0343\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5558 - val_loss: -3.0343\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.5496 - val_loss: -3.0343\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5573 - val_loss: -3.0343\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5532 - val_loss: -3.0344\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5531 - val_loss: -3.0345\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5547 - val_loss: -3.0347\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5566 - val_loss: -3.0348\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5581 - val_loss: -3.0349\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5573 - val_loss: -3.0349\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5577 - val_loss: -3.0350\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.5564 - val_loss: -3.0350\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5548 - val_loss: -3.0351\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5571 - val_loss: -3.0352\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5533 - val_loss: -3.0353\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5575 - val_loss: -3.0355\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5595 - val_loss: -3.0356\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5584 - val_loss: -3.0356\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5574 - val_loss: -3.0356\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5562 - val_loss: -3.0355\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5564 - val_loss: -3.0354\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5580 - val_loss: -3.0353\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5525 - val_loss: -3.0353\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5581 - val_loss: -3.0353\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5595 - val_loss: -3.0353\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5575 - val_loss: -3.0352\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5580 - val_loss: -3.0351\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5561 - val_loss: -3.0351\n",
      "Epoch 360/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5564 - val_loss: -3.0350\n",
      "Epoch 361/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5552 - val_loss: -3.0350\n",
      "Epoch 362/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5588 - val_loss: -3.0349\n",
      "Epoch 363/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5569 - val_loss: -3.0348\n",
      "Epoch 364/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5593 - val_loss: -3.0347\n",
      "Epoch 365/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5462 - val_loss: -3.0348\n",
      "Epoch 366/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5522 - val_loss: -3.0349\n",
      "Epoch 367/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -0.5521 - val_loss: -3.0351\n",
      "Epoch 368/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5579 - val_loss: -3.0352\n",
      "Epoch 369/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5536 - val_loss: -3.0353\n",
      "Epoch 370/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5511 - val_loss: -3.0355\n",
      "Epoch 371/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5543 - val_loss: -3.0357\n",
      "Epoch 372/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5531 - val_loss: -3.0359\n",
      "Epoch 373/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5562 - val_loss: -3.0360\n",
      "Epoch 374/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5538 - val_loss: -3.0361\n",
      "Epoch 375/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5576 - val_loss: -3.0363\n",
      "Epoch 376/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5522 - val_loss: -3.0365\n",
      "Epoch 377/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5500 - val_loss: -3.0368\n",
      "Epoch 378/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5556 - val_loss: -3.0370\n",
      "Epoch 379/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5561 - val_loss: -3.0372\n",
      "Epoch 380/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -0.5533 - val_loss: -3.0373\n",
      "Epoch 381/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5565 - val_loss: -3.0374\n",
      "Epoch 382/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5548 - val_loss: -3.0374\n",
      "Epoch 383/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5539 - val_loss: -3.0374\n",
      "Epoch 384/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5582 - val_loss: -3.0373\n",
      "Epoch 385/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5576 - val_loss: -3.0372\n",
      "Epoch 386/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5550 - val_loss: -3.0371\n",
      "Epoch 387/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5528 - val_loss: -3.0370\n",
      "Epoch 388/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.5534 - val_loss: -3.0369\n",
      "Epoch 389/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5551 - val_loss: -3.0369\n",
      "Epoch 390/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5571 - val_loss: -3.0369\n",
      "Epoch 391/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.5539 - val_loss: -3.0369\n",
      "Epoch 392/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5552 - val_loss: -3.0368\n",
      "Epoch 393/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5540 - val_loss: -3.0368\n",
      "Epoch 394/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5556 - val_loss: -3.0368\n",
      "Epoch 395/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5552 - val_loss: -3.0368\n",
      "Epoch 396/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5573 - val_loss: -3.0368\n",
      "Epoch 397/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.5569 - val_loss: -3.0367\n",
      "Epoch 398/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5603 - val_loss: -3.0367\n",
      "Epoch 399/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5571 - val_loss: -3.0366\n",
      "Epoch 400/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5564 - val_loss: -3.0365\n",
      "Epoch 401/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5576 - val_loss: -3.0365\n",
      "Epoch 402/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5572 - val_loss: -3.0365\n",
      "Epoch 403/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5571 - val_loss: -3.0365\n",
      "Epoch 404/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5545 - val_loss: -3.0364\n",
      "Epoch 405/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5546 - val_loss: -3.0364\n",
      "Epoch 406/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5538 - val_loss: -3.0364\n",
      "Epoch 407/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5581 - val_loss: -3.0364\n",
      "Epoch 408/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5555 - val_loss: -3.0365\n",
      "Epoch 409/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5574 - val_loss: -3.0365\n",
      "Epoch 410/1000\n",
      "2/2 [==============================] - 0s 147ms/step - loss: -0.5521 - val_loss: -3.0366\n",
      "Epoch 411/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5595 - val_loss: -3.0367\n",
      "Epoch 412/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5550 - val_loss: -3.0369\n",
      "Epoch 413/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5551 - val_loss: -3.0372\n",
      "Epoch 414/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.5508 - val_loss: -3.0376\n",
      "Epoch 415/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5572 - val_loss: -3.0378\n",
      "Epoch 416/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5589 - val_loss: -3.0380\n",
      "Epoch 417/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5560 - val_loss: -3.0381\n",
      "Epoch 418/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5579 - val_loss: -3.0382\n",
      "Epoch 419/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5562 - val_loss: -3.0383\n",
      "Epoch 420/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5584 - val_loss: -3.0383\n",
      "Epoch 421/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5475 - val_loss: -3.0384\n",
      "Epoch 422/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5581 - val_loss: -3.0387\n",
      "Epoch 423/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5561 - val_loss: -3.0388\n",
      "Epoch 424/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5581 - val_loss: -3.0389\n",
      "Epoch 425/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5602 - val_loss: -3.0389\n",
      "Epoch 426/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5599 - val_loss: -3.0389\n",
      "Epoch 427/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5487 - val_loss: -3.0390\n",
      "Epoch 428/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5580 - val_loss: -3.0391\n",
      "Epoch 429/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5572 - val_loss: -3.0391\n",
      "Epoch 430/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5576 - val_loss: -3.0391\n",
      "Epoch 431/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5564 - val_loss: -3.0391\n",
      "Epoch 432/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5575 - val_loss: -3.0391\n",
      "Epoch 433/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5565 - val_loss: -3.0391\n",
      "Epoch 434/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5563 - val_loss: -3.0391\n",
      "Epoch 435/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5554 - val_loss: -3.0392\n",
      "Epoch 436/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5510 - val_loss: -3.0392\n",
      "Epoch 437/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5564 - val_loss: -3.0393\n",
      "Epoch 438/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5597 - val_loss: -3.0394\n",
      "Epoch 439/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5586 - val_loss: -3.0394\n",
      "Epoch 440/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5571 - val_loss: -3.0395\n",
      "Epoch 441/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5581 - val_loss: -3.0395\n",
      "Epoch 442/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5534 - val_loss: -3.0395\n",
      "Epoch 443/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5583 - val_loss: -3.0396\n",
      "Epoch 444/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5560 - val_loss: -3.0396\n",
      "Epoch 445/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5556 - val_loss: -3.0396\n",
      "Epoch 446/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5560 - val_loss: -3.0396\n",
      "Epoch 447/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5562 - val_loss: -3.0397\n",
      "Epoch 448/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5540 - val_loss: -3.0398\n",
      "Epoch 449/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5565 - val_loss: -3.0399\n",
      "Epoch 450/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5556 - val_loss: -3.0400\n",
      "Epoch 451/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5561 - val_loss: -3.0401\n",
      "Epoch 452/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5575 - val_loss: -3.0401\n",
      "Epoch 453/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5538 - val_loss: -3.0402\n",
      "Epoch 454/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5558 - val_loss: -3.0403\n",
      "Epoch 455/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5561 - val_loss: -3.0403\n",
      "Epoch 456/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5569 - val_loss: -3.0403\n",
      "Epoch 457/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5579 - val_loss: -3.0403\n",
      "Epoch 458/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5575 - val_loss: -3.0403\n",
      "Epoch 459/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5562 - val_loss: -3.0403\n",
      "Epoch 460/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5565 - val_loss: -3.0402\n",
      "Epoch 461/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5567 - val_loss: -3.0401\n",
      "Epoch 462/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5525 - val_loss: -3.0400\n",
      "Epoch 463/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5559 - val_loss: -3.0398\n",
      "Epoch 464/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5567 - val_loss: -3.0397\n",
      "Epoch 465/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5589 - val_loss: -3.0396\n",
      "Epoch 466/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5560 - val_loss: -3.0395\n",
      "Epoch 467/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5570 - val_loss: -3.0395\n",
      "Epoch 468/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5539 - val_loss: -3.0395\n",
      "Epoch 469/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5567 - val_loss: -3.0394\n",
      "Epoch 470/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.5558 - val_loss: -3.0394\n",
      "Epoch 471/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5572 - val_loss: -3.0394\n",
      "Epoch 472/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5601 - val_loss: -3.0393\n",
      "Epoch 473/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5556 - val_loss: -3.0393\n",
      "Epoch 474/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5585 - val_loss: -3.0394\n",
      "Epoch 475/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5550 - val_loss: -3.0394\n",
      "Epoch 476/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5572 - val_loss: -3.0395\n",
      "Epoch 477/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5540 - val_loss: -3.0396\n",
      "Epoch 478/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5579 - val_loss: -3.0396\n",
      "Epoch 479/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -0.5559 - val_loss: -3.0396\n",
      "Epoch 480/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5609 - val_loss: -3.0395\n",
      "Epoch 481/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5575 - val_loss: -3.0394\n",
      "Epoch 482/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5580 - val_loss: -3.0393\n",
      "Epoch 483/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5581 - val_loss: -3.0392\n",
      "Epoch 484/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -0.5567 - val_loss: -3.0392\n",
      "Epoch 485/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5577 - val_loss: -3.0391\n",
      "Epoch 486/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5552 - val_loss: -3.0391\n",
      "Epoch 487/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5558 - val_loss: -3.0391\n",
      "Epoch 488/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5558 - val_loss: -3.0392\n",
      "Epoch 489/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5555 - val_loss: -3.0392\n",
      "Epoch 490/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5602 - val_loss: -3.0393\n",
      "Epoch 491/1000\n",
      "2/2 [==============================] - 0s 181ms/step - loss: -0.5564 - val_loss: -3.0393\n",
      "Epoch 492/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5575 - val_loss: -3.0393\n",
      "Epoch 493/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5559 - val_loss: -3.0392\n",
      "Epoch 494/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5552 - val_loss: -3.0391\n",
      "Epoch 495/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5500 - val_loss: -3.0391\n",
      "Epoch 496/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5570 - val_loss: -3.0392\n",
      "Epoch 497/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5559 - val_loss: -3.0393\n",
      "Epoch 498/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.5569 - val_loss: -3.0394\n",
      "Epoch 499/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5567 - val_loss: -3.0395\n",
      "Epoch 500/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.5577 - val_loss: -3.0396\n",
      "Epoch 501/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.5546 - val_loss: -3.0397\n",
      "Epoch 502/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.5593 - val_loss: -3.0397\n",
      "Epoch 503/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.5569 - val_loss: -3.0397\n",
      "Epoch 504/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5554 - val_loss: -3.0397\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00504: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_7 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 496ms/step - loss: -0.5922 - val_loss: -2.6186\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6086 - val_loss: -2.6363\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.6035 - val_loss: -2.6532\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6069 - val_loss: -2.6734\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5943 - val_loss: -2.6954\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.6086 - val_loss: -2.7195\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5991 - val_loss: -2.7420\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.6114 - val_loss: -2.7595\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -0.6230 - val_loss: -2.7742\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.6030 - val_loss: -2.7852\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -0.6245 - val_loss: -2.7971\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.6068 - val_loss: -2.8124\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.6186 - val_loss: -2.8269\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6084 - val_loss: -2.8420\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6017 - val_loss: -2.8594\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.6121 - val_loss: -2.8763\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5909 - val_loss: -2.8909\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.6204 - val_loss: -2.9030\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6146 - val_loss: -2.9157\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5995 - val_loss: -2.9299\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.6082 - val_loss: -2.9436\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.6035 - val_loss: -2.9549\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6159 - val_loss: -2.9634\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.6036 - val_loss: -2.9708\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.6281 - val_loss: -2.9791\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.6109 - val_loss: -2.9863\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6168 - val_loss: -2.9924\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6209 - val_loss: -2.9969\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5877 - val_loss: -3.0034\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.6241 - val_loss: -3.0097\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.6219 - val_loss: -3.0172\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6170 - val_loss: -3.0249\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.6299 - val_loss: -3.0332\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.6292 - val_loss: -3.0420\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.6262 - val_loss: -3.0495\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.6116 - val_loss: -3.0551\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6123 - val_loss: -3.0584\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.6155 - val_loss: -3.0628\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6105 - val_loss: -3.0678\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.6065 - val_loss: -3.0729\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6237 - val_loss: -3.0795\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -0.6243 - val_loss: -3.0881\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6254 - val_loss: -3.0971\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.6299 - val_loss: -3.1046\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6229 - val_loss: -3.1128\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6186 - val_loss: -3.1197\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -0.6149 - val_loss: -3.1261\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.6315 - val_loss: -3.1330\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6289 - val_loss: -3.1377\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.6160 - val_loss: -3.1422\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6287 - val_loss: -3.1463\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6320 - val_loss: -3.1510\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.6326 - val_loss: -3.1549\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6289 - val_loss: -3.1582\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6199 - val_loss: -3.1611\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.6187 - val_loss: -3.1631\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6264 - val_loss: -3.1646\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6321 - val_loss: -3.1647\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.6283 - val_loss: -3.1641\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6317 - val_loss: -3.1637\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6230 - val_loss: -3.1628\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.6306 - val_loss: -3.1610\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.6208 - val_loss: -3.1595\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.6331 - val_loss: -3.1599\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.6049 - val_loss: -3.1626\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.6123 - val_loss: -3.1665\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.6169 - val_loss: -3.1712\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.6076 - val_loss: -3.1752\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.6146 - val_loss: -3.1788\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.6149 - val_loss: -3.1818\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6341 - val_loss: -3.1842\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6030 - val_loss: -3.1854\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6243 - val_loss: -3.1856\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -0.6282 - val_loss: -3.1849\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.6245 - val_loss: -3.1852\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6194 - val_loss: -3.1873\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6351 - val_loss: -3.1886\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.6293 - val_loss: -3.1881\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.6367 - val_loss: -3.1862\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6390 - val_loss: -3.1839\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6292 - val_loss: -3.1815\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.6292 - val_loss: -3.1790\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6329 - val_loss: -3.1775\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.6227 - val_loss: -3.1763\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6253 - val_loss: -3.1744\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.6358 - val_loss: -3.1717\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.6207 - val_loss: -3.1698\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6053 - val_loss: -3.1675\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -0.6140 - val_loss: -3.1647\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.6317 - val_loss: -3.1608\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6324 - val_loss: -3.1580\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6426 - val_loss: -3.1553\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6324 - val_loss: -3.1520\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.6358 - val_loss: -3.1496\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.6423 - val_loss: -3.1471\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.6437 - val_loss: -3.1442\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6456 - val_loss: -3.1411\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6553 - val_loss: -3.1376\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.6348 - val_loss: -3.1350\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6354 - val_loss: -3.1347\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.6548 - val_loss: -3.1344\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6321 - val_loss: -3.1339\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.6478 - val_loss: -3.1332\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.6305 - val_loss: -3.1315\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6313 - val_loss: -3.1293\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6204 - val_loss: -3.1269\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6353 - val_loss: -3.1242\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6326 - val_loss: -3.1217\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.6355 - val_loss: -3.1193\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6449 - val_loss: -3.1155\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6356 - val_loss: -3.1109\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6385 - val_loss: -3.1063\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.6341 - val_loss: -3.1019\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.6263 - val_loss: -3.0976\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.6477 - val_loss: -3.0938\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6441 - val_loss: -3.0904\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6546 - val_loss: -3.0864\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 146ms/step - loss: -0.6319 - val_loss: -3.0832\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.6281 - val_loss: -3.0808\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.6560 - val_loss: -3.0793\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.6438 - val_loss: -3.0777\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.6484 - val_loss: -3.0757\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.6327 - val_loss: -3.0729\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.6580 - val_loss: -3.0683\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.6484 - val_loss: -3.0631\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.6541 - val_loss: -3.0565\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.6387 - val_loss: -3.0498\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00127: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_8 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 476ms/step - loss: -1.3434 - val_loss: -3.0807\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.3309 - val_loss: -3.0857\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.2980 - val_loss: -3.0844\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3406 - val_loss: -3.0827\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3683 - val_loss: -3.0789\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.3448 - val_loss: -3.0752\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3346 - val_loss: -3.0756\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3595 - val_loss: -3.0767\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.3137 - val_loss: -3.0783\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3827 - val_loss: -3.0818\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3311 - val_loss: -3.0864\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.3649 - val_loss: -3.0909\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.3606 - val_loss: -3.0967\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.3330 - val_loss: -3.1019\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.3498 - val_loss: -3.1048\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.3650 - val_loss: -3.1056\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.3842 - val_loss: -3.1071\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.3280 - val_loss: -3.1106\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.3968 - val_loss: -3.1160\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.3818 - val_loss: -3.1211\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3892 - val_loss: -3.1242\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.3280 - val_loss: -3.1269\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3735 - val_loss: -3.1295\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.3984 - val_loss: -3.1328\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.3397 - val_loss: -3.1366\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.3908 - val_loss: -3.1427\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.4121 - val_loss: -3.1485\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.3895 - val_loss: -3.1541\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.3611 - val_loss: -3.1595\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3408 - val_loss: -3.1684\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.4070 - val_loss: -3.1769\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.3542 - val_loss: -3.1844\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.3434 - val_loss: -3.1920\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.3992 - val_loss: -3.1982\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4064 - val_loss: -3.2008\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.4287 - val_loss: -3.2029\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3806 - val_loss: -3.2071\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.3928 - val_loss: -3.2105\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.4134 - val_loss: -3.2125\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.3712 - val_loss: -3.2140\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.3973 - val_loss: -3.2148\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 146ms/step - loss: -1.3880 - val_loss: -3.2148\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.4029 - val_loss: -3.2154\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.3393 - val_loss: -3.2158\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.3533 - val_loss: -3.2166\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.3650 - val_loss: -3.2160\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.3889 - val_loss: -3.2142\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3707 - val_loss: -3.2112\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.3644 - val_loss: -3.2063\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.4115 - val_loss: -3.2018\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.3919 - val_loss: -3.1981\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.4376 - val_loss: -3.1936\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.3565 - val_loss: -3.1879\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4045 - val_loss: -3.1816\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.4006 - val_loss: -3.1752\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.3706 - val_loss: -3.1693\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.4336 - val_loss: -3.1636\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.4333 - val_loss: -3.1586\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.4004 - val_loss: -3.1548\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.4265 - val_loss: -3.1502\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.3659 - val_loss: -3.1456\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.4235 - val_loss: -3.1417\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.4232 - val_loss: -3.1381\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.3928 - val_loss: -3.1339\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.3916 - val_loss: -3.1294\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.4202 - val_loss: -3.1259\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4238 - val_loss: -3.1228\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.4118 - val_loss: -3.1191\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.4380 - val_loss: -3.1133\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.3937 - val_loss: -3.1073\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.4313 - val_loss: -3.1026\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.4091 - val_loss: -3.0986\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3939 - val_loss: -3.0964\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.4263 - val_loss: -3.0928\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.4399 - val_loss: -3.0880\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.4514 - val_loss: -3.0837\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4199 - val_loss: -3.0791\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.4236 - val_loss: -3.0737\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.4345 - val_loss: -3.0684\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4201 - val_loss: -3.0635\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4220 - val_loss: -3.0595\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.4165 - val_loss: -3.0538\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.4065 - val_loss: -3.0479\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.4025 - val_loss: -3.0420\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.3787 - val_loss: -3.0361\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.3894 - val_loss: -3.0314\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.4144 - val_loss: -3.0272\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4273 - val_loss: -3.0233\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4002 - val_loss: -3.0194\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.4207 - val_loss: -3.0151\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.4277 - val_loss: -3.0098\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.4161 - val_loss: -3.0044\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.4207 - val_loss: -2.9996\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.4135 - val_loss: -2.9955\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.4252 - val_loss: -2.9919\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00095: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_9 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 517ms/step - loss: -1.5477 - val_loss: -1.9644\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.5460 - val_loss: -1.9754\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 136ms/step - loss: -1.5420 - val_loss: -1.9853\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6042 - val_loss: -1.9942\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6539 - val_loss: -2.0024\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.5636 - val_loss: -2.0109\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.6082 - val_loss: -2.0201\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6458 - val_loss: -2.0288\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 137ms/step - loss: -1.5492 - val_loss: -2.0371\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.5486 - val_loss: -2.0458\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6158 - val_loss: -2.0540\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.6391 - val_loss: -2.0614\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.5547 - val_loss: -2.0684\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.5830 - val_loss: -2.0753\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6006 - val_loss: -2.0827\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.6190 - val_loss: -2.0892\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6183 - val_loss: -2.0950\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.6352 - val_loss: -2.0998\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6398 - val_loss: -2.1033\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.5804 - val_loss: -2.1069\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6046 - val_loss: -2.1107\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6518 - val_loss: -2.1140\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.6171 - val_loss: -2.1167\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.5982 - val_loss: -2.1189\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.5910 - val_loss: -2.1219\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6385 - val_loss: -2.1251\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6273 - val_loss: -2.1280\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.5805 - val_loss: -2.1312\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6303 - val_loss: -2.1340\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.6128 - val_loss: -2.1375\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.5984 - val_loss: -2.1410\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.6018 - val_loss: -2.1446\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6513 - val_loss: -2.1482\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6228 - val_loss: -2.1510\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.6236 - val_loss: -2.1539\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6556 - val_loss: -2.1571\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.6242 - val_loss: -2.1601\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.6171 - val_loss: -2.1638\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.6723 - val_loss: -2.1673\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6201 - val_loss: -2.1701\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.6021 - val_loss: -2.1737\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.6416 - val_loss: -2.1775\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.6624 - val_loss: -2.1808\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.6134 - val_loss: -2.1841\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6408 - val_loss: -2.1878\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.6323 - val_loss: -2.1911\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.6280 - val_loss: -2.1939\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6409 - val_loss: -2.1958\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6457 - val_loss: -2.1967\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6773 - val_loss: -2.1979\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6633 - val_loss: -2.1991\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6333 - val_loss: -2.2009\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.6174 - val_loss: -2.2031\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6312 - val_loss: -2.2048\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6586 - val_loss: -2.2061\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.6638 - val_loss: -2.2075\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6119 - val_loss: -2.2089\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6388 - val_loss: -2.2101\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6259 - val_loss: -2.2111\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6167 - val_loss: -2.2126\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.6248 - val_loss: -2.2140\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.6351 - val_loss: -2.2154\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6456 - val_loss: -2.2173\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.6303 - val_loss: -2.2187\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -1.6457 - val_loss: -2.2200\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6824 - val_loss: -2.2207\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.6166 - val_loss: -2.2214\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.6073 - val_loss: -2.2223\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6334 - val_loss: -2.2231\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6424 - val_loss: -2.2243\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.6549 - val_loss: -2.2261\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.6057 - val_loss: -2.2278\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6272 - val_loss: -2.2298\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6697 - val_loss: -2.2314\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6723 - val_loss: -2.2318\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6479 - val_loss: -2.2320\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6431 - val_loss: -2.2322\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6694 - val_loss: -2.2320\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6793 - val_loss: -2.2317\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6377 - val_loss: -2.2311\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.6597 - val_loss: -2.2300\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.6765 - val_loss: -2.2289\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6450 - val_loss: -2.2282\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6396 - val_loss: -2.2279\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6728 - val_loss: -2.2281\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.6712 - val_loss: -2.2282\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.6374 - val_loss: -2.2280\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6502 - val_loss: -2.2281\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6646 - val_loss: -2.2279\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.6672 - val_loss: -2.2273\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6626 - val_loss: -2.2267\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.6330 - val_loss: -2.2259\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6421 - val_loss: -2.2248\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.6635 - val_loss: -2.2241\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6694 - val_loss: -2.2233\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.6507 - val_loss: -2.2222\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.6371 - val_loss: -2.2211\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.6341 - val_loss: -2.2200\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6493 - val_loss: -2.2195\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6727 - val_loss: -2.2185\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6724 - val_loss: -2.2173\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.6732 - val_loss: -2.2160\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6752 - val_loss: -2.2149\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6669 - val_loss: -2.2143\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6346 - val_loss: -2.2140\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6507 - val_loss: -2.2138\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6324 - val_loss: -2.2134\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.6623 - val_loss: -2.2129\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6509 - val_loss: -2.2124\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6549 - val_loss: -2.2122\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6661 - val_loss: -2.2119\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6633 - val_loss: -2.2112\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.6825 - val_loss: -2.2108\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6445 - val_loss: -2.2107\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6827 - val_loss: -2.2104\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.6831 - val_loss: -2.2101\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6538 - val_loss: -2.2097\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6267 - val_loss: -2.2095\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6550 - val_loss: -2.2097\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6512 - val_loss: -2.2101\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6317 - val_loss: -2.2104\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6729 - val_loss: -2.2107\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.6597 - val_loss: -2.2108\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.6744 - val_loss: -2.2112\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00124: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_10 (Softmax)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 512ms/step - loss: -1.3142 - val_loss: -0.7822\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3175 - val_loss: -0.7871\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.3447 - val_loss: -0.7920\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.3000 - val_loss: -0.7971\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.3137 - val_loss: -0.8022\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.3460 - val_loss: -0.8073\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.3518 - val_loss: -0.8126\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.3689 - val_loss: -0.8176\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3474 - val_loss: -0.8225\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.3392 - val_loss: -0.8275\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3358 - val_loss: -0.8325\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4383 - val_loss: -0.8377\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.4205 - val_loss: -0.8430\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3837 - val_loss: -0.8484\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.4057 - val_loss: -0.8540\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.4552 - val_loss: -0.8595\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.3677 - val_loss: -0.8649\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3862 - val_loss: -0.8703\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -1.4603 - val_loss: -0.8759\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.4331 - val_loss: -0.8819\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.4664 - val_loss: -0.8879\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.4541 - val_loss: -0.8939\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.4438 - val_loss: -0.9001\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.4867 - val_loss: -0.9062\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.4802 - val_loss: -0.9122\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.4764 - val_loss: -0.9181\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.4675 - val_loss: -0.9240\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.5588 - val_loss: -0.9300\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.4990 - val_loss: -0.9362\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.4451 - val_loss: -0.9426\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.5088 - val_loss: -0.9488\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5239 - val_loss: -0.9551\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.5476 - val_loss: -0.9615\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.4424 - val_loss: -0.9680\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.5232 - val_loss: -0.9746\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.5357 - val_loss: -0.9809\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.5391 - val_loss: -0.9871\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.5227 - val_loss: -0.9932\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5618 - val_loss: -0.9997\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.5611 - val_loss: -1.0062\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.5565 - val_loss: -1.0125\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6204 - val_loss: -1.0188\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.5824 - val_loss: -1.0250\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.5196 - val_loss: -1.0311\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.5808 - val_loss: -1.0372\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.5664 - val_loss: -1.0433\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6001 - val_loss: -1.0491\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.5462 - val_loss: -1.0549\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.6223 - val_loss: -1.0604\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6061 - val_loss: -1.0657\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.5990 - val_loss: -1.0712\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.5553 - val_loss: -1.0769\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.5979 - val_loss: -1.0826\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -1.6247 - val_loss: -1.0881\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6116 - val_loss: -1.0930\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6290 - val_loss: -1.0981\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.6565 - val_loss: -1.1033\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6395 - val_loss: -1.1083\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.6715 - val_loss: -1.1133\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.6636 - val_loss: -1.1179\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6487 - val_loss: -1.1226\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7064 - val_loss: -1.1271\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6049 - val_loss: -1.1318\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6265 - val_loss: -1.1366\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 132ms/step - loss: -1.7045 - val_loss: -1.1410\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6368 - val_loss: -1.1453\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7288 - val_loss: -1.1495\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7012 - val_loss: -1.1534\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6470 - val_loss: -1.1572\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6366 - val_loss: -1.1609\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.6658 - val_loss: -1.1644\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.6583 - val_loss: -1.1676\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6809 - val_loss: -1.1709\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.7189 - val_loss: -1.1743\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7263 - val_loss: -1.1778\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6766 - val_loss: -1.1817\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.6813 - val_loss: -1.1857\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.7041 - val_loss: -1.1895\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6875 - val_loss: -1.1932\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6926 - val_loss: -1.1969\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.6804 - val_loss: -1.2006\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.7241 - val_loss: -1.2045\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7522 - val_loss: -1.2081\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7018 - val_loss: -1.2116\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7120 - val_loss: -1.2150\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7131 - val_loss: -1.2184\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7123 - val_loss: -1.2216\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7326 - val_loss: -1.2244\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7039 - val_loss: -1.2273\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.6746 - val_loss: -1.2304\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7811 - val_loss: -1.2333\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7195 - val_loss: -1.2360\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6908 - val_loss: -1.2387\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7209 - val_loss: -1.2415\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7225 - val_loss: -1.2442\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.7486 - val_loss: -1.2469\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7375 - val_loss: -1.2497\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.7240 - val_loss: -1.2525\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.7288 - val_loss: -1.2553\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7367 - val_loss: -1.2580\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6749 - val_loss: -1.2607\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.7644 - val_loss: -1.2636\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7834 - val_loss: -1.2665\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7492 - val_loss: -1.2695\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6914 - val_loss: -1.2725\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7485 - val_loss: -1.2754\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7778 - val_loss: -1.2782\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7793 - val_loss: -1.2807\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7168 - val_loss: -1.2832\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.7506 - val_loss: -1.2856\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7541 - val_loss: -1.2880\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.7047 - val_loss: -1.2904\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7577 - val_loss: -1.2927\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7157 - val_loss: -1.2949\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.7989 - val_loss: -1.2970\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7985 - val_loss: -1.2989\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7503 - val_loss: -1.3008\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7682 - val_loss: -1.3029\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7545 - val_loss: -1.3048\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7732 - val_loss: -1.3067\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.7555 - val_loss: -1.3086\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7668 - val_loss: -1.3103\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7739 - val_loss: -1.3120\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7966 - val_loss: -1.3135\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.7403 - val_loss: -1.3150\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8118 - val_loss: -1.3163\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7831 - val_loss: -1.3174\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7689 - val_loss: -1.3185\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7671 - val_loss: -1.3196\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.8078 - val_loss: -1.3209\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.7836 - val_loss: -1.3222\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.7368 - val_loss: -1.3236\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7329 - val_loss: -1.3251\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.7767 - val_loss: -1.3266\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7332 - val_loss: -1.3281\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.7869 - val_loss: -1.3295\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.7350 - val_loss: -1.3309\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7462 - val_loss: -1.3323\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.7571 - val_loss: -1.3337\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7881 - val_loss: -1.3349\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.7977 - val_loss: -1.3359\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8105 - val_loss: -1.3368\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.7853 - val_loss: -1.3376\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7679 - val_loss: -1.3384\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.7604 - val_loss: -1.3392\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.7826 - val_loss: -1.3401\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.7676 - val_loss: -1.3411\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7964 - val_loss: -1.3421\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7757 - val_loss: -1.3430\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8069 - val_loss: -1.3439\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7634 - val_loss: -1.3448\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8171 - val_loss: -1.3456\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7800 - val_loss: -1.3464\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7759 - val_loss: -1.3472\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8123 - val_loss: -1.3479\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7890 - val_loss: -1.3487\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.7805 - val_loss: -1.3495\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7849 - val_loss: -1.3503\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.7756 - val_loss: -1.3511\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7928 - val_loss: -1.3518\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7936 - val_loss: -1.3525\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.7994 - val_loss: -1.3531\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7799 - val_loss: -1.3537\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7921 - val_loss: -1.3544\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.7701 - val_loss: -1.3550\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.7827 - val_loss: -1.3555\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7915 - val_loss: -1.3560\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7865 - val_loss: -1.3564\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7791 - val_loss: -1.3569\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8030 - val_loss: -1.3573\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7998 - val_loss: -1.3576\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.7616 - val_loss: -1.3580\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7950 - val_loss: -1.3584\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.7935 - val_loss: -1.3587\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.7849 - val_loss: -1.3591\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8086 - val_loss: -1.3595\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8084 - val_loss: -1.3597\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7842 - val_loss: -1.3600\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7997 - val_loss: -1.3603\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7956 - val_loss: -1.3606\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8028 - val_loss: -1.3609\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7820 - val_loss: -1.3612\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.8012 - val_loss: -1.3615\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7657 - val_loss: -1.3618\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.8204 - val_loss: -1.3620\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8035 - val_loss: -1.3622\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.7792 - val_loss: -1.3625\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7937 - val_loss: -1.3628\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7956 - val_loss: -1.3630\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8054 - val_loss: -1.3633\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8033 - val_loss: -1.3636\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7911 - val_loss: -1.3640\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7867 - val_loss: -1.3642\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.7517 - val_loss: -1.3645\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7997 - val_loss: -1.3648\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7982 - val_loss: -1.3651\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8013 - val_loss: -1.3655\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7783 - val_loss: -1.3659\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8028 - val_loss: -1.3663\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7756 - val_loss: -1.3668\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8063 - val_loss: -1.3672\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8200 - val_loss: -1.3676\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8003 - val_loss: -1.3680\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7958 - val_loss: -1.3683\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.8038 - val_loss: -1.3686\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7859 - val_loss: -1.3688\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.8099 - val_loss: -1.3691\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8002 - val_loss: -1.3693\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8090 - val_loss: -1.3694\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7997 - val_loss: -1.3695\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8032 - val_loss: -1.3696\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.7656 - val_loss: -1.3699\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.8078 - val_loss: -1.3702\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 137ms/step - loss: -1.7837 - val_loss: -1.3706\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.7789 - val_loss: -1.3709\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8010 - val_loss: -1.3713\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7918 - val_loss: -1.3716\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7985 - val_loss: -1.3720\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7912 - val_loss: -1.3723\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8226 - val_loss: -1.3727\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7953 - val_loss: -1.3730\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8017 - val_loss: -1.3733\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.7873 - val_loss: -1.3736\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7990 - val_loss: -1.3740\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8003 - val_loss: -1.3743\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.7884 - val_loss: -1.3746\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7861 - val_loss: -1.3749\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8062 - val_loss: -1.3751\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8065 - val_loss: -1.3754\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.7805 - val_loss: -1.3756\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8009 - val_loss: -1.3758\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.8013 - val_loss: -1.3760\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8071 - val_loss: -1.3762\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8140 - val_loss: -1.3762\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7782 - val_loss: -1.3763\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8043 - val_loss: -1.3764\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.8023 - val_loss: -1.3765\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 160ms/step - loss: -1.8125 - val_loss: -1.3766\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8035 - val_loss: -1.3767\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8158 - val_loss: -1.3767\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8110 - val_loss: -1.3768\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7936 - val_loss: -1.3768\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8060 - val_loss: -1.3769\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7935 - val_loss: -1.3770\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.8022 - val_loss: -1.3772\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7898 - val_loss: -1.3773\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8086 - val_loss: -1.3775\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.7973 - val_loss: -1.3776\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8141 - val_loss: -1.3777\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.7839 - val_loss: -1.3778\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8039 - val_loss: -1.3779\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8039 - val_loss: -1.3780\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8185 - val_loss: -1.3781\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8122 - val_loss: -1.3782\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8111 - val_loss: -1.3783\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7994 - val_loss: -1.3785\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.7799 - val_loss: -1.3787\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.7985 - val_loss: -1.3788\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.7954 - val_loss: -1.3790\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.8096 - val_loss: -1.3792\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.7915 - val_loss: -1.3794\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7890 - val_loss: -1.3797\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7968 - val_loss: -1.3799\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8191 - val_loss: -1.3800\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8059 - val_loss: -1.3802\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8126 - val_loss: -1.3803\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7761 - val_loss: -1.3804\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8122 - val_loss: -1.3805\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8132 - val_loss: -1.3806\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.8146 - val_loss: -1.3806\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8053 - val_loss: -1.3808\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8054 - val_loss: -1.3809\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.7891 - val_loss: -1.3810\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7950 - val_loss: -1.3812\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.7858 - val_loss: -1.3814\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8111 - val_loss: -1.3815\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8007 - val_loss: -1.3816\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8089 - val_loss: -1.3817\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.7995 - val_loss: -1.3818\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8174 - val_loss: -1.3818\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8085 - val_loss: -1.3819\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8163 - val_loss: -1.3820\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8092 - val_loss: -1.3820\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8012 - val_loss: -1.3822\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8078 - val_loss: -1.3823\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.7916 - val_loss: -1.3824\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8053 - val_loss: -1.3826\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.7962 - val_loss: -1.3828\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8056 - val_loss: -1.3830\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.8043 - val_loss: -1.3832\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8115 - val_loss: -1.3833\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8115 - val_loss: -1.3834\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7949 - val_loss: -1.3836\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8104 - val_loss: -1.3837\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8005 - val_loss: -1.3838\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8224 - val_loss: -1.3839\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8257 - val_loss: -1.3840\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8139 - val_loss: -1.3841\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8148 - val_loss: -1.3841\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8085 - val_loss: -1.3842\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8105 - val_loss: -1.3842\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7915 - val_loss: -1.3843\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8072 - val_loss: -1.3843\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8141 - val_loss: -1.3843\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8117 - val_loss: -1.3842\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8240 - val_loss: -1.3841\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.7859 - val_loss: -1.3841\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8115 - val_loss: -1.3840\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7849 - val_loss: -1.3840\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.7997 - val_loss: -1.3840\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 137ms/step - loss: -1.8094 - val_loss: -1.3840\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8001 - val_loss: -1.3840\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.8140 - val_loss: -1.3840\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8228 - val_loss: -1.3841\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.7987 - val_loss: -1.3841\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7985 - val_loss: -1.3842\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7964 - val_loss: -1.3843\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8128 - val_loss: -1.3844\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8076 - val_loss: -1.3845\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.8168 - val_loss: -1.3845\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8053 - val_loss: -1.3846\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8146 - val_loss: -1.3846\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8138 - val_loss: -1.3846\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8060 - val_loss: -1.3846\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8108 - val_loss: -1.3847\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8056 - val_loss: -1.3849\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8165 - val_loss: -1.3850\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7962 - val_loss: -1.3850\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8052 - val_loss: -1.3851\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7919 - val_loss: -1.3852\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7969 - val_loss: -1.3853\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.7907 - val_loss: -1.3854\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8100 - val_loss: -1.3855\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8165 - val_loss: -1.3855\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.7710 - val_loss: -1.3855\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8054 - val_loss: -1.3855\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.7968 - val_loss: -1.3855\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8042 - val_loss: -1.3856\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.7836 - val_loss: -1.3857\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8045 - val_loss: -1.3858\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8138 - val_loss: -1.3858\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8204 - val_loss: -1.3858\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.7879 - val_loss: -1.3858\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.8251 - val_loss: -1.3858\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8135 - val_loss: -1.3857\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7967 - val_loss: -1.3857\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8210 - val_loss: -1.3857\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8188 - val_loss: -1.3857\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8012 - val_loss: -1.3856\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8039 - val_loss: -1.3856\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8153 - val_loss: -1.3855\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7977 - val_loss: -1.3855\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8118 - val_loss: -1.3856\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.8162 - val_loss: -1.3857\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.8009 - val_loss: -1.3858\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7981 - val_loss: -1.3859\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8246 - val_loss: -1.3860\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8191 - val_loss: -1.3860\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8016 - val_loss: -1.3861\n",
      "Epoch 360/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8110 - val_loss: -1.3862\n",
      "Epoch 361/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8038 - val_loss: -1.3864\n",
      "Epoch 362/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.8059 - val_loss: -1.3865\n",
      "Epoch 363/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.7942 - val_loss: -1.3866\n",
      "Epoch 364/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.8195 - val_loss: -1.3867\n",
      "Epoch 365/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8067 - val_loss: -1.3868\n",
      "Epoch 366/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8181 - val_loss: -1.3869\n",
      "Epoch 367/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8012 - val_loss: -1.3870\n",
      "Epoch 368/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8048 - val_loss: -1.3870\n",
      "Epoch 369/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8083 - val_loss: -1.3871\n",
      "Epoch 370/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8058 - val_loss: -1.3871\n",
      "Epoch 371/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7872 - val_loss: -1.3871\n",
      "Epoch 372/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7824 - val_loss: -1.3873\n",
      "Epoch 373/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8164 - val_loss: -1.3873\n",
      "Epoch 374/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8095 - val_loss: -1.3874\n",
      "Epoch 375/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8120 - val_loss: -1.3874\n",
      "Epoch 376/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8136 - val_loss: -1.3875\n",
      "Epoch 377/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8150 - val_loss: -1.3876\n",
      "Epoch 378/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8127 - val_loss: -1.3876\n",
      "Epoch 379/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.8007 - val_loss: -1.3877\n",
      "Epoch 380/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8121 - val_loss: -1.3877\n",
      "Epoch 381/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8055 - val_loss: -1.3878\n",
      "Epoch 382/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8075 - val_loss: -1.3878\n",
      "Epoch 383/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8084 - val_loss: -1.3878\n",
      "Epoch 384/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7948 - val_loss: -1.3878\n",
      "Epoch 385/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7980 - val_loss: -1.3878\n",
      "Epoch 386/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8005 - val_loss: -1.3879\n",
      "Epoch 387/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8094 - val_loss: -1.3880\n",
      "Epoch 388/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.7880 - val_loss: -1.3881\n",
      "Epoch 389/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8127 - val_loss: -1.3882\n",
      "Epoch 390/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.7957 - val_loss: -1.3883\n",
      "Epoch 391/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8113 - val_loss: -1.3883\n",
      "Epoch 392/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8102 - val_loss: -1.3883\n",
      "Epoch 393/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.8094 - val_loss: -1.3884\n",
      "Epoch 394/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8038 - val_loss: -1.3884\n",
      "Epoch 395/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8168 - val_loss: -1.3884\n",
      "Epoch 396/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8103 - val_loss: -1.3884\n",
      "Epoch 397/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8130 - val_loss: -1.3884\n",
      "Epoch 398/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7980 - val_loss: -1.3884\n",
      "Epoch 399/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8056 - val_loss: -1.3885\n",
      "Epoch 400/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8111 - val_loss: -1.3885\n",
      "Epoch 401/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.7851 - val_loss: -1.3887\n",
      "Epoch 402/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8044 - val_loss: -1.3888\n",
      "Epoch 403/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8189 - val_loss: -1.3890\n",
      "Epoch 404/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8067 - val_loss: -1.3890\n",
      "Epoch 405/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8207 - val_loss: -1.3891\n",
      "Epoch 406/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8025 - val_loss: -1.3891\n",
      "Epoch 407/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8075 - val_loss: -1.3891\n",
      "Epoch 408/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8131 - val_loss: -1.3891\n",
      "Epoch 409/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8077 - val_loss: -1.3891\n",
      "Epoch 410/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8100 - val_loss: -1.3891\n",
      "Epoch 411/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8048 - val_loss: -1.3891\n",
      "Epoch 412/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8037 - val_loss: -1.3892\n",
      "Epoch 413/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8128 - val_loss: -1.3892\n",
      "Epoch 414/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8193 - val_loss: -1.3892\n",
      "Epoch 415/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8213 - val_loss: -1.3892\n",
      "Epoch 416/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.7992 - val_loss: -1.3892\n",
      "Epoch 417/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.8000 - val_loss: -1.3892\n",
      "Epoch 418/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8061 - val_loss: -1.3893\n",
      "Epoch 419/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8059 - val_loss: -1.3893\n",
      "Epoch 420/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8126 - val_loss: -1.3894\n",
      "Epoch 421/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8118 - val_loss: -1.3894\n",
      "Epoch 422/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8110 - val_loss: -1.3894\n",
      "Epoch 423/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8041 - val_loss: -1.3894\n",
      "Epoch 424/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7948 - val_loss: -1.3894\n",
      "Epoch 425/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8156 - val_loss: -1.3894\n",
      "Epoch 426/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8094 - val_loss: -1.3894\n",
      "Epoch 427/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.7923 - val_loss: -1.3894\n",
      "Epoch 428/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8208 - val_loss: -1.3894\n",
      "Epoch 429/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8136 - val_loss: -1.3893\n",
      "Epoch 430/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7972 - val_loss: -1.3893\n",
      "Epoch 431/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8011 - val_loss: -1.3893\n",
      "Epoch 432/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7989 - val_loss: -1.3893\n",
      "Epoch 433/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8054 - val_loss: -1.3894\n",
      "Epoch 434/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8199 - val_loss: -1.3894\n",
      "Epoch 435/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8056 - val_loss: -1.3894\n",
      "Epoch 436/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8068 - val_loss: -1.3894\n",
      "Epoch 437/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8203 - val_loss: -1.3895\n",
      "Epoch 438/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8122 - val_loss: -1.3895\n",
      "Epoch 439/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8125 - val_loss: -1.3895\n",
      "Epoch 440/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.8090 - val_loss: -1.3895\n",
      "Epoch 441/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8056 - val_loss: -1.3895\n",
      "Epoch 442/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8173 - val_loss: -1.3895\n",
      "Epoch 443/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8067 - val_loss: -1.3895\n",
      "Epoch 444/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8052 - val_loss: -1.3896\n",
      "Epoch 445/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.8165 - val_loss: -1.3896\n",
      "Epoch 446/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -1.7994 - val_loss: -1.3896\n",
      "Epoch 447/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -1.8171 - val_loss: -1.3896\n",
      "Epoch 448/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8103 - val_loss: -1.3896\n",
      "Epoch 449/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.8032 - val_loss: -1.3897\n",
      "Epoch 450/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8115 - val_loss: -1.3898\n",
      "Epoch 451/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8172 - val_loss: -1.3898\n",
      "Epoch 452/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8157 - val_loss: -1.3899\n",
      "Epoch 453/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8135 - val_loss: -1.3899\n",
      "Epoch 454/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.7922 - val_loss: -1.3899\n",
      "Epoch 455/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.8125 - val_loss: -1.3900\n",
      "Epoch 456/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8142 - val_loss: -1.3900\n",
      "Epoch 457/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8067 - val_loss: -1.3900\n",
      "Epoch 458/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.8013 - val_loss: -1.3901\n",
      "Epoch 459/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.7877 - val_loss: -1.3902\n",
      "Epoch 460/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8121 - val_loss: -1.3903\n",
      "Epoch 461/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8164 - val_loss: -1.3904\n",
      "Epoch 462/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8034 - val_loss: -1.3905\n",
      "Epoch 463/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.7982 - val_loss: -1.3906\n",
      "Epoch 464/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8072 - val_loss: -1.3907\n",
      "Epoch 465/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.7843 - val_loss: -1.3908\n",
      "Epoch 466/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.7938 - val_loss: -1.3909\n",
      "Epoch 467/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8112 - val_loss: -1.3910\n",
      "Epoch 468/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7988 - val_loss: -1.3910\n",
      "Epoch 469/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8164 - val_loss: -1.3910\n",
      "Epoch 470/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8206 - val_loss: -1.3910\n",
      "Epoch 471/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8287 - val_loss: -1.3909\n",
      "Epoch 472/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8194 - val_loss: -1.3909\n",
      "Epoch 473/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8013 - val_loss: -1.3908\n",
      "Epoch 474/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8192 - val_loss: -1.3908\n",
      "Epoch 475/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.8149 - val_loss: -1.3908\n",
      "Epoch 476/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8096 - val_loss: -1.3908\n",
      "Epoch 477/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8170 - val_loss: -1.3907\n",
      "Epoch 478/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7998 - val_loss: -1.3907\n",
      "Epoch 479/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8098 - val_loss: -1.3906\n",
      "Epoch 480/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8197 - val_loss: -1.3906\n",
      "Epoch 481/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8043 - val_loss: -1.3907\n",
      "Epoch 482/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8164 - val_loss: -1.3907\n",
      "Epoch 483/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8007 - val_loss: -1.3907\n",
      "Epoch 484/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8091 - val_loss: -1.3907\n",
      "Epoch 485/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.7976 - val_loss: -1.3907\n",
      "Epoch 486/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8093 - val_loss: -1.3908\n",
      "Epoch 487/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7994 - val_loss: -1.3908\n",
      "Epoch 488/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.8133 - val_loss: -1.3908\n",
      "Epoch 489/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8252 - val_loss: -1.3908\n",
      "Epoch 490/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8011 - val_loss: -1.3908\n",
      "Epoch 491/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8220 - val_loss: -1.3907\n",
      "Epoch 492/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7974 - val_loss: -1.3907\n",
      "Epoch 493/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8074 - val_loss: -1.3907\n",
      "Epoch 494/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8189 - val_loss: -1.3907\n",
      "Epoch 495/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.8202 - val_loss: -1.3906\n",
      "Epoch 496/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8130 - val_loss: -1.3905\n",
      "Epoch 497/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7962 - val_loss: -1.3905\n",
      "Epoch 498/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8070 - val_loss: -1.3905\n",
      "Epoch 499/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8025 - val_loss: -1.3905\n",
      "Epoch 500/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.8094 - val_loss: -1.3905\n",
      "Epoch 501/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8124 - val_loss: -1.3905\n",
      "Epoch 502/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -1.8138 - val_loss: -1.3906\n",
      "Epoch 503/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8032 - val_loss: -1.3906\n",
      "Epoch 504/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.8149 - val_loss: -1.3906\n",
      "Epoch 505/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.8114 - val_loss: -1.3906\n",
      "Epoch 506/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8106 - val_loss: -1.3906\n",
      "Epoch 507/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8205 - val_loss: -1.3905\n",
      "Epoch 508/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8220 - val_loss: -1.3904\n",
      "Epoch 509/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8170 - val_loss: -1.3904\n",
      "Epoch 510/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.7954 - val_loss: -1.3903\n",
      "Epoch 511/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8114 - val_loss: -1.3903\n",
      "Epoch 512/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8112 - val_loss: -1.3903\n",
      "Epoch 513/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8111 - val_loss: -1.3902\n",
      "Epoch 514/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8169 - val_loss: -1.3902\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00514: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_11 (Softmax)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 492ms/step - loss: -1.3081 - val_loss: -0.4250\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.2796 - val_loss: -0.4241\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.3369 - val_loss: -0.4231\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.2158 - val_loss: -0.4220\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.2513 - val_loss: -0.4209\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2269 - val_loss: -0.4198\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.3016 - val_loss: -0.4187\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.3582 - val_loss: -0.4175\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.2890 - val_loss: -0.4164\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 139ms/step - loss: -1.3268 - val_loss: -0.4152\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.4015 - val_loss: -0.4141\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.3525 - val_loss: -0.4129\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.3695 - val_loss: -0.4117\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.4034 - val_loss: -0.4106\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3370 - val_loss: -0.4094\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.3574 - val_loss: -0.4083\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.3332 - val_loss: -0.4071\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3650 - val_loss: -0.4059\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.3749 - val_loss: -0.4047\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -1.3637 - val_loss: -0.4036\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.4507 - val_loss: -0.4025\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -1.3202 - val_loss: -0.4014\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -1.4188 - val_loss: -0.4004\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.4869 - val_loss: -0.3994\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.4134 - val_loss: -0.3985\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4717 - val_loss: -0.3976\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.4334 - val_loss: -0.3967\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.5211 - val_loss: -0.3959\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.4606 - val_loss: -0.3951\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.4524 - val_loss: -0.3943\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6186 - val_loss: -0.3936\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.4711 - val_loss: -0.3929\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 147ms/step - loss: -1.5457 - val_loss: -0.3922\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5333 - val_loss: -0.3915\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.5633 - val_loss: -0.3909\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.5505 - val_loss: -0.3903\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.5056 - val_loss: -0.3898\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5270 - val_loss: -0.3892\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.5588 - val_loss: -0.3886\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.5801 - val_loss: -0.3881\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.5777 - val_loss: -0.3876\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.5911 - val_loss: -0.3873\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.5977 - val_loss: -0.3871\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.6601 - val_loss: -0.3869\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6184 - val_loss: -0.3867\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6271 - val_loss: -0.3864\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.5527 - val_loss: -0.3861\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6579 - val_loss: -0.3859\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6453 - val_loss: -0.3857\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6977 - val_loss: -0.3855\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.6740 - val_loss: -0.3854\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'assets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10881/1226690610.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mportfolio_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mweights_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'assets' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and predict\n",
    "1. Get the rolling window data from of T-250\n",
    "2. scale and split data to prepare for neural network input, Xtrain, Xtest, ytrain, ytest, Xpred\n",
    "3. Train the model with Xtrain,Xtest,ytrain,ytest\n",
    "4. Predict weight using Xpred\n",
    "5. Save weight for that T\n",
    "6. Repeat 1 - 5 for every T(rebal date)\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=50, verbose=2, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# we will start the prediction from year 2020\n",
    "pred_dates = [d for d in REBAL_DATES if d.year == 2020]\n",
    "\n",
    "def get_window(df,pred_date):\n",
    "    temp = df.copy()\n",
    "    pred_date_ind = np.where(temp.index == pred_date)[0][0]\n",
    "    window = temp.iloc[((pred_date_ind+1)-WINDOW):(pred_date_ind+1)]\n",
    "    return window\n",
    "\n",
    "def create_input(df):\n",
    "    data = df.copy()\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    transformer = scaler.fit(data)\n",
    "    scaled_data = pd.DataFrame(transformer.transform(data), columns=data.columns)\n",
    "    test_data = scaled_data.iloc[:100]\n",
    "    train_data = scaled_data.iloc[100:]\n",
    "    \n",
    "    n_test_sample = 100 - LOOKBACK - HORIZON\n",
    "    n_train_sample = 100 - LOOKBACK - HORIZON\n",
    "    \n",
    "    assets = scaled_data.columns.levels[0].tolist()\n",
    "    channels = scaled_data.columns.levels[1].tolist()\n",
    "    \n",
    "    n_channels = len(data.columns.levels[1])\n",
    "    channel = scaled_data.columns.levels[1][0]\n",
    "    for index, asset in enumerate(assets):\n",
    "        if (index == 0):\n",
    "            test_temp = test_data[asset, channel].values.reshape(-1,1)\n",
    "            train_temp = train_data[asset, channel].values.reshape(-1,1)\n",
    "            full_temp = scaled_data[asset, channel].values.reshape(-1,1)\n",
    "        else:\n",
    "            test_temp = np.concatenate((test_temp, test_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            train_temp = np.concatenate((train_temp, train_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            full_temp = np.concatenate((full_temp, scaled_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            \n",
    "    data_block_test = test_temp.reshape(n_channels,test_temp.shape[0],test_temp.shape[1])\n",
    "    data_block_train = train_temp.reshape(n_channels,train_temp.shape[0],train_temp.shape[1])\n",
    "    data_block = full_temp.reshape(n_channels,full_temp.shape[0],full_temp.shape[1])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, X_pred = [], [], [], [], []\n",
    "    \n",
    "    for i in range(n_train_sample):\n",
    "        X_train.append(data_block_train[:,i:(i+LOOKBACK),:])\n",
    "        y_train.append(data_block_train[:,(i+LOOKBACK):(i + LOOKBACK + HORIZON),:])\n",
    "        \n",
    "    for i in range(n_train_sample):\n",
    "        X_test.append(data_block_test[:,i:(i+LOOKBACK),:])\n",
    "        y_test.append(data_block_test[:,(i+LOOKBACK):(i + LOOKBACK + HORIZON),:])\n",
    "        \n",
    "    X_pred.append(data_block_test[:,-LOOKBACK:,:])\n",
    "        \n",
    "    return np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), np.array(X_pred)\n",
    "\n",
    "\"\"\"Here will start train and predict rolling window, getting weight for each rebalance date\"\"\"\n",
    "portfolio_weights = []\n",
    "for d in pred_dates:\n",
    "    data = get_window(df, d)\n",
    "    \n",
    "    # split 100 sample for train, 100 sample for test. Since we have 190 samples, some test data will overlap\n",
    "    X_train, y_train, X_test, y_test, X_pred = create_input(data)\n",
    "    print(f\"X_train shape = {X_train.shape}\")\n",
    "    print(f\"y_train shape = {y_train.shape}\")\n",
    "    print(f\"X_test shape = {X_test.shape}\")\n",
    "    print(f\"y_test shape = {y_test.shape}\")\n",
    "    print(f\"X_pred shape = {X_pred.shape}\")\n",
    "    \n",
    "    model = build_model(\n",
    "    n_assets=N_ASSETS,\n",
    "    input_shape = (N_FEATURES),\n",
    "    dropout=0.5\n",
    "    )\n",
    "\n",
    "    my_loss = make_my_loss([0.1, 0.2, 0.7])\n",
    "    model.compile(\n",
    "        loss= my_loss,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    X_train_in = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_in = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_in,\n",
    "        y_train,\n",
    "        validation_data=(X_test_in, y_test),\n",
    "        epochs=N_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[es]\n",
    "        )\n",
    "    \n",
    "    X_pred_in = X_pred.reshape(X_pred.shape[0], -1)\n",
    "    weights = model.predict(X_pred_in)\n",
    "    portfolio_weights.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6c59b9f-5e06-4374-b414-4b37bdc4c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "portfolio_weights = np.array(portfolio_weights)\n",
    "portfolio_weights = portfolio_weights.flatten().reshape(len(pred_dates),-1)\n",
    "weights_df = pd.DataFrame(portfolio_weights, columns=ASSETS, index=pred_dates)\n",
    "\n",
    "background_color = '#fbfbfb'\n",
    "fig = plt.figure(figsize=(12, 8), facecolor=background_color)\n",
    "stack_list = []\n",
    "for asset in ASSETS:\n",
    "    stack_list.append(weights_df[asset])\n",
    "        \n",
    "plt.stackplot(weights_df.index, stack_list, labels=ASSETS)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7722d-47a6-4ec2-926b-9e724cb3e6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
