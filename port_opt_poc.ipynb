{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6528bf7d-6352-48a7-9e9d-a40af3edc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "# plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a814ec-5b4c-45f5-9150-949957c778cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '~/deepdow_poc'\n",
    "raw_data = root_path + '/raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9a2bcb-32ae-4984-8feb-a53add39d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will load the dataset for each assets, filter only the adj close column to use as our feature and concat the assets into same dataframe\n",
    "\"\"\"\n",
    "\n",
    "def load_prepare_data(assets_list, suffix, col=['Adj Close']): #, 'Volume']):\n",
    "    assets_dict = {}\n",
    "    temp_list = []\n",
    "    \n",
    "    if suffix != None:\n",
    "        for asset in assets_list:\n",
    "            df_temp = pd.read_csv(raw_data + asset + suffix +'.csv')\n",
    "            df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "            df_temp.set_index('Date', inplace=True)\n",
    "            df_temp = pd.DataFrame(df_temp[col])\n",
    "\n",
    "            assets_dict[asset] = df_temp\n",
    "            temp_list.append(df_temp)\n",
    "    else:\n",
    "        for asset in assets_list:\n",
    "            df_temp = pd.read_csv(raw_data + asset +'.csv')\n",
    "            df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "            df_temp.set_index('Date', inplace=True)\n",
    "            df_temp = pd.DataFrame(df_temp[col])\n",
    "\n",
    "            assets_dict[asset] = df_temp\n",
    "            temp_list.append(df_temp)\n",
    "            \n",
    "    df = pd.concat(temp_list, keys=assets_list, axis=1)\n",
    "    return assets_dict, df\n",
    "\n",
    "\n",
    "ASSETS = ['AAPL', 'TSLA', 'SHOP', 'HLFNX','VGT']\n",
    "_,df_1 = load_prepare_data(ASSETS, '_train')\n",
    "_,df_2 = load_prepare_data(ASSETS, '_test')\n",
    "df = pd.concat([df_1, df_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623c0df0-2291-431e-9ed9-457d125f281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the data of the asset price to daily returns\n",
    "\"\"\"\n",
    "\n",
    "def to_return(data, col, log=True):\n",
    "    df = data.copy()\n",
    "    for asset in df.columns.levels[0]:\n",
    "        if log:\n",
    "            df[asset, 'Adj Close'] = df[asset, 'Adj Close'].pct_change().apply(lambda x: np.log(1+x))\n",
    "        else:\n",
    "            df[asset, 'Adj Close'] = df[asset, 'Adj Close'].pct_change()\n",
    "            \n",
    "    return df.iloc[1:]\n",
    "\n",
    "df_1 = to_return(df_1, 'Adj Close')\n",
    "df_2 = to_return(df_2, 'Adj Close')\n",
    "df = to_return(df, 'Adj Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b53b4f4-7227-4a46-b06a-945bfdbb7294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>SHOP</th>\n",
       "      <th>HLFNX</th>\n",
       "      <th>VGT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>-0.104924</td>\n",
       "      <td>-0.031978</td>\n",
       "      <td>-0.058433</td>\n",
       "      <td>-0.020795</td>\n",
       "      <td>-0.050685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>0.041803</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>0.061771</td>\n",
       "      <td>0.035463</td>\n",
       "      <td>0.042565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.009570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>0.016839</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.021629</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.012405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>-0.007000</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>-0.063956</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>-0.009144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-24</th>\n",
       "      <td>0.007683</td>\n",
       "      <td>0.024150</td>\n",
       "      <td>0.022745</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.006625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>0.035141</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>-0.066163</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.007003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>-0.013404</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.021177</td>\n",
       "      <td>-0.006415</td>\n",
       "      <td>-0.006635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>-0.008563</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      TSLA      SHOP     HLFNX       VGT\n",
       "           Adj Close Adj Close Adj Close Adj Close Adj Close\n",
       "Date                                                        \n",
       "2019-01-03 -0.104924 -0.031978 -0.058433 -0.020795 -0.050685\n",
       "2019-01-04  0.041803  0.056094  0.061771  0.035463  0.042565\n",
       "2019-01-07 -0.002228  0.052935  0.044830  0.006221  0.011111\n",
       "2019-01-08  0.018884  0.001164  0.007246  0.003096  0.009570\n",
       "2019-01-09  0.016839  0.009438  0.021629  0.008209  0.012405\n",
       "...              ...       ...       ...       ...       ...\n",
       "2020-12-23 -0.007000  0.008769 -0.063956  0.002863 -0.009144\n",
       "2020-12-24  0.007683  0.024150  0.022745  0.004635  0.006625\n",
       "2020-12-28  0.035141  0.002897 -0.066163  0.001422  0.007003\n",
       "2020-12-29 -0.013404  0.003459  0.021177 -0.006415 -0.006635\n",
       "2020-12-30 -0.008563  0.042321 -0.007376  0.004637  0.001331\n",
       "\n",
       "[502 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13299300-c31c-48f7-ac16-a9477159637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Establish constants\n",
    "1. Rebalance freq = BMS first business day of month\n",
    "2. Lookback = number of historical data in unit of days that \n",
    "   deep learning use to learn to predict weight for the horizon\n",
    "3. Horizon = days that user hold before rebalance. Corresponds to the Rebalance freq. \n",
    "    20 days = n_business_day in 1 month\n",
    "\"\"\"\n",
    "\n",
    "LOOKBACK = 40\n",
    "HORIZON = 20\n",
    "WINDOW = 250\n",
    "N_SAMPLES = WINDOW - LOOKBACK - HORIZON\n",
    "N_ASSETS = len(df.columns.levels[0])\n",
    "N_FEATURES = LOOKBACK * N_ASSETS\n",
    "\n",
    "\"\"\"\n",
    "first day of the month in the historical data\n",
    "go through each element in list, if month changes, that is the first day of month, assuming time is ordered in the list\n",
    "\"\"\"\n",
    "def get_first_date(datelist):\n",
    "    rebal_dates = []\n",
    "    for index,d in enumerate(df.index):\n",
    "        if index==0:\n",
    "            rebal_dates.append(d)\n",
    "            current_month = d.month\n",
    "        else:\n",
    "            if d.month != current_month:\n",
    "                rebal_dates.append(d)\n",
    "                current_month = d.month\n",
    "            else:\n",
    "                continue\n",
    "    return rebal_dates\n",
    "\n",
    "REBAL_DATES = get_first_date(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4faf5ea9-504a-48d3-b1eb-451d51ce7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train and optimize.\n",
    "Training and optimize will be done to 1 rolling window to predict the weight to use for the horizon.\n",
    "Rolling window will start from T(rebalance date) and go backwards\n",
    "\"\"\"\n",
    "\n",
    "# first will define the loss functions\n",
    "import tensorflow as tf\n",
    "\n",
    "def portfolio_returns(weights, y):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    --------\n",
    "    weights = the predicted values output from neural network. (n_samples, n_asset)\n",
    "    y = test data (n_samples, horizon, n_asset) use for loss calculation\n",
    "    \"\"\"\n",
    "    # exp to reverse log transform\n",
    "    y = tf.math.exp(y) - 1\n",
    "    n_samples = tf.shape(y)[0]\n",
    "    horizon = tf.shape(y)[1]\n",
    "    n_assets = tf.shape(y)[2]\n",
    "    \n",
    "    \"\"\" \n",
    "    reshape the weight into (n_samples, horizon, n_asset) dimension\n",
    "    first we have (n_samples, n_assets) weight output from neural network output\n",
    "    we want to repeat the vector horizon amount of time\n",
    "    \"\"\"\n",
    "    weights_ = tf.tile(tf.reshape(weights, [n_samples, 1, n_assets]), (1, horizon, 1))\n",
    "    \n",
    "    \"\"\"\n",
    "    rebalance : now will always be false (same for all time steps in the horizon)\n",
    "        If True, each timestep the weights are adjusted to be equal to be equal to the original ones. Note that\n",
    "        this assumes that we tinker with the portfolio. If False, the portfolio evolves untouched.\n",
    "    \"\"\"\n",
    "    rebalance=False\n",
    "    \n",
    "    if not rebalance:\n",
    "        weights_unscaled = tf.math.cumprod((1 + y),1)[:, :-1, :] * weights_[:, 1:, :]\n",
    "        \n",
    "        output_list = []\n",
    "        output_list.append(weights_[:, 0, :])\n",
    "        \n",
    "        remaining = weights_unscaled / tf.math.reduce_sum(weights_unscaled, 2, keepdims=True)\n",
    "        rows = remaining.get_shape()\n",
    "        for row in range(rows[1]):\n",
    "            output_list.append(remaining[:, row,:])\n",
    "        \n",
    "        new_weights = tf.stack(output_list, axis=1)\n",
    "\n",
    "    out = tf.math.reduce_sum((y * new_weights),-1)\n",
    "    \n",
    "    # shape (n_samples, horizon) representing per timestep portfolio returns\n",
    "    return out\n",
    "\n",
    "def MeanReturns(y, weights):#, rebalance=False):\n",
    "    \"\"\"Negative mean returns\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])#, rebalance=rebalance)\n",
    "    \n",
    "    \"\"\"Tensor of shape `(n_samples,)` representing the per sample negative mean returns.\"\"\"\n",
    "    # neg_mean = tf.math.exp(prets)\n",
    "    mean = tf.math.reduce_mean(prets, 1)\n",
    "    neg_mean = mean * -1\n",
    "    return neg_mean\n",
    "\n",
    "def SharpeRatio(y, weights, rf=0.01, eps=1e-4):\n",
    "    \"\"\"Negative Sharpe ratio\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])#, rebalance=rebalance)\n",
    "    \n",
    "    \"\"\"Tensor of shape `(n_samples,)` representing the per sample negative mean returns.\"\"\"\n",
    "    sharpe = ((tf.math.reduce_mean(prets, 1) - rf)/(tf.math.reduce_std(prets, 1) + eps))\n",
    "    neg_sharpe = sharpe * -1\n",
    "    return neg_sharpe\n",
    "\n",
    "def LargestWeight(y, weights):\n",
    "    \"\"\"Penalize low diversity, or over powering weight in 1 asset, lower = better\"\"\"\n",
    "    return tf.reduce_max(weights, 1)[0]\n",
    "\n",
    "def StandardDeviation(y, weights):\n",
    "    \"\"\"Returns std dev of portfolio return of the horizon, lower = better\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])\n",
    "    return tf.math.reduce_std(prets, 1)\n",
    "\n",
    "def make_my_loss(alpha):\n",
    "    def my_loss(y, weights):\n",
    "        \"\"\"Combination of different loss functions for the final loss calculation\"\"\"\n",
    "        return (alpha[0] * MeanReturns(y, weights)\n",
    "                + alpha[1] * SharpeRatio(y, weights)\n",
    "                + alpha[2] * StandardDeviation(y, weights)\n",
    "               )\n",
    "    return my_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b46bf7b-9893-43e5-81bf-d42420250a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the model archtecture\n",
    "-----------------------------\n",
    "This is a simple feedfoward model that the input is the flattened array of (n_samples, n_features)\n",
    "n_features = n_assets * horizon\n",
    "\n",
    "The output is softmax of dimension n_assets. Softmax --> sum of all weight = 1\n",
    "\"\"\"\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Softmax\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "def build_model(n_assets, input_shape, dropout):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    x = Dropout(dropout)(x)\n",
    "    weights = Dense(n_assets, activation=\"linear\")(x)\n",
    "    outputs = Softmax()(weights)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7921a069-5130-48b4-b2ea-a01332699e20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 15:45:20.684171: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-03 15:45:20.825124: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 539ms/step - loss: -0.7719 - val_loss: -1.0597\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.7482 - val_loss: -1.0601\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.7679 - val_loss: -1.0605\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.7860 - val_loss: -1.0609\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.7718 - val_loss: -1.0613\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.7660 - val_loss: -1.0617\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.7744 - val_loss: -1.0621\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7714 - val_loss: -1.0624\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.7962 - val_loss: -1.0626\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.7816 - val_loss: -1.0627\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.8448 - val_loss: -1.0627\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 132ms/step - loss: -0.7815 - val_loss: -1.0625\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.7998 - val_loss: -1.0621\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.8143 - val_loss: -1.0616\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.8080 - val_loss: -1.0611\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.8218 - val_loss: -1.0606\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.8095 - val_loss: -1.0599\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8355 - val_loss: -1.0591\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.8354 - val_loss: -1.0582\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.7896 - val_loss: -1.0571\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.8296 - val_loss: -1.0561\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.8616 - val_loss: -1.0549\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.8306 - val_loss: -1.0536\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.8239 - val_loss: -1.0522\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.8467 - val_loss: -1.0506\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.8188 - val_loss: -1.0490\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.8426 - val_loss: -1.0473\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.8240 - val_loss: -1.0455\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.8567 - val_loss: -1.0437\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.8629 - val_loss: -1.0420\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.8659 - val_loss: -1.0403\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.8620 - val_loss: -1.0385\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.8958 - val_loss: -1.0367\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.8776 - val_loss: -1.0348\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.8256 - val_loss: -1.0329\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.8814 - val_loss: -1.0310\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.8710 - val_loss: -1.0292\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.8640 - val_loss: -1.0273\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.8674 - val_loss: -1.0255\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.8716 - val_loss: -1.0237\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.8662 - val_loss: -1.0217\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.8750 - val_loss: -1.0198\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.8926 - val_loss: -1.0178\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8701 - val_loss: -1.0156\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.8820 - val_loss: -1.0133\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.8810 - val_loss: -1.0111\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.8882 - val_loss: -1.0089\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.8808 - val_loss: -1.0067\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.9005 - val_loss: -1.0045\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.8967 - val_loss: -1.0023\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.8788 - val_loss: -1.0002\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.9021 - val_loss: -0.9982\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.8806 - val_loss: -0.9962\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.8796 - val_loss: -0.9942\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.8815 - val_loss: -0.9922\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -0.8734 - val_loss: -0.9902\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.9105 - val_loss: -0.9883\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00057: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 564ms/step - loss: -0.7241 - val_loss: -0.7219\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 145ms/step - loss: -0.7372 - val_loss: -0.7212\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 151ms/step - loss: -0.7218 - val_loss: -0.7203\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.7415 - val_loss: -0.7193\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 153ms/step - loss: -0.7477 - val_loss: -0.7182\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.7436 - val_loss: -0.7170\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.7480 - val_loss: -0.7156\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.7660 - val_loss: -0.7142\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.7446 - val_loss: -0.7126\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.7366 - val_loss: -0.7109\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.7491 - val_loss: -0.7090\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.7617 - val_loss: -0.7070\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.7863 - val_loss: -0.7051\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7762 - val_loss: -0.7031\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.7659 - val_loss: -0.7011\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.7769 - val_loss: -0.6990\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7663 - val_loss: -0.6969\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.7582 - val_loss: -0.6946\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.7746 - val_loss: -0.6923\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.7784 - val_loss: -0.6901\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 148ms/step - loss: -0.7807 - val_loss: -0.6880\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.7997 - val_loss: -0.6858\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.7862 - val_loss: -0.6838\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.7844 - val_loss: -0.6821\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -0.7793 - val_loss: -0.6806\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.7723 - val_loss: -0.6789\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.7845 - val_loss: -0.6772\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.8087 - val_loss: -0.6754\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.8024 - val_loss: -0.6737\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.7825 - val_loss: -0.6716\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.7874 - val_loss: -0.6698\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.7989 - val_loss: -0.6677\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.8005 - val_loss: -0.6656\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.8164 - val_loss: -0.6637\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.7890 - val_loss: -0.6622\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8171 - val_loss: -0.6606\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -0.7906 - val_loss: -0.6587\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.8065 - val_loss: -0.6568\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.8216 - val_loss: -0.6552\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.8054 - val_loss: -0.6538\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8105 - val_loss: -0.6528\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.8050 - val_loss: -0.6519\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 150ms/step - loss: -0.7894 - val_loss: -0.6506\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.8260 - val_loss: -0.6495\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: -0.8302 - val_loss: -0.6486\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.8395 - val_loss: -0.6480\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.8044 - val_loss: -0.6472\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.8077 - val_loss: -0.6466\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -0.8305 - val_loss: -0.6461\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.8336 - val_loss: -0.6458\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.8297 - val_loss: -0.6455\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 767ms/step - loss: -1.0736 - val_loss: -0.8843\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.1056 - val_loss: -0.8870\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.1635 - val_loss: -0.8900\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.0896 - val_loss: -0.8931\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.1320 - val_loss: -0.8961\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.1335 - val_loss: -0.8991\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.1532 - val_loss: -0.9021\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.1437 - val_loss: -0.9050\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.1039 - val_loss: -0.9076\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.1505 - val_loss: -0.9104\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.1781 - val_loss: -0.9133\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.1251 - val_loss: -0.9162\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.1307 - val_loss: -0.9190\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.1379 - val_loss: -0.9218\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.1288 - val_loss: -0.9247\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.1590 - val_loss: -0.9276\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.1739 - val_loss: -0.9305\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.1901 - val_loss: -0.9335\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.1667 - val_loss: -0.9366\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -1.1442 - val_loss: -0.9396\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.1820 - val_loss: -0.9429\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.1792 - val_loss: -0.9460\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.2029 - val_loss: -0.9493\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.1486 - val_loss: -0.9526\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.2279 - val_loss: -0.9558\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.1610 - val_loss: -0.9590\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.1745 - val_loss: -0.9624\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.1917 - val_loss: -0.9658\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.1632 - val_loss: -0.9692\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.1555 - val_loss: -0.9726\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.2217 - val_loss: -0.9760\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.1951 - val_loss: -0.9796\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.2196 - val_loss: -0.9830\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.1896 - val_loss: -0.9862\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.1706 - val_loss: -0.9889\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.2368 - val_loss: -0.9914\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.2427 - val_loss: -0.9939\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.2459 - val_loss: -0.9964\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.2474 - val_loss: -0.9990\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.2655 - val_loss: -1.0016\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.2551 - val_loss: -1.0040\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.2112 - val_loss: -1.0063\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.2273 - val_loss: -1.0087\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2385 - val_loss: -1.0109\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.2712 - val_loss: -1.0131\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.2362 - val_loss: -1.0152\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.2640 - val_loss: -1.0174\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.2181 - val_loss: -1.0196\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.2718 - val_loss: -1.0218\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.2494 - val_loss: -1.0238\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.2107 - val_loss: -1.0257\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.2471 - val_loss: -1.0275\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.2984 - val_loss: -1.0298\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.2066 - val_loss: -1.0322\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.2692 - val_loss: -1.0343\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.2865 - val_loss: -1.0360\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.2786 - val_loss: -1.0378\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.2864 - val_loss: -1.0403\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.2796 - val_loss: -1.0431\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.3073 - val_loss: -1.0460\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.2432 - val_loss: -1.0488\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.2931 - val_loss: -1.0515\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.2990 - val_loss: -1.0541\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.2781 - val_loss: -1.0561\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.2645 - val_loss: -1.0579\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.2515 - val_loss: -1.0595\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.3012 - val_loss: -1.0613\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3011 - val_loss: -1.0630\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.2470 - val_loss: -1.0642\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.2951 - val_loss: -1.0645\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.2688 - val_loss: -1.0647\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.2820 - val_loss: -1.0651\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3056 - val_loss: -1.0656\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.2957 - val_loss: -1.0661\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.2842 - val_loss: -1.0670\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -1.2956 - val_loss: -1.0685\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3346 - val_loss: -1.0704\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -1.2989 - val_loss: -1.0722\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -1.3083 - val_loss: -1.0736\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.3321 - val_loss: -1.0750\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3158 - val_loss: -1.0764\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.3336 - val_loss: -1.0780\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2884 - val_loss: -1.0797\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.2817 - val_loss: -1.0810\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3472 - val_loss: -1.0824\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3381 - val_loss: -1.0841\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.3418 - val_loss: -1.0857\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.2717 - val_loss: -1.0874\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.3082 - val_loss: -1.0888\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.3346 - val_loss: -1.0896\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.3234 - val_loss: -1.0899\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.2957 - val_loss: -1.0904\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.3421 - val_loss: -1.0907\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.3080 - val_loss: -1.0910\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.3046 - val_loss: -1.0917\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.2691 - val_loss: -1.0929\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3426 - val_loss: -1.0941\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.3296 - val_loss: -1.0953\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3023 - val_loss: -1.0965\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.2942 - val_loss: -1.0977\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.2912 - val_loss: -1.0990\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.2906 - val_loss: -1.1000\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.2748 - val_loss: -1.1004\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.3302 - val_loss: -1.1007\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.2601 - val_loss: -1.1012\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3539 - val_loss: -1.1016\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.3203 - val_loss: -1.1018\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3422 - val_loss: -1.1020\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3341 - val_loss: -1.1024\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.3099 - val_loss: -1.1030\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3255 - val_loss: -1.1038\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.3103 - val_loss: -1.1044\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.3430 - val_loss: -1.1049\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.3085 - val_loss: -1.1053\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.2836 - val_loss: -1.1063\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.2807 - val_loss: -1.1075\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.3482 - val_loss: -1.1090\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.3732 - val_loss: -1.1102\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3083 - val_loss: -1.1105\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.3137 - val_loss: -1.1107\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.3197 - val_loss: -1.1107\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3454 - val_loss: -1.1104\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 138ms/step - loss: -1.3352 - val_loss: -1.1105\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.3321 - val_loss: -1.1110\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3436 - val_loss: -1.1115\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.3173 - val_loss: -1.1118\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.2972 - val_loss: -1.1115\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3721 - val_loss: -1.1111\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.3199 - val_loss: -1.1109\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3910 - val_loss: -1.1106\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.3440 - val_loss: -1.1103\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.3721 - val_loss: -1.1098\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.3219 - val_loss: -1.1090\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.3063 - val_loss: -1.1083\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.3473 - val_loss: -1.1073\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.3322 - val_loss: -1.1061\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.3359 - val_loss: -1.1050\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 158ms/step - loss: -1.3408 - val_loss: -1.1040\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3421 - val_loss: -1.1032\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3224 - val_loss: -1.1024\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3275 - val_loss: -1.1015\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.3732 - val_loss: -1.1009\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.3423 - val_loss: -1.1004\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.3311 - val_loss: -1.0999\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.4047 - val_loss: -1.0992\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3377 - val_loss: -1.0987\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.3235 - val_loss: -1.0987\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3326 - val_loss: -1.0991\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.3041 - val_loss: -1.0996\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3737 - val_loss: -1.0999\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.3049 - val_loss: -1.1002\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.3039 - val_loss: -1.1005\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.3023 - val_loss: -1.1010\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3430 - val_loss: -1.1010\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3617 - val_loss: -1.1010\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3682 - val_loss: -1.1009\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3295 - val_loss: -1.1004\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.3498 - val_loss: -1.0999\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.3634 - val_loss: -1.1000\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.3379 - val_loss: -1.1000\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3369 - val_loss: -1.1002\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3603 - val_loss: -1.1004\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.3296 - val_loss: -1.1003\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.3861 - val_loss: -1.1004\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3152 - val_loss: -1.1008\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.3972 - val_loss: -1.1016\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.2919 - val_loss: -1.1019\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.3000 - val_loss: -1.1019\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.3115 - val_loss: -1.1021\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3434 - val_loss: -1.1024\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.3380 - val_loss: -1.1024\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.3580 - val_loss: -1.1028\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3199 - val_loss: -1.1036\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3251 - val_loss: -1.1043\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3306 - val_loss: -1.1052\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00175: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_3 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 502ms/step - loss: -2.4555 - val_loss: -1.9610\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -2.4995 - val_loss: -1.9716\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.5788 - val_loss: -1.9822\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.5588 - val_loss: -1.9928\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.5095 - val_loss: -2.0030\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -2.4294 - val_loss: -2.0131\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.5453 - val_loss: -2.0233\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.5543 - val_loss: -2.0333\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.4975 - val_loss: -2.0434\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.5192 - val_loss: -2.0534\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.5071 - val_loss: -2.0633\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.4871 - val_loss: -2.0729\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.6366 - val_loss: -2.0825\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.5641 - val_loss: -2.0918\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.6878 - val_loss: -2.1012\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.6521 - val_loss: -2.1104\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.7740 - val_loss: -2.1197\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.6166 - val_loss: -2.1289\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.5977 - val_loss: -2.1381\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -2.5961 - val_loss: -2.1472\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.7109 - val_loss: -2.1565\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.6142 - val_loss: -2.1658\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.6761 - val_loss: -2.1748\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -2.7138 - val_loss: -2.1837\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.7198 - val_loss: -2.1925\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.7742 - val_loss: -2.2012\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.7652 - val_loss: -2.2100\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.7948 - val_loss: -2.2189\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.8457 - val_loss: -2.2279\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.8899 - val_loss: -2.2368\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.8150 - val_loss: -2.2457\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.7991 - val_loss: -2.2546\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.7992 - val_loss: -2.2635\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.9497 - val_loss: -2.2726\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.8920 - val_loss: -2.2817\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.9792 - val_loss: -2.2908\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -3.0250 - val_loss: -2.3002\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.9127 - val_loss: -2.3096\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.9791 - val_loss: -2.3189\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -3.0428 - val_loss: -2.3284\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.0039 - val_loss: -2.3380\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.9438 - val_loss: -2.3478\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.9622 - val_loss: -2.3576\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.2707 - val_loss: -2.3674\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -3.0601 - val_loss: -2.3771\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.1339 - val_loss: -2.3868\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.1021 - val_loss: -2.3963\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.1565 - val_loss: -2.4060\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -3.2742 - val_loss: -2.4156\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.1204 - val_loss: -2.4251\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.2025 - val_loss: -2.4345\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.1581 - val_loss: -2.4442\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.2262 - val_loss: -2.4540\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -3.1365 - val_loss: -2.4637\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.2413 - val_loss: -2.4730\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.2582 - val_loss: -2.4821\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: -3.2751 - val_loss: -2.4911\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -3.2492 - val_loss: -2.5000\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -3.3705 - val_loss: -2.5090\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.3114 - val_loss: -2.5178\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.3220 - val_loss: -2.5267\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.3510 - val_loss: -2.5355\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -3.2339 - val_loss: -2.5439\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.4385 - val_loss: -2.5523\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.5157 - val_loss: -2.5604\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.3517 - val_loss: -2.5686\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.5320 - val_loss: -2.5767\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.4725 - val_loss: -2.5850\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.4237 - val_loss: -2.5934\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -3.5878 - val_loss: -2.6017\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -3.5820 - val_loss: -2.6099\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -3.5149 - val_loss: -2.6179\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -3.5996 - val_loss: -2.6261\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.5201 - val_loss: -2.6345\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.5234 - val_loss: -2.6427\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.6037 - val_loss: -2.6506\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.5630 - val_loss: -2.6583\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.5959 - val_loss: -2.6660\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -3.5777 - val_loss: -2.6737\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -3.6447 - val_loss: -2.6818\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.6704 - val_loss: -2.6899\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.7842 - val_loss: -2.6982\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -3.6096 - val_loss: -2.7066\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.6121 - val_loss: -2.7147\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6333 - val_loss: -2.7227\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6647 - val_loss: -2.7306\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.7669 - val_loss: -2.7387\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -3.6364 - val_loss: -2.7468\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.8725 - val_loss: -2.7548\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -3.8798 - val_loss: -2.7629\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.7743 - val_loss: -2.7709\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -3.7849 - val_loss: -2.7788\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -3.9317 - val_loss: -2.7870\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.7300 - val_loss: -2.7950\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.8445 - val_loss: -2.8030\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.9599 - val_loss: -2.8112\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.9135 - val_loss: -2.8197\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -4.0059 - val_loss: -2.8284\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.8581 - val_loss: -2.8371\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.8694 - val_loss: -2.8458\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.8659 - val_loss: -2.8547\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.8435 - val_loss: -2.8634\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.9497 - val_loss: -2.8721\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -3.9531 - val_loss: -2.8809\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -3.9026 - val_loss: -2.8896\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -4.0032 - val_loss: -2.8982\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -3.9485 - val_loss: -2.9067\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -3.9837 - val_loss: -2.9153\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.9858 - val_loss: -2.9238\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.2098 - val_loss: -2.9323\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.1253 - val_loss: -2.9408\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -4.0325 - val_loss: -2.9491\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -4.0341 - val_loss: -2.9573\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -4.0640 - val_loss: -2.9653\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.0168 - val_loss: -2.9733\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.2724 - val_loss: -2.9811\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.1325 - val_loss: -2.9887\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.2717 - val_loss: -2.9961\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.1347 - val_loss: -3.0034\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 151ms/step - loss: -4.2046 - val_loss: -3.0102\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.2814 - val_loss: -3.0166\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -4.2746 - val_loss: -3.0226\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.1052 - val_loss: -3.0283\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.1879 - val_loss: -3.0337\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -4.2012 - val_loss: -3.0388\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.3045 - val_loss: -3.0438\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.2813 - val_loss: -3.0484\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -4.2990 - val_loss: -3.0528\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -4.4219 - val_loss: -3.0568\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -4.3322 - val_loss: -3.0607\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.3130 - val_loss: -3.0645\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.3298 - val_loss: -3.0682\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.4256 - val_loss: -3.0716\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -4.3785 - val_loss: -3.0750\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.2377 - val_loss: -3.0783\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -4.2760 - val_loss: -3.0813\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.4640 - val_loss: -3.0842\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.3173 - val_loss: -3.0870\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.4159 - val_loss: -3.0896\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.2894 - val_loss: -3.0922\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.3907 - val_loss: -3.0947\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.4029 - val_loss: -3.0969\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.4424 - val_loss: -3.0990\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.4101 - val_loss: -3.1010\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -4.4155 - val_loss: -3.1029\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.4486 - val_loss: -3.1046\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -4.4769 - val_loss: -3.1062\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.4049 - val_loss: -3.1077\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -4.5394 - val_loss: -3.1090\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.4605 - val_loss: -3.1103\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.5053 - val_loss: -3.1115\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.4406 - val_loss: -3.1126\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -4.3918 - val_loss: -3.1137\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -4.4074 - val_loss: -3.1148\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -4.4733 - val_loss: -3.1158\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.4712 - val_loss: -3.1168\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.4679 - val_loss: -3.1177\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -4.4768 - val_loss: -3.1186\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -4.5654 - val_loss: -3.1194\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -4.5670 - val_loss: -3.1201\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.5507 - val_loss: -3.1208\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.4837 - val_loss: -3.1215\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -4.5379 - val_loss: -3.1221\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -4.5329 - val_loss: -3.1227\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -4.5002 - val_loss: -3.1233\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -4.5702 - val_loss: -3.1238\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.5760 - val_loss: -3.1242\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -4.6030 - val_loss: -3.1247\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -4.4728 - val_loss: -3.1251\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.5241 - val_loss: -3.1255\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -4.5450 - val_loss: -3.1259\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.4562 - val_loss: -3.1263\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.5350 - val_loss: -3.1267\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.4807 - val_loss: -3.1270\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.4792 - val_loss: -3.1274\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.5854 - val_loss: -3.1277\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -4.5481 - val_loss: -3.1280\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.5710 - val_loss: -3.1282\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -4.5531 - val_loss: -3.1285\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -4.5345 - val_loss: -3.1287\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -4.5475 - val_loss: -3.1289\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.5323 - val_loss: -3.1291\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.6134 - val_loss: -3.1293\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.5798 - val_loss: -3.1295\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -4.5848 - val_loss: -3.1297\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.5789 - val_loss: -3.1299\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -4.4933 - val_loss: -3.1300\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.6086 - val_loss: -3.1302\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.6088 - val_loss: -3.1304\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -4.6096 - val_loss: -3.1305\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.5533 - val_loss: -3.1306\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -4.5900 - val_loss: -3.1307\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -4.5425 - val_loss: -3.1308\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.5870 - val_loss: -3.1309\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -4.5979 - val_loss: -3.1310\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.5369 - val_loss: -3.1310\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.6086 - val_loss: -3.1311\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.6295 - val_loss: -3.1312\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.5418 - val_loss: -3.1312\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.6292 - val_loss: -3.1313\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -4.6199 - val_loss: -3.1314\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.6145 - val_loss: -3.1314\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -4.6197 - val_loss: -3.1315\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.5649 - val_loss: -3.1316\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.5657 - val_loss: -3.1316\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.5884 - val_loss: -3.1317\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.6114 - val_loss: -3.1318\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -4.6369 - val_loss: -3.1318\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -4.6188 - val_loss: -3.1319\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.5761 - val_loss: -3.1320\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.6337 - val_loss: -3.1320\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.6343 - val_loss: -3.1321\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -4.6317 - val_loss: -3.1321\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -4.6307 - val_loss: -3.1322\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.5724 - val_loss: -3.1322\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -4.6149 - val_loss: -3.1322\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -4.5900 - val_loss: -3.1323\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.5695 - val_loss: -3.1323\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -4.6461 - val_loss: -3.1323\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -4.6185 - val_loss: -3.1323\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -4.6344 - val_loss: -3.1323\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.6165 - val_loss: -3.1324\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.5536 - val_loss: -3.1324\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -4.5956 - val_loss: -3.1324\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -4.6435 - val_loss: -3.1324\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -4.6213 - val_loss: -3.1324\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -4.6409 - val_loss: -3.1324\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.6372 - val_loss: -3.1324\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.6562 - val_loss: -3.1324\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.5907 - val_loss: -3.1325\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -4.6128 - val_loss: -3.1325\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.6265 - val_loss: -3.1325\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6277 - val_loss: -3.1325\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.6506 - val_loss: -3.1325\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.6317 - val_loss: -3.1325\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.6583 - val_loss: -3.1326\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -4.6322 - val_loss: -3.1326\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -4.6394 - val_loss: -3.1326\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -4.6296 - val_loss: -3.1326\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -4.6442 - val_loss: -3.1326\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6237 - val_loss: -3.1326\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.6241 - val_loss: -3.1327\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -4.6317 - val_loss: -3.1327\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.6357 - val_loss: -3.1327\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -4.6413 - val_loss: -3.1327\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -4.6326 - val_loss: -3.1327\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.6047 - val_loss: -3.1327\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.5720 - val_loss: -3.1327\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -4.6311 - val_loss: -3.1327\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -4.5964 - val_loss: -3.1327\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -4.6549 - val_loss: -3.1327\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 150ms/step - loss: -4.6357 - val_loss: -3.1327\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.6222 - val_loss: -3.1327\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -4.6555 - val_loss: -3.1327\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6140 - val_loss: -3.1327\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6352 - val_loss: -3.1327\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.5827 - val_loss: -3.1327\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.5639 - val_loss: -3.1327\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6333 - val_loss: -3.1327\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.6400 - val_loss: -3.1326\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.6359 - val_loss: -3.1326\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.6427 - val_loss: -3.1326\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.6515 - val_loss: -3.1326\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -4.6678 - val_loss: -3.1326\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.6461 - val_loss: -3.1326\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.6564 - val_loss: -3.1326\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -4.6488 - val_loss: -3.1326\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.6064 - val_loss: -3.1326\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -4.6392 - val_loss: -3.1326\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -4.6379 - val_loss: -3.1326\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.6193 - val_loss: -3.1326\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.6491 - val_loss: -3.1326\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00272: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 505ms/step - loss: -2.0698 - val_loss: -1.7593\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.0119 - val_loss: -1.7652\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.0390 - val_loss: -1.7712\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.1515 - val_loss: -1.7772\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: -2.2163 - val_loss: -1.7832\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.2304 - val_loss: -1.7891\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0910 - val_loss: -1.7948\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -2.2185 - val_loss: -1.8004\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.1662 - val_loss: -1.8060\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.2270 - val_loss: -1.8115\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.1928 - val_loss: -1.8171\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.2152 - val_loss: -1.8225\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.2386 - val_loss: -1.8279\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.2880 - val_loss: -1.8332\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -2.3328 - val_loss: -1.8386\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.2518 - val_loss: -1.8439\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.3565 - val_loss: -1.8491\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.3340 - val_loss: -1.8544\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.3183 - val_loss: -1.8596\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.3336 - val_loss: -1.8650\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.2794 - val_loss: -1.8705\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.3705 - val_loss: -1.8760\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.4903 - val_loss: -1.8817\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.4109 - val_loss: -1.8873\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.4227 - val_loss: -1.8929\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.3412 - val_loss: -1.8983\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.3852 - val_loss: -1.9035\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.2700 - val_loss: -1.9085\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.4657 - val_loss: -1.9134\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.5184 - val_loss: -1.9183\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.4231 - val_loss: -1.9232\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.5810 - val_loss: -1.9282\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.5026 - val_loss: -1.9329\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.5476 - val_loss: -1.9375\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.5077 - val_loss: -1.9421\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.5524 - val_loss: -1.9466\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.5549 - val_loss: -1.9510\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.5683 - val_loss: -1.9551\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.5157 - val_loss: -1.9592\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.6574 - val_loss: -1.9632\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.6207 - val_loss: -1.9671\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.6513 - val_loss: -1.9711\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -2.5271 - val_loss: -1.9751\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.7074 - val_loss: -1.9792\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.7400 - val_loss: -1.9833\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.6277 - val_loss: -1.9873\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.6577 - val_loss: -1.9912\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -2.7880 - val_loss: -1.9954\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.7282 - val_loss: -1.9997\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.6917 - val_loss: -2.0037\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.7480 - val_loss: -2.0078\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.7389 - val_loss: -2.0121\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.8591 - val_loss: -2.0165\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.8080 - val_loss: -2.0207\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -2.8059 - val_loss: -2.0249\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.7718 - val_loss: -2.0290\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.9111 - val_loss: -2.0335\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.8427 - val_loss: -2.0380\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.7926 - val_loss: -2.0424\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.8951 - val_loss: -2.0468\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -2.9667 - val_loss: -2.0513\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 166ms/step - loss: -2.9531 - val_loss: -2.0559\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.7851 - val_loss: -2.0605\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.8805 - val_loss: -2.0651\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.8478 - val_loss: -2.0698\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.8508 - val_loss: -2.0744\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.0057 - val_loss: -2.0791\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.0179 - val_loss: -2.0840\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.0165 - val_loss: -2.0890\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.9605 - val_loss: -2.0940\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -2.9529 - val_loss: -2.0990\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -3.1374 - val_loss: -2.1041\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -3.0188 - val_loss: -2.1092\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.0812 - val_loss: -2.1144\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.9477 - val_loss: -2.1195\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.1175 - val_loss: -2.1245\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.9948 - val_loss: -2.1293\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -3.0786 - val_loss: -2.1341\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -3.0612 - val_loss: -2.1391\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.1608 - val_loss: -2.1439\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.1724 - val_loss: -2.1488\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.1688 - val_loss: -2.1537\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.1721 - val_loss: -2.1585\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.2315 - val_loss: -2.1630\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -3.0870 - val_loss: -2.1676\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.1450 - val_loss: -2.1721\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.0935 - val_loss: -2.1765\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -3.2144 - val_loss: -2.1808\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -3.1800 - val_loss: -2.1850\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.2050 - val_loss: -2.1891\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -3.2169 - val_loss: -2.1931\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -3.2820 - val_loss: -2.1970\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.2777 - val_loss: -2.2009\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.2193 - val_loss: -2.2047\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -3.2370 - val_loss: -2.2085\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.3159 - val_loss: -2.2120\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -3.3209 - val_loss: -2.2154\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -3.2909 - val_loss: -2.2186\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -3.2543 - val_loss: -2.2218\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.2880 - val_loss: -2.2247\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -3.3026 - val_loss: -2.2275\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.2852 - val_loss: -2.2303\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.3656 - val_loss: -2.2330\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.2941 - val_loss: -2.2356\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -3.2898 - val_loss: -2.2383\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -3.3324 - val_loss: -2.2408\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.3292 - val_loss: -2.2432\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.3003 - val_loss: -2.2455\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.3188 - val_loss: -2.2478\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.3617 - val_loss: -2.2500\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -3.3419 - val_loss: -2.2521\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.3681 - val_loss: -2.2540\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.4016 - val_loss: -2.2560\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -3.4346 - val_loss: -2.2578\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.3572 - val_loss: -2.2595\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -3.3447 - val_loss: -2.2611\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.4650 - val_loss: -2.2627\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.4364 - val_loss: -2.2642\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.4126 - val_loss: -2.2656\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.4631 - val_loss: -2.2669\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.4702 - val_loss: -2.2681\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.4635 - val_loss: -2.2692\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.4233 - val_loss: -2.2703\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -3.4589 - val_loss: -2.2713\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.4652 - val_loss: -2.2724\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.4702 - val_loss: -2.2734\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.4682 - val_loss: -2.2743\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.4167 - val_loss: -2.2752\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -3.4673 - val_loss: -2.2760\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.5250 - val_loss: -2.2768\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.5338 - val_loss: -2.2775\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -3.5150 - val_loss: -2.2782\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 190ms/step - loss: -3.5470 - val_loss: -2.2788\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -3.5263 - val_loss: -2.2794\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.4957 - val_loss: -2.2799\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -3.4627 - val_loss: -2.2805\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.5147 - val_loss: -2.2811\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -3.5360 - val_loss: -2.2816\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.5619 - val_loss: -2.2821\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -3.5155 - val_loss: -2.2825\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.5215 - val_loss: -2.2830\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.5451 - val_loss: -2.2835\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -3.5318 - val_loss: -2.2839\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.5742 - val_loss: -2.2843\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.4695 - val_loss: -2.2848\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -3.5385 - val_loss: -2.2852\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.5254 - val_loss: -2.2855\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.5191 - val_loss: -2.2859\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.5402 - val_loss: -2.2863\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.5818 - val_loss: -2.2866\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.5556 - val_loss: -2.2869\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.5228 - val_loss: -2.2872\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.5795 - val_loss: -2.2875\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.5904 - val_loss: -2.2877\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.6085 - val_loss: -2.2880\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -3.5511 - val_loss: -2.2882\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -3.5581 - val_loss: -2.2885\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -3.5852 - val_loss: -2.2887\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.5799 - val_loss: -2.2889\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.5844 - val_loss: -2.2891\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.5711 - val_loss: -2.2893\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -3.5873 - val_loss: -2.2895\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -3.6035 - val_loss: -2.2897\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.5606 - val_loss: -2.2899\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -3.5795 - val_loss: -2.2900\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -3.5885 - val_loss: -2.2902\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.5834 - val_loss: -2.2904\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.5895 - val_loss: -2.2906\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -3.5877 - val_loss: -2.2907\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.5973 - val_loss: -2.2909\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -3.5892 - val_loss: -2.2910\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.6133 - val_loss: -2.2912\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.5917 - val_loss: -2.2913\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.6307 - val_loss: -2.2915\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.6048 - val_loss: -2.2916\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.5984 - val_loss: -2.2917\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.5962 - val_loss: -2.2919\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.6234 - val_loss: -2.2920\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.6063 - val_loss: -2.2921\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -3.6062 - val_loss: -2.2922\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -3.6291 - val_loss: -2.2923\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -3.6058 - val_loss: -2.2924\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.6221 - val_loss: -2.2925\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.5988 - val_loss: -2.2926\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -3.6072 - val_loss: -2.2927\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.5972 - val_loss: -2.2928\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -3.6180 - val_loss: -2.2929\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.5945 - val_loss: -2.2930\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -3.5996 - val_loss: -2.2931\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -3.6069 - val_loss: -2.2932\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -3.5976 - val_loss: -2.2932\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -3.6011 - val_loss: -2.2933\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.5983 - val_loss: -2.2934\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -3.6103 - val_loss: -2.2935\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -3.6014 - val_loss: -2.2936\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6149 - val_loss: -2.2937\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.6226 - val_loss: -2.2937\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.6319 - val_loss: -2.2938\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6171 - val_loss: -2.2939\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -3.5928 - val_loss: -2.2940\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.6271 - val_loss: -2.2940\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -3.6268 - val_loss: -2.2941\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -3.6131 - val_loss: -2.2942\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.6163 - val_loss: -2.2942\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -3.6337 - val_loss: -2.2943\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.6043 - val_loss: -2.2943\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.6082 - val_loss: -2.2944\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -3.6347 - val_loss: -2.2944\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.6342 - val_loss: -2.2945\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.6343 - val_loss: -2.2945\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6233 - val_loss: -2.2946\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.6075 - val_loss: -2.2946\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.6274 - val_loss: -2.2947\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.6323 - val_loss: -2.2948\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6284 - val_loss: -2.2948\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6002 - val_loss: -2.2948\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -3.6191 - val_loss: -2.2949\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -3.6077 - val_loss: -2.2949\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6300 - val_loss: -2.2949\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.6282 - val_loss: -2.2950\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.6161 - val_loss: -2.2950\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6246 - val_loss: -2.2950\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6401 - val_loss: -2.2951\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.6352 - val_loss: -2.2951\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -3.6166 - val_loss: -2.2951\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6276 - val_loss: -2.2952\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -3.6298 - val_loss: -2.2952\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6260 - val_loss: -2.2952\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.6367 - val_loss: -2.2953\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.6214 - val_loss: -2.2953\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.6334 - val_loss: -2.2953\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -3.6443 - val_loss: -2.2954\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.6363 - val_loss: -2.2954\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.6458 - val_loss: -2.2954\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.6488 - val_loss: -2.2955\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.6350 - val_loss: -2.2955\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.6391 - val_loss: -2.2955\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.6281 - val_loss: -2.2955\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.6260 - val_loss: -2.2956\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -3.6399 - val_loss: -2.2956\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.6468 - val_loss: -2.2956\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6310 - val_loss: -2.2957\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.6442 - val_loss: -2.2957\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.6302 - val_loss: -2.2957\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -3.6357 - val_loss: -2.2957\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.6477 - val_loss: -2.2958\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -3.6379 - val_loss: -2.2958\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.6408 - val_loss: -2.2958\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -3.6437 - val_loss: -2.2958\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6488 - val_loss: -2.2959\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6485 - val_loss: -2.2959\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -3.6334 - val_loss: -2.2959\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6220 - val_loss: -2.2959\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -3.6354 - val_loss: -2.2959\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6257 - val_loss: -2.2959\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -3.6573 - val_loss: -2.2960\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6319 - val_loss: -2.2960\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6544 - val_loss: -2.2960\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6438 - val_loss: -2.2960\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -3.6279 - val_loss: -2.2960\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.6590 - val_loss: -2.2960\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6344 - val_loss: -2.2961\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6455 - val_loss: -2.2961\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -3.6372 - val_loss: -2.2961\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -3.6501 - val_loss: -2.2961\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.6366 - val_loss: -2.2961\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -3.6472 - val_loss: -2.2961\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -3.6430 - val_loss: -2.2961\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -3.6545 - val_loss: -2.2962\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -3.6410 - val_loss: -2.2962\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -3.6379 - val_loss: -2.2962\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.6328 - val_loss: -2.2962\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -3.6442 - val_loss: -2.2962\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6359 - val_loss: -2.2962\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6530 - val_loss: -2.2962\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.6526 - val_loss: -2.2963\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -3.6466 - val_loss: -2.2963\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.6508 - val_loss: -2.2963\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -3.6556 - val_loss: -2.2963\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6489 - val_loss: -2.2963\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -3.6277 - val_loss: -2.2963\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -3.6494 - val_loss: -2.2963\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -3.6490 - val_loss: -2.2963\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -3.6465 - val_loss: -2.2963\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.6470 - val_loss: -2.2964\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.6386 - val_loss: -2.2964\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.6487 - val_loss: -2.2964\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.6601 - val_loss: -2.2964\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 150ms/step - loss: -3.6507 - val_loss: -2.2964\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 140ms/step - loss: -3.6561 - val_loss: -2.2964\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.6532 - val_loss: -2.2964\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.6452 - val_loss: -2.2964\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.6538 - val_loss: -2.2964\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6526 - val_loss: -2.2965\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -3.6512 - val_loss: -2.2965\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.6514 - val_loss: -2.2965\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -3.6637 - val_loss: -2.2965\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -3.6601 - val_loss: -2.2965\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.6342 - val_loss: -2.2965\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.6565 - val_loss: -2.2965\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -3.6389 - val_loss: -2.2965\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -3.6586 - val_loss: -2.2965\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -3.6431 - val_loss: -2.2965\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -3.6489 - val_loss: -2.2965\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6397 - val_loss: -2.2965\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -3.6511 - val_loss: -2.2966\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.6531 - val_loss: -2.2966\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -3.6568 - val_loss: -2.2966\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.6430 - val_loss: -2.2966\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -3.6540 - val_loss: -2.2966\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.6440 - val_loss: -2.2966\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00311: early stopping\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5db5cc6ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_5 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 515ms/step - loss: -1.2121 - val_loss: -1.9632\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.2078 - val_loss: -1.9648\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.2160 - val_loss: -1.9660\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.2798 - val_loss: -1.9673\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.2814 - val_loss: -1.9686\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.2459 - val_loss: -1.9700\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.1507 - val_loss: -1.9713\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.3537 - val_loss: -1.9728\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3770 - val_loss: -1.9746\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.3488 - val_loss: -1.9766\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.2968 - val_loss: -1.9787\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.2954 - val_loss: -1.9807\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.3049 - val_loss: -1.9826\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.3380 - val_loss: -1.9844\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.3365 - val_loss: -1.9862\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.3432 - val_loss: -1.9879\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.3084 - val_loss: -1.9895\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.3475 - val_loss: -1.9912\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3696 - val_loss: -1.9928\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.3817 - val_loss: -1.9944\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.4099 - val_loss: -1.9961\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 173ms/step - loss: -1.3375 - val_loss: -1.9979\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.3708 - val_loss: -1.9999\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.3987 - val_loss: -2.0017\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.4183 - val_loss: -2.0035\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.3595 - val_loss: -2.0053\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.3802 - val_loss: -2.0071\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.4280 - val_loss: -2.0094\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.4271 - val_loss: -2.0119\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.4727 - val_loss: -2.0146\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3943 - val_loss: -2.0175\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.4389 - val_loss: -2.0207\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.3938 - val_loss: -2.0236\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.4069 - val_loss: -2.0265\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.4418 - val_loss: -2.0292\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.4725 - val_loss: -2.0322\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.4643 - val_loss: -2.0353\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.4441 - val_loss: -2.0381\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.4520 - val_loss: -2.0409\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4338 - val_loss: -2.0438\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.4223 - val_loss: -2.0467\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.5033 - val_loss: -2.0498\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.5524 - val_loss: -2.0530\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.4950 - val_loss: -2.0563\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.5284 - val_loss: -2.0601\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.5127 - val_loss: -2.0640\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.5010 - val_loss: -2.0679\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.4946 - val_loss: -2.0718\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -1.5212 - val_loss: -2.0756\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.5336 - val_loss: -2.0797\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.5561 - val_loss: -2.0844\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.5124 - val_loss: -2.0891\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.5635 - val_loss: -2.0939\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.5562 - val_loss: -2.0989\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.5482 - val_loss: -2.1042\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.5293 - val_loss: -2.1094\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.5363 - val_loss: -2.1145\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 132ms/step - loss: -1.6590 - val_loss: -2.1203\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6012 - val_loss: -2.1266\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.6147 - val_loss: -2.1331\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6282 - val_loss: -2.1399\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6579 - val_loss: -2.1468\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.6002 - val_loss: -2.1538\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.5856 - val_loss: -2.1608\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6705 - val_loss: -2.1680\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6227 - val_loss: -2.1754\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.5766 - val_loss: -2.1826\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6116 - val_loss: -2.1898\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6785 - val_loss: -2.1970\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.6497 - val_loss: -2.2042\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6552 - val_loss: -2.2115\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.5863 - val_loss: -2.2188\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.6708 - val_loss: -2.2262\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6366 - val_loss: -2.2336\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6702 - val_loss: -2.2410\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7073 - val_loss: -2.2484\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6668 - val_loss: -2.2556\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6793 - val_loss: -2.2627\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -1.6782 - val_loss: -2.2694\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 147ms/step - loss: -1.7281 - val_loss: -2.2761\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6787 - val_loss: -2.2828\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7193 - val_loss: -2.2896\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.7577 - val_loss: -2.2963\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.7216 - val_loss: -2.3030\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7186 - val_loss: -2.3096\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.7396 - val_loss: -2.3160\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.7408 - val_loss: -2.3222\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.7392 - val_loss: -2.3283\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.7906 - val_loss: -2.3343\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.7567 - val_loss: -2.3403\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7836 - val_loss: -2.3463\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.8003 - val_loss: -2.3522\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7813 - val_loss: -2.3578\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8283 - val_loss: -2.3632\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8160 - val_loss: -2.3684\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8391 - val_loss: -2.3734\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8176 - val_loss: -2.3782\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8716 - val_loss: -2.3827\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8388 - val_loss: -2.3871\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8716 - val_loss: -2.3913\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8249 - val_loss: -2.3953\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7859 - val_loss: -2.3991\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8671 - val_loss: -2.4028\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8723 - val_loss: -2.4062\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8410 - val_loss: -2.4095\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8373 - val_loss: -2.4128\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.9002 - val_loss: -2.4160\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.8341 - val_loss: -2.4190\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8731 - val_loss: -2.4218\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8720 - val_loss: -2.4246\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8787 - val_loss: -2.4273\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.9315 - val_loss: -2.4297\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8666 - val_loss: -2.4320\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8693 - val_loss: -2.4342\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.9346 - val_loss: -2.4364\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.8924 - val_loss: -2.4384\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.9521 - val_loss: -2.4403\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.9459 - val_loss: -2.4421\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.9369 - val_loss: -2.4438\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.9342 - val_loss: -2.4455\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.9052 - val_loss: -2.4472\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.8989 - val_loss: -2.4488\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.9370 - val_loss: -2.4503\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.9437 - val_loss: -2.4518\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.9336 - val_loss: -2.4532\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.9373 - val_loss: -2.4545\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.9489 - val_loss: -2.4558\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.9404 - val_loss: -2.4570\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.9515 - val_loss: -2.4581\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.9656 - val_loss: -2.4592\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.9469 - val_loss: -2.4602\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.9891 - val_loss: -2.4611\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.9621 - val_loss: -2.4620\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.9485 - val_loss: -2.4630\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.9426 - val_loss: -2.4639\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.9661 - val_loss: -2.4647\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.9765 - val_loss: -2.4656\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.9734 - val_loss: -2.4664\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.9633 - val_loss: -2.4672\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -1.98 - 0s 87ms/step - loss: -1.9695 - val_loss: -2.4679\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.9949 - val_loss: -2.4686\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.9795 - val_loss: -2.4693\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.9752 - val_loss: -2.4700\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.9906 - val_loss: -2.4706\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.9913 - val_loss: -2.4712\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.9964 - val_loss: -2.4717\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.9841 - val_loss: -2.4723\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.9717 - val_loss: -2.4728\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.9889 - val_loss: -2.4734\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.9886 - val_loss: -2.4739\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.9891 - val_loss: -2.4744\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.9904 - val_loss: -2.4748\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.9889 - val_loss: -2.4753\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.9967 - val_loss: -2.4758\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.9959 - val_loss: -2.4763\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.9818 - val_loss: -2.4767\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.9945 - val_loss: -2.4772\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.9892 - val_loss: -2.4776\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.9874 - val_loss: -2.4780\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.9745 - val_loss: -2.4784\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0109 - val_loss: -2.4787\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.9791 - val_loss: -2.4791\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.9941 - val_loss: -2.4795\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.0050 - val_loss: -2.4798\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -2.0174 - val_loss: -2.4801\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -2.0188 - val_loss: -2.4804\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -2.0030 - val_loss: -2.4807\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -2.0262 - val_loss: -2.4810\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -2.0063 - val_loss: -2.4813\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0031 - val_loss: -2.4816\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 138ms/step - loss: -2.0003 - val_loss: -2.4818\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -2.0125 - val_loss: -2.4821\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0134 - val_loss: -2.4824\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0048 - val_loss: -2.4826\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0251 - val_loss: -2.4828\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.0225 - val_loss: -2.4831\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -2.0188 - val_loss: -2.4833\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0115 - val_loss: -2.4835\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0095 - val_loss: -2.4837\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.0101 - val_loss: -2.4839\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0063 - val_loss: -2.4840\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -2.0164 - val_loss: -2.4842\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0149 - val_loss: -2.4844\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0327 - val_loss: -2.4846\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -2.0255 - val_loss: -2.4847\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.0210 - val_loss: -2.4849\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0017 - val_loss: -2.4851\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -2.0284 - val_loss: -2.4852\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0238 - val_loss: -2.4854\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0169 - val_loss: -2.4855\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.0378 - val_loss: -2.4856\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -2.0272 - val_loss: -2.4858\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0104 - val_loss: -2.4859\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0304 - val_loss: -2.4860\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -2.0395 - val_loss: -2.4861\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.0329 - val_loss: -2.4862\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -2.0201 - val_loss: -2.4863\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -2.0369 - val_loss: -2.4865\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -2.0075 - val_loss: -2.4866\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -2.0013 - val_loss: -2.4867\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0111 - val_loss: -2.4868\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.0207 - val_loss: -2.4869\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -2.0229 - val_loss: -2.4871\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0412 - val_loss: -2.4872\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0298 - val_loss: -2.4873\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -2.0354 - val_loss: -2.4874\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -2.0255 - val_loss: -2.4875\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0318 - val_loss: -2.4876\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0238 - val_loss: -2.4877\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0359 - val_loss: -2.4878\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0398 - val_loss: -2.4879\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.0318 - val_loss: -2.4880\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0362 - val_loss: -2.4881\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0364 - val_loss: -2.4882\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0292 - val_loss: -2.4883\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0157 - val_loss: -2.4884\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -2.0274 - val_loss: -2.4884\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0371 - val_loss: -2.4885\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0246 - val_loss: -2.4886\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -2.0257 - val_loss: -2.4887\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0378 - val_loss: -2.4888\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0287 - val_loss: -2.4888\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0440 - val_loss: -2.4889\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0399 - val_loss: -2.4890\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0411 - val_loss: -2.4890\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -2.0417 - val_loss: -2.4891\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0361 - val_loss: -2.4892\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0235 - val_loss: -2.4892\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -2.0307 - val_loss: -2.4893\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.0448 - val_loss: -2.4894\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0331 - val_loss: -2.4894\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0436 - val_loss: -2.4895\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.0405 - val_loss: -2.4895\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.0356 - val_loss: -2.4896\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -2.0414 - val_loss: -2.4897\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -2.0358 - val_loss: -2.4897\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0452 - val_loss: -2.4898\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0290 - val_loss: -2.4898\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0265 - val_loss: -2.4899\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0423 - val_loss: -2.4900\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0300 - val_loss: -2.4900\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0421 - val_loss: -2.4901\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0532 - val_loss: -2.4901\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0328 - val_loss: -2.4902\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0427 - val_loss: -2.4902\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0430 - val_loss: -2.4903\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0348 - val_loss: -2.4903\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0450 - val_loss: -2.4904\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0471 - val_loss: -2.4904\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -2.0483 - val_loss: -2.4905\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0319 - val_loss: -2.4905\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 145ms/step - loss: -2.0528 - val_loss: -2.4905\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0390 - val_loss: -2.4906\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0415 - val_loss: -2.4906\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.0513 - val_loss: -2.4906\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.0449 - val_loss: -2.4907\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0435 - val_loss: -2.4907\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -2.0454 - val_loss: -2.4908\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0499 - val_loss: -2.4908\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -2.0453 - val_loss: -2.4908\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0489 - val_loss: -2.4909\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -2.0474 - val_loss: -2.4909\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -2.0497 - val_loss: -2.4909\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0542 - val_loss: -2.4910\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0521 - val_loss: -2.4910\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0443 - val_loss: -2.4910\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0453 - val_loss: -2.4911\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -2.0523 - val_loss: -2.4911\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0513 - val_loss: -2.4911\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0477 - val_loss: -2.4912\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -2.0494 - val_loss: -2.4912\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0442 - val_loss: -2.4912\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.0573 - val_loss: -2.4913\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0517 - val_loss: -2.4913\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0478 - val_loss: -2.4913\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0475 - val_loss: -2.4913\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0346 - val_loss: -2.4914\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0540 - val_loss: -2.4914\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0492 - val_loss: -2.4914\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0512 - val_loss: -2.4915\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.0493 - val_loss: -2.4915\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0512 - val_loss: -2.4915\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.0528 - val_loss: -2.4915\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0394 - val_loss: -2.4916\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -2.0463 - val_loss: -2.4916\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0500 - val_loss: -2.4916\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0489 - val_loss: -2.4916\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -2.0473 - val_loss: -2.4917\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0535 - val_loss: -2.4917\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.0461 - val_loss: -2.4917\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.0509 - val_loss: -2.4917\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0543 - val_loss: -2.4918\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0549 - val_loss: -2.4918\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0477 - val_loss: -2.4918\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -2.0530 - val_loss: -2.4918\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0552 - val_loss: -2.4918\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0558 - val_loss: -2.4919\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0490 - val_loss: -2.4919\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0499 - val_loss: -2.4919\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0510 - val_loss: -2.4919\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0481 - val_loss: -2.4920\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0565 - val_loss: -2.4920\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0567 - val_loss: -2.4920\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0566 - val_loss: -2.4920\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0538 - val_loss: -2.4920\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -2.0526 - val_loss: -2.4920\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0496 - val_loss: -2.4921\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -2.0477 - val_loss: -2.4921\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0570 - val_loss: -2.4921\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -2.0557 - val_loss: -2.4921\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -2.0523 - val_loss: -2.4921\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -2.0531 - val_loss: -2.4922\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0503 - val_loss: -2.4922\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0568 - val_loss: -2.4922\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0481 - val_loss: -2.4922\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -2.0534 - val_loss: -2.4922\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0552 - val_loss: -2.4923\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0541 - val_loss: -2.4923\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.0473 - val_loss: -2.4923\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0508 - val_loss: -2.4923\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0511 - val_loss: -2.4923\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -2.0460 - val_loss: -2.4923\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0485 - val_loss: -2.4924\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0567 - val_loss: -2.4924\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0554 - val_loss: -2.4924\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -2.0530 - val_loss: -2.4924\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0497 - val_loss: -2.4924\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.0566 - val_loss: -2.4925\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.0511 - val_loss: -2.4925\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0550 - val_loss: -2.4925\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0565 - val_loss: -2.4925\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.0518 - val_loss: -2.4925\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0521 - val_loss: -2.4925\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.0572 - val_loss: -2.4925\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.0526 - val_loss: -2.4926\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0531 - val_loss: -2.4926\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.0433 - val_loss: -2.4926\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0560 - val_loss: -2.4926\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0479 - val_loss: -2.4926\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -2.0550 - val_loss: -2.4926\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.0516 - val_loss: -2.4926\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0516 - val_loss: -2.4927\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.0524 - val_loss: -2.4927\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0467 - val_loss: -2.4927\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.0555 - val_loss: -2.4927\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0546 - val_loss: -2.4927\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -2.0517 - val_loss: -2.4927\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0529 - val_loss: -2.4928\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.0529 - val_loss: -2.4928\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0576 - val_loss: -2.4928\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.0550 - val_loss: -2.4928\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -2.0556 - val_loss: -2.4928\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0516 - val_loss: -2.4928\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0548 - val_loss: -2.4928\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -2.0520 - val_loss: -2.4928\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -2.0567 - val_loss: -2.4929\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0556 - val_loss: -2.4929\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -2.0563 - val_loss: -2.4929\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0537 - val_loss: -2.4929\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00359: early stopping\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5db449cef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_6 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 481ms/step - loss: -0.4859 - val_loss: -2.2672\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.4698 - val_loss: -2.2699\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -0.5014 - val_loss: -2.2767\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.4731 - val_loss: -2.2847\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.4588 - val_loss: -2.2935\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.4697 - val_loss: -2.3025\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.4903 - val_loss: -2.3114\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.4716 - val_loss: -2.3218\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.4450 - val_loss: -2.3316\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.4972 - val_loss: -2.3414\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.4914 - val_loss: -2.3513\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.4710 - val_loss: -2.3611\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5017 - val_loss: -2.3712\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5044 - val_loss: -2.3812\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.4882 - val_loss: -2.3914\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.4868 - val_loss: -2.4027\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.4892 - val_loss: -2.4140\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.4789 - val_loss: -2.4235\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.4819 - val_loss: -2.4330\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5126 - val_loss: -2.4427\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.4933 - val_loss: -2.4529\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5069 - val_loss: -2.4626\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5017 - val_loss: -2.4724\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.5025 - val_loss: -2.4830\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.4872 - val_loss: -2.4954\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5058 - val_loss: -2.5085\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5030 - val_loss: -2.5218\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5007 - val_loss: -2.5348\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5168 - val_loss: -2.5472\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5179 - val_loss: -2.5587\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5053 - val_loss: -2.5693\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5016 - val_loss: -2.5794\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5064 - val_loss: -2.5887\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5253 - val_loss: -2.5981\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5094 - val_loss: -2.6078\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5259 - val_loss: -2.6166\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5122 - val_loss: -2.6255\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5096 - val_loss: -2.6344\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.5354 - val_loss: -2.6433\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5312 - val_loss: -2.6520\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5208 - val_loss: -2.6605\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5245 - val_loss: -2.6681\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5302 - val_loss: -2.6751\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5181 - val_loss: -2.6814\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5200 - val_loss: -2.6878\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5105 - val_loss: -2.6949\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5127 - val_loss: -2.7022\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5280 - val_loss: -2.7097\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5327 - val_loss: -2.7163\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5014 - val_loss: -2.7227\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5413 - val_loss: -2.7287\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 154ms/step - loss: -0.5223 - val_loss: -2.7348\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.4969 - val_loss: -2.7406\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5270 - val_loss: -2.7461\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5167 - val_loss: -2.7514\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5177 - val_loss: -2.7575\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5343 - val_loss: -2.7634\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5200 - val_loss: -2.7694\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5335 - val_loss: -2.7754\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.5260 - val_loss: -2.7817\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5296 - val_loss: -2.7876\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5372 - val_loss: -2.7929\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5153 - val_loss: -2.7978\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5467 - val_loss: -2.8018\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5380 - val_loss: -2.8058\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.5341 - val_loss: -2.8102\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5200 - val_loss: -2.8153\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5092 - val_loss: -2.8209\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5430 - val_loss: -2.8265\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5341 - val_loss: -2.8315\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5368 - val_loss: -2.8359\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.5281 - val_loss: -2.8399\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5179 - val_loss: -2.8439\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5279 - val_loss: -2.8482\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5280 - val_loss: -2.8520\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5328 - val_loss: -2.8557\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.5513 - val_loss: -2.8593\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5352 - val_loss: -2.8632\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5274 - val_loss: -2.8668\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5290 - val_loss: -2.8701\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5484 - val_loss: -2.8729\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5253 - val_loss: -2.8756\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5371 - val_loss: -2.8786\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -0.5277 - val_loss: -2.8825\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.5231 - val_loss: -2.8862\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5423 - val_loss: -2.8897\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.5186 - val_loss: -2.8933\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5436 - val_loss: -2.8971\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5471 - val_loss: -2.9005\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5355 - val_loss: -2.9036\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5348 - val_loss: -2.9066\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5355 - val_loss: -2.9094\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5551 - val_loss: -2.9117\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5357 - val_loss: -2.9140\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5410 - val_loss: -2.9163\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5443 - val_loss: -2.9187\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5338 - val_loss: -2.9214\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5253 - val_loss: -2.9241\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5540 - val_loss: -2.9262\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5453 - val_loss: -2.9282\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -0.5575 - val_loss: -2.9297\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5457 - val_loss: -2.9313\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5424 - val_loss: -2.9331\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5527 - val_loss: -2.9350\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5557 - val_loss: -2.9367\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5604 - val_loss: -2.9380\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5537 - val_loss: -2.9390\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5497 - val_loss: -2.9401\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5452 - val_loss: -2.9416\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5459 - val_loss: -2.9434\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5515 - val_loss: -2.9452\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: -0.5522 - val_loss: -2.9467\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5461 - val_loss: -2.9484\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5487 - val_loss: -2.9502\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5510 - val_loss: -2.9520\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5335 - val_loss: -2.9538\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5268 - val_loss: -2.9557\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5427 - val_loss: -2.9573\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5354 - val_loss: -2.9588\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5482 - val_loss: -2.9603\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.5474 - val_loss: -2.9619\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.5517 - val_loss: -2.9634\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5531 - val_loss: -2.9647\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5454 - val_loss: -2.9658\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5505 - val_loss: -2.9669\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -0.5478 - val_loss: -2.9679\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5453 - val_loss: -2.9691\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5507 - val_loss: -2.9703\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5355 - val_loss: -2.9717\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5513 - val_loss: -2.9730\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5455 - val_loss: -2.9741\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5514 - val_loss: -2.9750\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5552 - val_loss: -2.9759\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5529 - val_loss: -2.9766\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5452 - val_loss: -2.9774\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5447 - val_loss: -2.9785\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5562 - val_loss: -2.9796\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5561 - val_loss: -2.9806\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5547 - val_loss: -2.9814\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5440 - val_loss: -2.9821\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5407 - val_loss: -2.9829\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5500 - val_loss: -2.9838\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5510 - val_loss: -2.9847\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5434 - val_loss: -2.9855\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5350 - val_loss: -2.9862\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5568 - val_loss: -2.9868\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5513 - val_loss: -2.9874\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5409 - val_loss: -2.9879\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5570 - val_loss: -2.9883\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5546 - val_loss: -2.9888\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5503 - val_loss: -2.9892\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5543 - val_loss: -2.9895\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5625 - val_loss: -2.9896\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -0.5528 - val_loss: -2.9898\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5514 - val_loss: -2.9901\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5569 - val_loss: -2.9904\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5537 - val_loss: -2.9908\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5488 - val_loss: -2.9914\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5544 - val_loss: -2.9920\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5564 - val_loss: -2.9926\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5507 - val_loss: -2.9930\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5525 - val_loss: -2.9934\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5614 - val_loss: -2.9935\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5559 - val_loss: -2.9937\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5529 - val_loss: -2.9940\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5560 - val_loss: -2.9942\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.5534 - val_loss: -2.9944\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5555 - val_loss: -2.9947\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5558 - val_loss: -2.9949\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5531 - val_loss: -2.9952\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -0.5419 - val_loss: -2.9956\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.5429 - val_loss: -2.9960\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5472 - val_loss: -2.9966\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5524 - val_loss: -2.9970\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5573 - val_loss: -2.9973\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5567 - val_loss: -2.9975\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -0.5499 - val_loss: -2.9978\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5547 - val_loss: -2.9979\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5469 - val_loss: -2.9982\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5530 - val_loss: -2.9986\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5468 - val_loss: -2.9993\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5544 - val_loss: -2.9998\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.5518 - val_loss: -3.0004\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5508 - val_loss: -3.0010\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5547 - val_loss: -3.0014\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.5523 - val_loss: -3.0018\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5596 - val_loss: -3.0021\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5542 - val_loss: -3.0024\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5509 - val_loss: -3.0028\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5500 - val_loss: -3.0033\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5555 - val_loss: -3.0039\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5444 - val_loss: -3.0044\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5558 - val_loss: -3.0047\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5542 - val_loss: -3.0051\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5527 - val_loss: -3.0055\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5530 - val_loss: -3.0060\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5590 - val_loss: -3.0066\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5456 - val_loss: -3.0071\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5518 - val_loss: -3.0075\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5568 - val_loss: -3.0078\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5519 - val_loss: -3.0082\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5564 - val_loss: -3.0085\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5541 - val_loss: -3.0087\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5523 - val_loss: -3.0089\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5465 - val_loss: -3.0093\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5515 - val_loss: -3.0097\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5565 - val_loss: -3.0100\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5548 - val_loss: -3.0103\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5559 - val_loss: -3.0105\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5560 - val_loss: -3.0106\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5504 - val_loss: -3.0108\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5542 - val_loss: -3.0109\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5575 - val_loss: -3.0110\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5457 - val_loss: -3.0114\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5537 - val_loss: -3.0119\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.5549 - val_loss: -3.0124\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5460 - val_loss: -3.0129\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5480 - val_loss: -3.0137\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5561 - val_loss: -3.0145\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5580 - val_loss: -3.0152\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5526 - val_loss: -3.0159\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5597 - val_loss: -3.0167\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5459 - val_loss: -3.0174\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5446 - val_loss: -3.0182\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5504 - val_loss: -3.0191\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5471 - val_loss: -3.0199\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5581 - val_loss: -3.0205\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5523 - val_loss: -3.0211\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5521 - val_loss: -3.0216\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5556 - val_loss: -3.0219\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5533 - val_loss: -3.0222\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5535 - val_loss: -3.0226\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5553 - val_loss: -3.0228\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5526 - val_loss: -3.0231\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5508 - val_loss: -3.0234\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5509 - val_loss: -3.0238\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5503 - val_loss: -3.0242\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5569 - val_loss: -3.0245\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5562 - val_loss: -3.0247\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5568 - val_loss: -3.0250\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5535 - val_loss: -3.0251\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -0.5531 - val_loss: -3.0253\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5500 - val_loss: -3.0255\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5556 - val_loss: -3.0257\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5598 - val_loss: -3.0257\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5506 - val_loss: -3.0258\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5580 - val_loss: -3.0258\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5588 - val_loss: -3.0258\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5577 - val_loss: -3.0257\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5487 - val_loss: -3.0257\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5595 - val_loss: -3.0257\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5552 - val_loss: -3.0257\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5548 - val_loss: -3.0259\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5552 - val_loss: -3.0260\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5557 - val_loss: -3.0262\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5566 - val_loss: -3.0263\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -0.51 - 0s 96ms/step - loss: -0.5469 - val_loss: -3.0264\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5573 - val_loss: -3.0265\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5565 - val_loss: -3.0266\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -0.5535 - val_loss: -3.0266\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5564 - val_loss: -3.0266\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5554 - val_loss: -3.0266\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5561 - val_loss: -3.0265\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5551 - val_loss: -3.0265\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5489 - val_loss: -3.0265\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5560 - val_loss: -3.0266\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.5470 - val_loss: -3.0268\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.5532 - val_loss: -3.0270\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5537 - val_loss: -3.0272\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.5570 - val_loss: -3.0273\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5558 - val_loss: -3.0274\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5549 - val_loss: -3.0276\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5542 - val_loss: -3.0278\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 152ms/step - loss: -0.5587 - val_loss: -3.0280\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5564 - val_loss: -3.0280\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 154ms/step - loss: -0.5560 - val_loss: -3.0280\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5511 - val_loss: -3.0281\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5517 - val_loss: -3.0282\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5562 - val_loss: -3.0282\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5587 - val_loss: -3.0282\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5565 - val_loss: -3.0283\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5564 - val_loss: -3.0284\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5562 - val_loss: -3.0284\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5570 - val_loss: -3.0284\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5579 - val_loss: -3.0284\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5571 - val_loss: -3.0284\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5502 - val_loss: -3.0284\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5561 - val_loss: -3.0284\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5542 - val_loss: -3.0284\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5552 - val_loss: -3.0285\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5539 - val_loss: -3.0286\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -0.5521 - val_loss: -3.0288\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5580 - val_loss: -3.0288\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.5546 - val_loss: -3.0289\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5579 - val_loss: -3.0289\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5592 - val_loss: -3.0289\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5598 - val_loss: -3.0288\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5528 - val_loss: -3.0287\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5571 - val_loss: -3.0287\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5580 - val_loss: -3.0287\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5564 - val_loss: -3.0287\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5611 - val_loss: -3.0285\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.5507 - val_loss: -3.0284\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5508 - val_loss: -3.0285\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5552 - val_loss: -3.0285\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5557 - val_loss: -3.0286\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.5544 - val_loss: -3.0287\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5548 - val_loss: -3.0288\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5572 - val_loss: -3.0288\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5595 - val_loss: -3.0288\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.5568 - val_loss: -3.0288\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.5540 - val_loss: -3.0289\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5537 - val_loss: -3.0291\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5532 - val_loss: -3.0294\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5583 - val_loss: -3.0296\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5589 - val_loss: -3.0297\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5557 - val_loss: -3.0298\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5579 - val_loss: -3.0299\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5569 - val_loss: -3.0300\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -0.5533 - val_loss: -3.0301\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5583 - val_loss: -3.0301\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5543 - val_loss: -3.0302\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5574 - val_loss: -3.0303\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5552 - val_loss: -3.0304\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5567 - val_loss: -3.0305\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5588 - val_loss: -3.0306\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5581 - val_loss: -3.0306\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5553 - val_loss: -3.0305\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5538 - val_loss: -3.0305\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -0.5535 - val_loss: -3.0305\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5528 - val_loss: -3.0306\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5594 - val_loss: -3.0306\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5542 - val_loss: -3.0307\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5522 - val_loss: -3.0308\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5559 - val_loss: -3.0309\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5542 - val_loss: -3.0310\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5480 - val_loss: -3.0313\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5546 - val_loss: -3.0316\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5616 - val_loss: -3.0318\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5577 - val_loss: -3.0319\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5499 - val_loss: -3.0322\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5562 - val_loss: -3.0327\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5584 - val_loss: -3.0332\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5572 - val_loss: -3.0335\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5487 - val_loss: -3.0337\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5578 - val_loss: -3.0340\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5597 - val_loss: -3.0342\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5555 - val_loss: -3.0344\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5477 - val_loss: -3.0347\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5540 - val_loss: -3.0351\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5465 - val_loss: -3.0355\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -0.5588 - val_loss: -3.0358\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5513 - val_loss: -3.0362\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5590 - val_loss: -3.0365\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5556 - val_loss: -3.0367\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5566 - val_loss: -3.0369\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5527 - val_loss: -3.0371\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5552 - val_loss: -3.0372\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5569 - val_loss: -3.0374\n",
      "Epoch 360/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5578 - val_loss: -3.0375\n",
      "Epoch 361/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5556 - val_loss: -3.0376\n",
      "Epoch 362/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5564 - val_loss: -3.0376\n",
      "Epoch 363/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5581 - val_loss: -3.0377\n",
      "Epoch 364/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5537 - val_loss: -3.0377\n",
      "Epoch 365/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5575 - val_loss: -3.0377\n",
      "Epoch 366/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5528 - val_loss: -3.0378\n",
      "Epoch 367/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5582 - val_loss: -3.0378\n",
      "Epoch 368/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5538 - val_loss: -3.0379\n",
      "Epoch 369/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5565 - val_loss: -3.0380\n",
      "Epoch 370/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5555 - val_loss: -3.0381\n",
      "Epoch 371/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5554 - val_loss: -3.0382\n",
      "Epoch 372/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -0.5569 - val_loss: -3.0384\n",
      "Epoch 373/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.5548 - val_loss: -3.0385\n",
      "Epoch 374/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5533 - val_loss: -3.0388\n",
      "Epoch 375/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.5569 - val_loss: -3.0390\n",
      "Epoch 376/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5607 - val_loss: -3.0392\n",
      "Epoch 377/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5551 - val_loss: -3.0393\n",
      "Epoch 378/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5580 - val_loss: -3.0394\n",
      "Epoch 379/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5553 - val_loss: -3.0395\n",
      "Epoch 380/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5556 - val_loss: -3.0396\n",
      "Epoch 381/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5587 - val_loss: -3.0396\n",
      "Epoch 382/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -0.5573 - val_loss: -3.0397\n",
      "Epoch 383/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5586 - val_loss: -3.0397\n",
      "Epoch 384/1000\n",
      "2/2 [==============================] - 0s 146ms/step - loss: -0.5562 - val_loss: -3.0397\n",
      "Epoch 385/1000\n",
      "2/2 [==============================] - 0s 142ms/step - loss: -0.5541 - val_loss: -3.0397\n",
      "Epoch 386/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -0.5569 - val_loss: -3.0397\n",
      "Epoch 387/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5570 - val_loss: -3.0398\n",
      "Epoch 388/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5555 - val_loss: -3.0398\n",
      "Epoch 389/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5564 - val_loss: -3.0399\n",
      "Epoch 390/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5536 - val_loss: -3.0400\n",
      "Epoch 391/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5572 - val_loss: -3.0401\n",
      "Epoch 392/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5582 - val_loss: -3.0401\n",
      "Epoch 393/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5572 - val_loss: -3.0402\n",
      "Epoch 394/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5576 - val_loss: -3.0402\n",
      "Epoch 395/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5601 - val_loss: -3.0401\n",
      "Epoch 396/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5559 - val_loss: -3.0401\n",
      "Epoch 397/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5533 - val_loss: -3.0401\n",
      "Epoch 398/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5565 - val_loss: -3.0401\n",
      "Epoch 399/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5582 - val_loss: -3.0401\n",
      "Epoch 400/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5570 - val_loss: -3.0401\n",
      "Epoch 401/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5563 - val_loss: -3.0401\n",
      "Epoch 402/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5539 - val_loss: -3.0402\n",
      "Epoch 403/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5586 - val_loss: -3.0402\n",
      "Epoch 404/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5556 - val_loss: -3.0403\n",
      "Epoch 405/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.5518 - val_loss: -3.0405\n",
      "Epoch 406/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5557 - val_loss: -3.0406\n",
      "Epoch 407/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5570 - val_loss: -3.0407\n",
      "Epoch 408/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5588 - val_loss: -3.0408\n",
      "Epoch 409/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5584 - val_loss: -3.0408\n",
      "Epoch 410/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5581 - val_loss: -3.0407\n",
      "Epoch 411/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5537 - val_loss: -3.0407\n",
      "Epoch 412/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5581 - val_loss: -3.0407\n",
      "Epoch 413/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5582 - val_loss: -3.0407\n",
      "Epoch 414/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5554 - val_loss: -3.0406\n",
      "Epoch 415/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5573 - val_loss: -3.0406\n",
      "Epoch 416/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5556 - val_loss: -3.0407\n",
      "Epoch 417/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5568 - val_loss: -3.0407\n",
      "Epoch 418/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5576 - val_loss: -3.0408\n",
      "Epoch 419/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5582 - val_loss: -3.0409\n",
      "Epoch 420/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5570 - val_loss: -3.0410\n",
      "Epoch 421/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5576 - val_loss: -3.0410\n",
      "Epoch 422/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5574 - val_loss: -3.0410\n",
      "Epoch 423/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -0.5551 - val_loss: -3.0410\n",
      "Epoch 424/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5551 - val_loss: -3.0411\n",
      "Epoch 425/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -0.5537 - val_loss: -3.0411\n",
      "Epoch 426/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5588 - val_loss: -3.0412\n",
      "Epoch 427/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5582 - val_loss: -3.0412\n",
      "Epoch 428/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -0.5567 - val_loss: -3.0412\n",
      "Epoch 429/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5572 - val_loss: -3.0412\n",
      "Epoch 430/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5601 - val_loss: -3.0411\n",
      "Epoch 431/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5583 - val_loss: -3.0410\n",
      "Epoch 432/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5585 - val_loss: -3.0409\n",
      "Epoch 433/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5588 - val_loss: -3.0407\n",
      "Epoch 434/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5572 - val_loss: -3.0406\n",
      "Epoch 435/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5578 - val_loss: -3.0406\n",
      "Epoch 436/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5538 - val_loss: -3.0406\n",
      "Epoch 437/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5556 - val_loss: -3.0406\n",
      "Epoch 438/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5563 - val_loss: -3.0407\n",
      "Epoch 439/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5573 - val_loss: -3.0407\n",
      "Epoch 440/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5596 - val_loss: -3.0408\n",
      "Epoch 441/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5567 - val_loss: -3.0408\n",
      "Epoch 442/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5533 - val_loss: -3.0410\n",
      "Epoch 443/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5584 - val_loss: -3.0412\n",
      "Epoch 444/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5564 - val_loss: -3.0413\n",
      "Epoch 445/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5537 - val_loss: -3.0415\n",
      "Epoch 446/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5580 - val_loss: -3.0416\n",
      "Epoch 447/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5556 - val_loss: -3.0417\n",
      "Epoch 448/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5553 - val_loss: -3.0418\n",
      "Epoch 449/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5557 - val_loss: -3.0419\n",
      "Epoch 450/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.5570 - val_loss: -3.0420\n",
      "Epoch 451/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5561 - val_loss: -3.0421\n",
      "Epoch 452/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5573 - val_loss: -3.0421\n",
      "Epoch 453/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5541 - val_loss: -3.0422\n",
      "Epoch 454/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5573 - val_loss: -3.0423\n",
      "Epoch 455/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.5579 - val_loss: -3.0423\n",
      "Epoch 456/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -0.5571 - val_loss: -3.0423\n",
      "Epoch 457/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5578 - val_loss: -3.0423\n",
      "Epoch 458/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5552 - val_loss: -3.0423\n",
      "Epoch 459/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5580 - val_loss: -3.0424\n",
      "Epoch 460/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5532 - val_loss: -3.0425\n",
      "Epoch 461/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5593 - val_loss: -3.0425\n",
      "Epoch 462/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5568 - val_loss: -3.0425\n",
      "Epoch 463/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5558 - val_loss: -3.0425\n",
      "Epoch 464/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -0.5575 - val_loss: -3.0425\n",
      "Epoch 465/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5560 - val_loss: -3.0425\n",
      "Epoch 466/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5554 - val_loss: -3.0425\n",
      "Epoch 467/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5575 - val_loss: -3.0425\n",
      "Epoch 468/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5543 - val_loss: -3.0425\n",
      "Epoch 469/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5569 - val_loss: -3.0425\n",
      "Epoch 470/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.5595 - val_loss: -3.0425\n",
      "Epoch 471/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.5553 - val_loss: -3.0425\n",
      "Epoch 472/1000\n",
      "2/2 [==============================] - 0s 139ms/step - loss: -0.5564 - val_loss: -3.0425\n",
      "Epoch 473/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5541 - val_loss: -3.0425\n",
      "Epoch 474/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5609 - val_loss: -3.0425\n",
      "Epoch 475/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5585 - val_loss: -3.0425\n",
      "Epoch 476/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5552 - val_loss: -3.0425\n",
      "Epoch 477/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.5574 - val_loss: -3.0424\n",
      "Epoch 478/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5568 - val_loss: -3.0424\n",
      "Epoch 479/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5565 - val_loss: -3.0424\n",
      "Epoch 480/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5581 - val_loss: -3.0424\n",
      "Epoch 481/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5582 - val_loss: -3.0423\n",
      "Epoch 482/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5552 - val_loss: -3.0423\n",
      "Epoch 483/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5568 - val_loss: -3.0424\n",
      "Epoch 484/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5564 - val_loss: -3.0425\n",
      "Epoch 485/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5592 - val_loss: -3.0426\n",
      "Epoch 486/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5578 - val_loss: -3.0427\n",
      "Epoch 487/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.5582 - val_loss: -3.0427\n",
      "Epoch 488/1000\n",
      "2/2 [==============================] - 0s 138ms/step - loss: -0.5597 - val_loss: -3.0427\n",
      "Epoch 489/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.5588 - val_loss: -3.0426\n",
      "Epoch 490/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5572 - val_loss: -3.0425\n",
      "Epoch 491/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5587 - val_loss: -3.0425\n",
      "Epoch 492/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5556 - val_loss: -3.0425\n",
      "Epoch 493/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5561 - val_loss: -3.0425\n",
      "Epoch 494/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.5590 - val_loss: -3.0425\n",
      "Epoch 495/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5557 - val_loss: -3.0424\n",
      "Epoch 496/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5591 - val_loss: -3.0423\n",
      "Epoch 497/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5554 - val_loss: -3.0423\n",
      "Epoch 498/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5561 - val_loss: -3.0422\n",
      "Epoch 499/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5591 - val_loss: -3.0421\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00499: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_7 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 497ms/step - loss: -0.5575 - val_loss: -1.6103\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.5192 - val_loss: -1.6325\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.5168 - val_loss: -1.6547\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5569 - val_loss: -1.6773\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5325 - val_loss: -1.7005\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 151ms/step - loss: -0.5377 - val_loss: -1.7240\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5501 - val_loss: -1.7477\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.5588 - val_loss: -1.7712\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5708 - val_loss: -1.7949\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5785 - val_loss: -1.8191\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5764 - val_loss: -1.8435\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5635 - val_loss: -1.8684\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5410 - val_loss: -1.8939\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5797 - val_loss: -1.9202\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5894 - val_loss: -1.9460\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5626 - val_loss: -1.9712\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5846 - val_loss: -1.9966\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5695 - val_loss: -2.0224\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -0.5907 - val_loss: -2.0492\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5903 - val_loss: -2.0763\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5831 - val_loss: -2.1040\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -0.5808 - val_loss: -2.1311\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5990 - val_loss: -2.1575\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5767 - val_loss: -2.1839\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5720 - val_loss: -2.2098\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 137ms/step - loss: -0.5717 - val_loss: -2.2365\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6128 - val_loss: -2.2628\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5728 - val_loss: -2.2892\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5901 - val_loss: -2.3152\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.6009 - val_loss: -2.3386\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.6040 - val_loss: -2.3600\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5931 - val_loss: -2.3814\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.5892 - val_loss: -2.4037\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5995 - val_loss: -2.4251\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.6047 - val_loss: -2.4449\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.6024 - val_loss: -2.4654\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5973 - val_loss: -2.4863\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5915 - val_loss: -2.5086\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5790 - val_loss: -2.5323\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6371 - val_loss: -2.5545\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6053 - val_loss: -2.5757\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6055 - val_loss: -2.5957\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6219 - val_loss: -2.6148\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6064 - val_loss: -2.6325\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.6277 - val_loss: -2.6482\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.6024 - val_loss: -2.6623\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.6027 - val_loss: -2.6765\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.6457 - val_loss: -2.6890\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5666 - val_loss: -2.7017\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.6158 - val_loss: -2.7151\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.6037 - val_loss: -2.7283\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.6101 - val_loss: -2.7412\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5980 - val_loss: -2.7540\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.6059 - val_loss: -2.7658\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6128 - val_loss: -2.7773\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6118 - val_loss: -2.7898\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6073 - val_loss: -2.8033\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -0.6255 - val_loss: -2.8178\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5895 - val_loss: -2.8336\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.6333 - val_loss: -2.8482\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.6201 - val_loss: -2.8610\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -0.6134 - val_loss: -2.8729\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6314 - val_loss: -2.8834\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6069 - val_loss: -2.8937\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6240 - val_loss: -2.9038\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6171 - val_loss: -2.9137\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.6306 - val_loss: -2.9231\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6065 - val_loss: -2.9336\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6219 - val_loss: -2.9447\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.6114 - val_loss: -2.9562\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.6219 - val_loss: -2.9677\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5943 - val_loss: -2.9787\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.6332 - val_loss: -2.9890\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6436 - val_loss: -2.9975\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.6211 - val_loss: -3.0045\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6293 - val_loss: -3.0105\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.6323 - val_loss: -3.0156\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6357 - val_loss: -3.0192\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.6248 - val_loss: -3.0218\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.6422 - val_loss: -3.0232\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.6135 - val_loss: -3.0248\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6312 - val_loss: -3.0260\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.6371 - val_loss: -3.0270\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6145 - val_loss: -3.0287\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.6026 - val_loss: -3.0292\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.6241 - val_loss: -3.0302\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6355 - val_loss: -3.0318\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.6102 - val_loss: -3.0341\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.6396 - val_loss: -3.0355\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.6390 - val_loss: -3.0361\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.6150 - val_loss: -3.0376\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6295 - val_loss: -3.0394\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.6141 - val_loss: -3.0419\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6205 - val_loss: -3.0441\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6306 - val_loss: -3.0448\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.5949 - val_loss: -3.0465\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6266 - val_loss: -3.0496\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6307 - val_loss: -3.0531\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.6380 - val_loss: -3.0559\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6178 - val_loss: -3.0570\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.6306 - val_loss: -3.0571\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.6299 - val_loss: -3.0563\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6305 - val_loss: -3.0553\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.6273 - val_loss: -3.0543\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6274 - val_loss: -3.0535\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.6272 - val_loss: -3.0524\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.6369 - val_loss: -3.0515\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.6162 - val_loss: -3.0516\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.6289 - val_loss: -3.0508\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6176 - val_loss: -3.0500\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6362 - val_loss: -3.0488\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6242 - val_loss: -3.0471\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.6314 - val_loss: -3.0442\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.6272 - val_loss: -3.0415\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.6350 - val_loss: -3.0397\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.6295 - val_loss: -3.0378\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.6361 - val_loss: -3.0358\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.6448 - val_loss: -3.0338\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6428 - val_loss: -3.0317\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 145ms/step - loss: -0.6365 - val_loss: -3.0302\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6554 - val_loss: -3.0286\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6384 - val_loss: -3.0265\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.6291 - val_loss: -3.0242\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.6302 - val_loss: -3.0219\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.6333 - val_loss: -3.0197\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.6317 - val_loss: -3.0174\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6392 - val_loss: -3.0150\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6292 - val_loss: -3.0126\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.6314 - val_loss: -3.0100\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.6453 - val_loss: -3.0064\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6410 - val_loss: -3.0029\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.6434 - val_loss: -2.9998\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6324 - val_loss: -2.9976\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6261 - val_loss: -2.9964\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.6346 - val_loss: -2.9968\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6300 - val_loss: -2.9976\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.6239 - val_loss: -2.9976\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.6323 - val_loss: -2.9974\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6389 - val_loss: -2.9961\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6397 - val_loss: -2.9936\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.6429 - val_loss: -2.9903\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.6278 - val_loss: -2.9876\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6504 - val_loss: -2.9854\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6442 - val_loss: -2.9831\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6516 - val_loss: -2.9812\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.6274 - val_loss: -2.9795\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.6425 - val_loss: -2.9780\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.6316 - val_loss: -2.9767\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.6311 - val_loss: -2.9759\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6249 - val_loss: -2.9748\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00150: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_8 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 501ms/step - loss: -1.3386 - val_loss: -2.6568\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.3190 - val_loss: -2.6632\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3509 - val_loss: -2.6709\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.3176 - val_loss: -2.6786\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.3012 - val_loss: -2.6914\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.3785 - val_loss: -2.7075\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.3584 - val_loss: -2.7240\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.3845 - val_loss: -2.7412\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.3518 - val_loss: -2.7593\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.3718 - val_loss: -2.7774\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3845 - val_loss: -2.7942\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.4070 - val_loss: -2.8124\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3388 - val_loss: -2.8305\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.3426 - val_loss: -2.8490\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.3970 - val_loss: -2.8653\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.3564 - val_loss: -2.8794\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.3883 - val_loss: -2.8941\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.3731 - val_loss: -2.9078\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.4075 - val_loss: -2.9229\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.4160 - val_loss: -2.9375\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4101 - val_loss: -2.9530\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.3857 - val_loss: -2.9635\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3813 - val_loss: -2.9720\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.3807 - val_loss: -2.9821\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3974 - val_loss: -2.9927\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.3821 - val_loss: -3.0023\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.3966 - val_loss: -3.0111\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3871 - val_loss: -3.0186\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.4109 - val_loss: -3.0246\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.4015 - val_loss: -3.0308\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.4314 - val_loss: -3.0372\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.3696 - val_loss: -3.0439\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.4200 - val_loss: -3.0503\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.4045 - val_loss: -3.0566\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.4221 - val_loss: -3.0604\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.4039 - val_loss: -3.0636\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.4277 - val_loss: -3.0655\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.4067 - val_loss: -3.0658\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3932 - val_loss: -3.0644\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.4010 - val_loss: -3.0623\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.3973 - val_loss: -3.0610\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.4115 - val_loss: -3.0612\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.4060 - val_loss: -3.0613\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.4257 - val_loss: -3.0589\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.4321 - val_loss: -3.0560\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.4393 - val_loss: -3.0541\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.4022 - val_loss: -3.0524\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3929 - val_loss: -3.0514\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.4028 - val_loss: -3.0493\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.4147 - val_loss: -3.0459\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4051 - val_loss: -3.0436\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.4255 - val_loss: -3.0411\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.4146 - val_loss: -3.0380\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.4158 - val_loss: -3.0345\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.4237 - val_loss: -3.0306\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.4075 - val_loss: -3.0281\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -1.3816 - val_loss: -3.0256\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.4088 - val_loss: -3.0216\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.4080 - val_loss: -3.0176\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.4048 - val_loss: -3.0144\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.4080 - val_loss: -3.0117\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.4464 - val_loss: -3.0089\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.4146 - val_loss: -3.0059\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.3739 - val_loss: -3.0026\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4600 - val_loss: -2.9992\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4579 - val_loss: -2.9959\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.4592 - val_loss: -2.9932\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.4683 - val_loss: -2.9913\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.4214 - val_loss: -2.9893\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.4138 - val_loss: -2.9876\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.4497 - val_loss: -2.9868\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4052 - val_loss: -2.9848\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.4129 - val_loss: -2.9822\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.4447 - val_loss: -2.9793\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.4147 - val_loss: -2.9769\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 138ms/step - loss: -1.4588 - val_loss: -2.9750\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.3999 - val_loss: -2.9724\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.4411 - val_loss: -2.9700\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.4113 - val_loss: -2.9673\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4122 - val_loss: -2.9638\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.3917 - val_loss: -2.9599\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.4316 - val_loss: -2.9568\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.4449 - val_loss: -2.9533\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.4291 - val_loss: -2.9493\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.4316 - val_loss: -2.9451\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.4189 - val_loss: -2.9412\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.4483 - val_loss: -2.9380\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00087: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_9 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 563ms/step - loss: -1.5271 - val_loss: -1.9063\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.5050 - val_loss: -1.9174\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.5077 - val_loss: -1.9306\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.5347 - val_loss: -1.9422\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.4897 - val_loss: -1.9512\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.5369 - val_loss: -1.9588\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.5809 - val_loss: -1.9669\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -1.4851 - val_loss: -1.9747\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.5420 - val_loss: -1.9815\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.5653 - val_loss: -1.9871\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.4886 - val_loss: -1.9923\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.5967 - val_loss: -1.9981\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.5601 - val_loss: -2.0039\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.5561 - val_loss: -2.0093\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.5755 - val_loss: -2.0152\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.5954 - val_loss: -2.0200\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.5333 - val_loss: -2.0234\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.5513 - val_loss: -2.0267\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.6203 - val_loss: -2.0299\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.5627 - val_loss: -2.0312\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.5888 - val_loss: -2.0317\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.5993 - val_loss: -2.0328\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.5752 - val_loss: -2.0340\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.5993 - val_loss: -2.0359\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6083 - val_loss: -2.0393\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6151 - val_loss: -2.0432\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.5799 - val_loss: -2.0468\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.5621 - val_loss: -2.0511\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.5865 - val_loss: -2.0546\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.5924 - val_loss: -2.0583\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5513 - val_loss: -2.0625\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6127 - val_loss: -2.0662\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6238 - val_loss: -2.0680\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5910 - val_loss: -2.0694\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6218 - val_loss: -2.0717\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.5863 - val_loss: -2.0744\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6145 - val_loss: -2.0769\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.6261 - val_loss: -2.0784\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6363 - val_loss: -2.0798\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.6012 - val_loss: -2.0819\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.6286 - val_loss: -2.0848\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.6457 - val_loss: -2.0872\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.6558 - val_loss: -2.0894\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6693 - val_loss: -2.0915\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.5903 - val_loss: -2.0932\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.6042 - val_loss: -2.0944\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6119 - val_loss: -2.0954\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6202 - val_loss: -2.0963\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.6222 - val_loss: -2.0968\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6155 - val_loss: -2.0968\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.6240 - val_loss: -2.0963\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -1.6332 - val_loss: -2.0960\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6612 - val_loss: -2.0959\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6484 - val_loss: -2.0964\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.6145 - val_loss: -2.0966\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.6906 - val_loss: -2.0966\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.6452 - val_loss: -2.0975\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6111 - val_loss: -2.0991\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6430 - val_loss: -2.1012\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.5801 - val_loss: -2.1031\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6573 - val_loss: -2.1048\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6187 - val_loss: -2.1065\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6626 - val_loss: -2.1083\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.6283 - val_loss: -2.1100\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6538 - val_loss: -2.1117\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6610 - val_loss: -2.1124\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6065 - val_loss: -2.1122\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.6622 - val_loss: -2.1123\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.6419 - val_loss: -2.1127\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.6251 - val_loss: -2.1133\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6308 - val_loss: -2.1144\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6452 - val_loss: -2.1155\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6262 - val_loss: -2.1167\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6306 - val_loss: -2.1170\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6031 - val_loss: -2.1166\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.6359 - val_loss: -2.1163\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6427 - val_loss: -2.1162\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6734 - val_loss: -2.1167\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6761 - val_loss: -2.1167\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.6232 - val_loss: -2.1164\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.6234 - val_loss: -2.1156\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6312 - val_loss: -2.1149\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6588 - val_loss: -2.1139\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6361 - val_loss: -2.1134\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.6065 - val_loss: -2.1133\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6500 - val_loss: -2.1132\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6650 - val_loss: -2.1134\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.6388 - val_loss: -2.1135\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6354 - val_loss: -2.1133\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.6722 - val_loss: -2.1127\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.5999 - val_loss: -2.1119\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6450 - val_loss: -2.1111\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -1.6483 - val_loss: -2.1099\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.6375 - val_loss: -2.1083\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6487 - val_loss: -2.1067\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6720 - val_loss: -2.1053\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.6523 - val_loss: -2.1043\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.6941 - val_loss: -2.1033\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6229 - val_loss: -2.1026\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6257 - val_loss: -2.1021\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.6295 - val_loss: -2.1017\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6462 - val_loss: -2.1009\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.6652 - val_loss: -2.1001\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6584 - val_loss: -2.0998\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.6509 - val_loss: -2.0994\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6997 - val_loss: -2.0995\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6425 - val_loss: -2.0999\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.6321 - val_loss: -2.1002\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.6441 - val_loss: -2.1004\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7026 - val_loss: -2.1008\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6367 - val_loss: -2.1014\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6363 - val_loss: -2.1024\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6530 - val_loss: -2.1030\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6846 - val_loss: -2.1035\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6454 - val_loss: -2.1037\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6433 - val_loss: -2.1038\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6887 - val_loss: -2.1039\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6648 - val_loss: -2.1040\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6619 - val_loss: -2.1042\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6909 - val_loss: -2.1044\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.6526 - val_loss: -2.1048\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.6598 - val_loss: -2.1053\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6418 - val_loss: -2.1057\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00123: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_10 (Softmax)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 505ms/step - loss: -1.1929 - val_loss: -0.5909\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.2034 - val_loss: -0.5969\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.1810 - val_loss: -0.6029\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.2331 - val_loss: -0.6090\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.2269 - val_loss: -0.6151\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.2572 - val_loss: -0.6214\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -1.2876 - val_loss: -0.6277\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.3201 - val_loss: -0.6339\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.2668 - val_loss: -0.6401\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.2446 - val_loss: -0.6463\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3413 - val_loss: -0.6526\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.2148 - val_loss: -0.6589\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.2680 - val_loss: -0.6654\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.2736 - val_loss: -0.6720\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3106 - val_loss: -0.6785\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.3254 - val_loss: -0.6852\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -1.3502 - val_loss: -0.6919\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.3094 - val_loss: -0.6989\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.4276 - val_loss: -0.7059\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4095 - val_loss: -0.7129\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3666 - val_loss: -0.7199\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.3894 - val_loss: -0.7271\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.3948 - val_loss: -0.7344\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3234 - val_loss: -0.7418\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3301 - val_loss: -0.7491\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.4011 - val_loss: -0.7562\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.4133 - val_loss: -0.7634\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.4663 - val_loss: -0.7706\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.4099 - val_loss: -0.7778\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.4487 - val_loss: -0.7847\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.4385 - val_loss: -0.7915\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4670 - val_loss: -0.7983\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.4212 - val_loss: -0.8053\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.4879 - val_loss: -0.8124\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.4647 - val_loss: -0.8196\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.4604 - val_loss: -0.8269\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.5069 - val_loss: -0.8344\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.5657 - val_loss: -0.8419\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.5024 - val_loss: -0.8493\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.4790 - val_loss: -0.8569\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.4515 - val_loss: -0.8644\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5106 - val_loss: -0.8720\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.4635 - val_loss: -0.8794\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.5342 - val_loss: -0.8867\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.5174 - val_loss: -0.8937\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.4911 - val_loss: -0.9005\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.4863 - val_loss: -0.9072\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.5061 - val_loss: -0.9140\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.5717 - val_loss: -0.9206\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.5166 - val_loss: -0.9271\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.5652 - val_loss: -0.9334\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.5999 - val_loss: -0.9396\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.4917 - val_loss: -0.9459\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.5655 - val_loss: -0.9522\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.5304 - val_loss: -0.9582\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.5223 - val_loss: -0.9642\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.5233 - val_loss: -0.9699\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.5532 - val_loss: -0.9755\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.5874 - val_loss: -0.9811\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.5712 - val_loss: -0.9867\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5533 - val_loss: -0.9925\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6265 - val_loss: -0.9983\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6191 - val_loss: -1.0038\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.6258 - val_loss: -1.0093\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.5976 - val_loss: -1.0148\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.5667 - val_loss: -1.0203\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6439 - val_loss: -1.0257\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6152 - val_loss: -1.0310\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.5317 - val_loss: -1.0363\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.5304 - val_loss: -1.0418\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6123 - val_loss: -1.0473\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6313 - val_loss: -1.0530\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6711 - val_loss: -1.0588\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.5665 - val_loss: -1.0645\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6250 - val_loss: -1.0701\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.5769 - val_loss: -1.0754\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6697 - val_loss: -1.0805\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6377 - val_loss: -1.0856\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6978 - val_loss: -1.0903\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.6256 - val_loss: -1.0952\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.6650 - val_loss: -1.1001\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.6081 - val_loss: -1.1052\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6348 - val_loss: -1.1103\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.6546 - val_loss: -1.1153\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7195 - val_loss: -1.1202\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7083 - val_loss: -1.1250\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6665 - val_loss: -1.1295\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.6530 - val_loss: -1.1336\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7199 - val_loss: -1.1378\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6983 - val_loss: -1.1422\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6537 - val_loss: -1.1465\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.7309 - val_loss: -1.1508\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6950 - val_loss: -1.1551\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7320 - val_loss: -1.1592\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6554 - val_loss: -1.1631\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6950 - val_loss: -1.1668\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7393 - val_loss: -1.1705\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6365 - val_loss: -1.1746\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7037 - val_loss: -1.1787\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7078 - val_loss: -1.1826\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.6670 - val_loss: -1.1865\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7298 - val_loss: -1.1904\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.7061 - val_loss: -1.1941\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7106 - val_loss: -1.1976\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7655 - val_loss: -1.2010\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7180 - val_loss: -1.2044\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7276 - val_loss: -1.2079\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.6874 - val_loss: -1.2114\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7663 - val_loss: -1.2148\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7219 - val_loss: -1.2181\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.7258 - val_loss: -1.2213\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6860 - val_loss: -1.2246\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7079 - val_loss: -1.2277\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7222 - val_loss: -1.2308\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6978 - val_loss: -1.2339\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7499 - val_loss: -1.2369\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7309 - val_loss: -1.2398\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7318 - val_loss: -1.2426\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.7194 - val_loss: -1.2457\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7210 - val_loss: -1.2487\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7211 - val_loss: -1.2516\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7636 - val_loss: -1.2541\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7592 - val_loss: -1.2566\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7669 - val_loss: -1.2591\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.7764 - val_loss: -1.2617\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7198 - val_loss: -1.2644\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.7712 - val_loss: -1.2670\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.7601 - val_loss: -1.2693\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7537 - val_loss: -1.2714\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.7699 - val_loss: -1.2735\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7585 - val_loss: -1.2755\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7421 - val_loss: -1.2775\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7369 - val_loss: -1.2795\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.7637 - val_loss: -1.2814\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7686 - val_loss: -1.2833\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.7636 - val_loss: -1.2850\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7773 - val_loss: -1.2869\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7660 - val_loss: -1.2888\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7561 - val_loss: -1.2907\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7649 - val_loss: -1.2925\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.7054 - val_loss: -1.2943\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7545 - val_loss: -1.2961\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.7565 - val_loss: -1.2977\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6954 - val_loss: -1.2993\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7767 - val_loss: -1.3008\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8081 - val_loss: -1.3023\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.7678 - val_loss: -1.3038\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.7423 - val_loss: -1.3054\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.7224 - val_loss: -1.3070\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7584 - val_loss: -1.3085\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7540 - val_loss: -1.3100\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7849 - val_loss: -1.3114\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7919 - val_loss: -1.3127\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7612 - val_loss: -1.3138\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7850 - val_loss: -1.3149\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7555 - val_loss: -1.3159\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7709 - val_loss: -1.3168\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7811 - val_loss: -1.3177\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.8084 - val_loss: -1.3187\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 152ms/step - loss: -1.8021 - val_loss: -1.3197\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7661 - val_loss: -1.3207\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8131 - val_loss: -1.3217\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.7843 - val_loss: -1.3226\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7865 - val_loss: -1.3236\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7696 - val_loss: -1.3245\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7922 - val_loss: -1.3253\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.8054 - val_loss: -1.3260\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.7780 - val_loss: -1.3268\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7982 - val_loss: -1.3277\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8025 - val_loss: -1.3285\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.7692 - val_loss: -1.3294\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.7480 - val_loss: -1.3304\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.7734 - val_loss: -1.3316\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7608 - val_loss: -1.3328\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7817 - val_loss: -1.3338\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7921 - val_loss: -1.3347\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.7750 - val_loss: -1.3357\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8046 - val_loss: -1.3366\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7866 - val_loss: -1.3375\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.7900 - val_loss: -1.3383\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7881 - val_loss: -1.3390\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7821 - val_loss: -1.3397\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7960 - val_loss: -1.3404\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.7934 - val_loss: -1.3410\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.7534 - val_loss: -1.3417\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7746 - val_loss: -1.3424\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7964 - val_loss: -1.3432\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.7924 - val_loss: -1.3438\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8068 - val_loss: -1.3444\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8049 - val_loss: -1.3450\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.7752 - val_loss: -1.3455\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8027 - val_loss: -1.3461\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7893 - val_loss: -1.3466\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8154 - val_loss: -1.3471\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -1.7729 - val_loss: -1.3476\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.7911 - val_loss: -1.3480\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7947 - val_loss: -1.3484\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8122 - val_loss: -1.3488\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8005 - val_loss: -1.3492\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.7949 - val_loss: -1.3496\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7893 - val_loss: -1.3501\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7687 - val_loss: -1.3507\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7936 - val_loss: -1.3512\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.7900 - val_loss: -1.3517\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7926 - val_loss: -1.3522\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8111 - val_loss: -1.3527\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7873 - val_loss: -1.3532\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.8028 - val_loss: -1.3537\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.7683 - val_loss: -1.3543\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7753 - val_loss: -1.3548\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8178 - val_loss: -1.3553\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7929 - val_loss: -1.3557\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7888 - val_loss: -1.3561\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.7707 - val_loss: -1.3566\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7970 - val_loss: -1.3571\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8018 - val_loss: -1.3576\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7817 - val_loss: -1.3580\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7578 - val_loss: -1.3584\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8016 - val_loss: -1.3588\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.7809 - val_loss: -1.3592\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.7984 - val_loss: -1.3597\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7927 - val_loss: -1.3600\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7998 - val_loss: -1.3605\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7971 - val_loss: -1.3610\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7932 - val_loss: -1.3614\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8088 - val_loss: -1.3619\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7952 - val_loss: -1.3623\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7942 - val_loss: -1.3627\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8027 - val_loss: -1.3631\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.7955 - val_loss: -1.3635\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.8101 - val_loss: -1.3638\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8031 - val_loss: -1.3641\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7958 - val_loss: -1.3643\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8156 - val_loss: -1.3645\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8065 - val_loss: -1.3646\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7847 - val_loss: -1.3649\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7897 - val_loss: -1.3651\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.7929 - val_loss: -1.3654\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.8135 - val_loss: -1.3656\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7964 - val_loss: -1.3659\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7794 - val_loss: -1.3662\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8072 - val_loss: -1.3666\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7896 - val_loss: -1.3669\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7743 - val_loss: -1.3673\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.7912 - val_loss: -1.3677\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7985 - val_loss: -1.3681\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7916 - val_loss: -1.3686\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.7618 - val_loss: -1.3691\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7881 - val_loss: -1.3696\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8069 - val_loss: -1.3701\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7983 - val_loss: -1.3705\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.7889 - val_loss: -1.3710\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.7952 - val_loss: -1.3714\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.8123 - val_loss: -1.3719\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8009 - val_loss: -1.3722\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8090 - val_loss: -1.3726\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7876 - val_loss: -1.3729\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8170 - val_loss: -1.3731\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.7991 - val_loss: -1.3732\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8004 - val_loss: -1.3734\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.7905 - val_loss: -1.3736\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7882 - val_loss: -1.3738\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8061 - val_loss: -1.3740\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.7879 - val_loss: -1.3742\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.7715 - val_loss: -1.3744\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8129 - val_loss: -1.3747\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8004 - val_loss: -1.3749\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.7872 - val_loss: -1.3751\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8048 - val_loss: -1.3754\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7748 - val_loss: -1.3756\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8185 - val_loss: -1.3758\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8045 - val_loss: -1.3760\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7788 - val_loss: -1.3762\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.7969 - val_loss: -1.3765\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7946 - val_loss: -1.3767\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7773 - val_loss: -1.3769\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8114 - val_loss: -1.3771\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8021 - val_loss: -1.3773\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7913 - val_loss: -1.3775\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7968 - val_loss: -1.3778\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7880 - val_loss: -1.3781\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7924 - val_loss: -1.3783\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.8226 - val_loss: -1.3785\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.7997 - val_loss: -1.3787\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.7831 - val_loss: -1.3789\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7927 - val_loss: -1.3790\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7858 - val_loss: -1.3792\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8124 - val_loss: -1.3795\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8102 - val_loss: -1.3797\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8044 - val_loss: -1.3798\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8023 - val_loss: -1.3800\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8171 - val_loss: -1.3801\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8101 - val_loss: -1.3801\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.7807 - val_loss: -1.3802\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7866 - val_loss: -1.3802\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8050 - val_loss: -1.3803\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8006 - val_loss: -1.3804\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.8176 - val_loss: -1.3804\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8203 - val_loss: -1.3805\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7881 - val_loss: -1.3806\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8078 - val_loss: -1.3807\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7915 - val_loss: -1.3808\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8045 - val_loss: -1.3809\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.7961 - val_loss: -1.3809\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8091 - val_loss: -1.3810\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8016 - val_loss: -1.3812\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7980 - val_loss: -1.3813\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8012 - val_loss: -1.3815\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7920 - val_loss: -1.3816\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7969 - val_loss: -1.3818\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8113 - val_loss: -1.3820\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8007 - val_loss: -1.3822\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8047 - val_loss: -1.3824\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7904 - val_loss: -1.3825\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8066 - val_loss: -1.3827\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.8022 - val_loss: -1.3829\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8064 - val_loss: -1.3830\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8025 - val_loss: -1.3831\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8018 - val_loss: -1.3832\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8191 - val_loss: -1.3833\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.7957 - val_loss: -1.3834\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7988 - val_loss: -1.3835\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8012 - val_loss: -1.3836\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8145 - val_loss: -1.3837\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7900 - val_loss: -1.3839\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: -1.7846 - val_loss: -1.3841\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8105 - val_loss: -1.3843\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7855 - val_loss: -1.3845\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8023 - val_loss: -1.3847\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8004 - val_loss: -1.3848\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8073 - val_loss: -1.3849\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7888 - val_loss: -1.3850\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7996 - val_loss: -1.3851\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.7961 - val_loss: -1.3852\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8171 - val_loss: -1.3854\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8001 - val_loss: -1.3855\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7959 - val_loss: -1.3856\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8034 - val_loss: -1.3857\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.8096 - val_loss: -1.3858\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7864 - val_loss: -1.3859\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7960 - val_loss: -1.3860\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8043 - val_loss: -1.3861\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8115 - val_loss: -1.3862\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8182 - val_loss: -1.3863\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8021 - val_loss: -1.3863\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8051 - val_loss: -1.3864\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8087 - val_loss: -1.3864\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.7891 - val_loss: -1.3865\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7944 - val_loss: -1.3865\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.7787 - val_loss: -1.3866\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7995 - val_loss: -1.3867\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8027 - val_loss: -1.3867\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.8002 - val_loss: -1.3868\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7993 - val_loss: -1.3868\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7908 - val_loss: -1.3869\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7933 - val_loss: -1.3870\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7891 - val_loss: -1.3871\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7962 - val_loss: -1.3873\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8001 - val_loss: -1.3875\n",
      "Epoch 360/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.7831 - val_loss: -1.3877\n",
      "Epoch 361/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8136 - val_loss: -1.3879\n",
      "Epoch 362/1000\n",
      "2/2 [==============================] - 0s 166ms/step - loss: -1.7829 - val_loss: -1.3881\n",
      "Epoch 363/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8120 - val_loss: -1.3883\n",
      "Epoch 364/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8085 - val_loss: -1.3885\n",
      "Epoch 365/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8109 - val_loss: -1.3886\n",
      "Epoch 366/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.7911 - val_loss: -1.3887\n",
      "Epoch 367/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8094 - val_loss: -1.3888\n",
      "Epoch 368/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8052 - val_loss: -1.3890\n",
      "Epoch 369/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7981 - val_loss: -1.3891\n",
      "Epoch 370/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7934 - val_loss: -1.3892\n",
      "Epoch 371/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8156 - val_loss: -1.3893\n",
      "Epoch 372/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.7900 - val_loss: -1.3895\n",
      "Epoch 373/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8075 - val_loss: -1.3896\n",
      "Epoch 374/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8057 - val_loss: -1.3897\n",
      "Epoch 375/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8029 - val_loss: -1.3898\n",
      "Epoch 376/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8112 - val_loss: -1.3898\n",
      "Epoch 377/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8192 - val_loss: -1.3898\n",
      "Epoch 378/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7865 - val_loss: -1.3899\n",
      "Epoch 379/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.8030 - val_loss: -1.3899\n",
      "Epoch 380/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.8051 - val_loss: -1.3900\n",
      "Epoch 381/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8140 - val_loss: -1.3899\n",
      "Epoch 382/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8093 - val_loss: -1.3899\n",
      "Epoch 383/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8057 - val_loss: -1.3899\n",
      "Epoch 384/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7990 - val_loss: -1.3899\n",
      "Epoch 385/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8048 - val_loss: -1.3899\n",
      "Epoch 386/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7997 - val_loss: -1.3899\n",
      "Epoch 387/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8188 - val_loss: -1.3899\n",
      "Epoch 388/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8062 - val_loss: -1.3899\n",
      "Epoch 389/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8005 - val_loss: -1.3899\n",
      "Epoch 390/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8090 - val_loss: -1.3900\n",
      "Epoch 391/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.8095 - val_loss: -1.3900\n",
      "Epoch 392/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8040 - val_loss: -1.3900\n",
      "Epoch 393/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8087 - val_loss: -1.3900\n",
      "Epoch 394/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8108 - val_loss: -1.3900\n",
      "Epoch 395/1000\n",
      "2/2 [==============================] - 0s 132ms/step - loss: -1.8151 - val_loss: -1.3900\n",
      "Epoch 396/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.8176 - val_loss: -1.3900\n",
      "Epoch 397/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8036 - val_loss: -1.3899\n",
      "Epoch 398/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8093 - val_loss: -1.3899\n",
      "Epoch 399/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8139 - val_loss: -1.3900\n",
      "Epoch 400/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8077 - val_loss: -1.3900\n",
      "Epoch 401/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.7954 - val_loss: -1.3900\n",
      "Epoch 402/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7917 - val_loss: -1.3901\n",
      "Epoch 403/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.8092 - val_loss: -1.3901\n",
      "Epoch 404/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.8095 - val_loss: -1.3902\n",
      "Epoch 405/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7830 - val_loss: -1.3903\n",
      "Epoch 406/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8090 - val_loss: -1.3904\n",
      "Epoch 407/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8073 - val_loss: -1.3905\n",
      "Epoch 408/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7969 - val_loss: -1.3906\n",
      "Epoch 409/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7995 - val_loss: -1.3907\n",
      "Epoch 410/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.8045 - val_loss: -1.3908\n",
      "Epoch 411/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.8157 - val_loss: -1.3908\n",
      "Epoch 412/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8035 - val_loss: -1.3909\n",
      "Epoch 413/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.7976 - val_loss: -1.3909\n",
      "Epoch 414/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8115 - val_loss: -1.3909\n",
      "Epoch 415/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8052 - val_loss: -1.3910\n",
      "Epoch 416/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7915 - val_loss: -1.3910\n",
      "Epoch 417/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8018 - val_loss: -1.3910\n",
      "Epoch 418/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.8038 - val_loss: -1.3910\n",
      "Epoch 419/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7955 - val_loss: -1.3910\n",
      "Epoch 420/1000\n",
      "2/2 [==============================] - 0s 132ms/step - loss: -1.8042 - val_loss: -1.3911\n",
      "Epoch 421/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7822 - val_loss: -1.3912\n",
      "Epoch 422/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7975 - val_loss: -1.3912\n",
      "Epoch 423/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8036 - val_loss: -1.3913\n",
      "Epoch 424/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8057 - val_loss: -1.3913\n",
      "Epoch 425/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8086 - val_loss: -1.3914\n",
      "Epoch 426/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8318 - val_loss: -1.3914\n",
      "Epoch 427/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8141 - val_loss: -1.3913\n",
      "Epoch 428/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8156 - val_loss: -1.3913\n",
      "Epoch 429/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8064 - val_loss: -1.3913\n",
      "Epoch 430/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8106 - val_loss: -1.3913\n",
      "Epoch 431/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8025 - val_loss: -1.3912\n",
      "Epoch 432/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7854 - val_loss: -1.3912\n",
      "Epoch 433/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8047 - val_loss: -1.3913\n",
      "Epoch 434/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.7979 - val_loss: -1.3913\n",
      "Epoch 435/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7993 - val_loss: -1.3913\n",
      "Epoch 436/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.8004 - val_loss: -1.3914\n",
      "Epoch 437/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8018 - val_loss: -1.3914\n",
      "Epoch 438/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7879 - val_loss: -1.3915\n",
      "Epoch 439/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8083 - val_loss: -1.3916\n",
      "Epoch 440/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8039 - val_loss: -1.3916\n",
      "Epoch 441/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8162 - val_loss: -1.3917\n",
      "Epoch 442/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8001 - val_loss: -1.3917\n",
      "Epoch 443/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.8036 - val_loss: -1.3918\n",
      "Epoch 444/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7949 - val_loss: -1.3918\n",
      "Epoch 445/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8113 - val_loss: -1.3919\n",
      "Epoch 446/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8111 - val_loss: -1.3919\n",
      "Epoch 447/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.8064 - val_loss: -1.3919\n",
      "Epoch 448/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -1.8050 - val_loss: -1.3920\n",
      "Epoch 449/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -1.8113 - val_loss: -1.3920\n",
      "Epoch 450/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8042 - val_loss: -1.3920\n",
      "Epoch 451/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8062 - val_loss: -1.3920\n",
      "Epoch 452/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8054 - val_loss: -1.3920\n",
      "Epoch 453/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8067 - val_loss: -1.3920\n",
      "Epoch 454/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8179 - val_loss: -1.3920\n",
      "Epoch 455/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8197 - val_loss: -1.3920\n",
      "Epoch 456/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7887 - val_loss: -1.3920\n",
      "Epoch 457/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8075 - val_loss: -1.3921\n",
      "Epoch 458/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.8116 - val_loss: -1.3921\n",
      "Epoch 459/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8109 - val_loss: -1.3921\n",
      "Epoch 460/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.8102 - val_loss: -1.3921\n",
      "Epoch 461/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8102 - val_loss: -1.3921\n",
      "Epoch 462/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8068 - val_loss: -1.3921\n",
      "Epoch 463/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8040 - val_loss: -1.3921\n",
      "Epoch 464/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8082 - val_loss: -1.3921\n",
      "Epoch 465/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8059 - val_loss: -1.3921\n",
      "Epoch 466/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8047 - val_loss: -1.3921\n",
      "Epoch 467/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7877 - val_loss: -1.3922\n",
      "Epoch 468/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8082 - val_loss: -1.3922\n",
      "Epoch 469/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7908 - val_loss: -1.3922\n",
      "Epoch 470/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.8134 - val_loss: -1.3922\n",
      "Epoch 471/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.7896 - val_loss: -1.3923\n",
      "Epoch 472/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7982 - val_loss: -1.3923\n",
      "Epoch 473/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8003 - val_loss: -1.3924\n",
      "Epoch 474/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8112 - val_loss: -1.3924\n",
      "Epoch 475/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8076 - val_loss: -1.3925\n",
      "Epoch 476/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8119 - val_loss: -1.3925\n",
      "Epoch 477/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8126 - val_loss: -1.3925\n",
      "Epoch 478/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8094 - val_loss: -1.3925\n",
      "Epoch 479/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8091 - val_loss: -1.3925\n",
      "Epoch 480/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7999 - val_loss: -1.3925\n",
      "Epoch 481/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8042 - val_loss: -1.3925\n",
      "Epoch 482/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8150 - val_loss: -1.3926\n",
      "Epoch 483/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7926 - val_loss: -1.3926\n",
      "Epoch 484/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8070 - val_loss: -1.3927\n",
      "Epoch 485/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8144 - val_loss: -1.3927\n",
      "Epoch 486/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8022 - val_loss: -1.3927\n",
      "Epoch 487/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8066 - val_loss: -1.3928\n",
      "Epoch 488/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.8085 - val_loss: -1.3928\n",
      "Epoch 489/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.7892 - val_loss: -1.3929\n",
      "Epoch 490/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8006 - val_loss: -1.3930\n",
      "Epoch 491/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8145 - val_loss: -1.3931\n",
      "Epoch 492/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8104 - val_loss: -1.3932\n",
      "Epoch 493/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8081 - val_loss: -1.3933\n",
      "Epoch 494/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8067 - val_loss: -1.3933\n",
      "Epoch 495/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7963 - val_loss: -1.3934\n",
      "Epoch 496/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8013 - val_loss: -1.3935\n",
      "Epoch 497/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: -1.8106 - val_loss: -1.3936\n",
      "Epoch 498/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7973 - val_loss: -1.3937\n",
      "Epoch 499/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8031 - val_loss: -1.3938\n",
      "Epoch 500/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8067 - val_loss: -1.3939\n",
      "Epoch 501/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7982 - val_loss: -1.3940\n",
      "Epoch 502/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8087 - val_loss: -1.3941\n",
      "Epoch 503/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.8107 - val_loss: -1.3943\n",
      "Epoch 504/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8137 - val_loss: -1.3943\n",
      "Epoch 505/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.8187 - val_loss: -1.3944\n",
      "Epoch 506/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8084 - val_loss: -1.3945\n",
      "Epoch 507/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8010 - val_loss: -1.3945\n",
      "Epoch 508/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.8124 - val_loss: -1.3945\n",
      "Epoch 509/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8150 - val_loss: -1.3945\n",
      "Epoch 510/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8104 - val_loss: -1.3945\n",
      "Epoch 511/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8030 - val_loss: -1.3945\n",
      "Epoch 512/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8104 - val_loss: -1.3945\n",
      "Epoch 513/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8045 - val_loss: -1.3945\n",
      "Epoch 514/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8088 - val_loss: -1.3945\n",
      "Epoch 515/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8050 - val_loss: -1.3946\n",
      "Epoch 516/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8130 - val_loss: -1.3946\n",
      "Epoch 517/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.8185 - val_loss: -1.3946\n",
      "Epoch 518/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.8138 - val_loss: -1.3945\n",
      "Epoch 519/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8078 - val_loss: -1.3945\n",
      "Epoch 520/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8094 - val_loss: -1.3945\n",
      "Epoch 521/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7968 - val_loss: -1.3945\n",
      "Epoch 522/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8123 - val_loss: -1.3945\n",
      "Epoch 523/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7932 - val_loss: -1.3945\n",
      "Epoch 524/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8046 - val_loss: -1.3945\n",
      "Epoch 525/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8128 - val_loss: -1.3945\n",
      "Epoch 526/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8076 - val_loss: -1.3944\n",
      "Epoch 527/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8064 - val_loss: -1.3944\n",
      "Epoch 528/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8103 - val_loss: -1.3944\n",
      "Epoch 529/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.8104 - val_loss: -1.3943\n",
      "Epoch 530/1000\n",
      "2/2 [==============================] - 0s 138ms/step - loss: -1.7847 - val_loss: -1.3943\n",
      "Epoch 531/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7845 - val_loss: -1.3943\n",
      "Epoch 532/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8151 - val_loss: -1.3944\n",
      "Epoch 533/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8146 - val_loss: -1.3944\n",
      "Epoch 534/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8166 - val_loss: -1.3943\n",
      "Epoch 535/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8092 - val_loss: -1.3943\n",
      "Epoch 536/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8158 - val_loss: -1.3943\n",
      "Epoch 537/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8113 - val_loss: -1.3942\n",
      "Epoch 538/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8126 - val_loss: -1.3942\n",
      "Epoch 539/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.8029 - val_loss: -1.3941\n",
      "Epoch 540/1000\n",
      "2/2 [==============================] - 0s 135ms/step - loss: -1.8105 - val_loss: -1.3941\n",
      "Epoch 541/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.8074 - val_loss: -1.3941\n",
      "Epoch 542/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.7921 - val_loss: -1.3941\n",
      "Epoch 543/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8124 - val_loss: -1.3941\n",
      "Epoch 544/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.8129 - val_loss: -1.3942\n",
      "Epoch 545/1000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: -1.8066 - val_loss: -1.3942\n",
      "Epoch 546/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8111 - val_loss: -1.3942\n",
      "Epoch 547/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8020 - val_loss: -1.3942\n",
      "Epoch 548/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8074 - val_loss: -1.3942\n",
      "Epoch 549/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -1.86 - 0s 104ms/step - loss: -1.8161 - val_loss: -1.3942\n",
      "Epoch 550/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8090 - val_loss: -1.3942\n",
      "Epoch 551/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8116 - val_loss: -1.3941\n",
      "Epoch 552/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8102 - val_loss: -1.3941\n",
      "Epoch 553/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8101 - val_loss: -1.3941\n",
      "Epoch 554/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.8037 - val_loss: -1.3941\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00554: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_11 (Softmax)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 585ms/step - loss: -1.6024 - val_loss: -0.3979\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6468 - val_loss: -0.3979\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.5433 - val_loss: -0.3979\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.6216 - val_loss: -0.3979\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.5899 - val_loss: -0.3980\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6319 - val_loss: -0.3980\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.6428 - val_loss: -0.3980\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7096 - val_loss: -0.3980\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7127 - val_loss: -0.3980\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.6972 - val_loss: -0.3981\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.6917 - val_loss: -0.3981\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.6441 - val_loss: -0.3982\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.6688 - val_loss: -0.3982\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8044 - val_loss: -0.3983\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8077 - val_loss: -0.3984\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.7060 - val_loss: -0.3984\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.7379 - val_loss: -0.3985\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.7602 - val_loss: -0.3986\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.8915 - val_loss: -0.3987\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7683 - val_loss: -0.3987\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.9068 - val_loss: -0.3988\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8351 - val_loss: -0.3989\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8189 - val_loss: -0.3989\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.9038 - val_loss: -0.3990\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8836 - val_loss: -0.3991\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -2.0083 - val_loss: -0.3991\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.7923 - val_loss: -0.3992\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.9014 - val_loss: -0.3992\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9393 - val_loss: -0.3992\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.0161 - val_loss: -0.3993\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8670 - val_loss: -0.3993\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.1039 - val_loss: -0.3993\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0364 - val_loss: -0.3993\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0163 - val_loss: -0.3993\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.0345 - val_loss: -0.3993\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0744 - val_loss: -0.3993\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0399 - val_loss: -0.3992\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.1105 - val_loss: -0.3992\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.1816 - val_loss: -0.3991\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -2.1188 - val_loss: -0.3990\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0959 - val_loss: -0.3989\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -2.1071 - val_loss: -0.3988\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.1054 - val_loss: -0.3987\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.1762 - val_loss: -0.3986\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.1711 - val_loss: -0.3985\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.1424 - val_loss: -0.3983\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.1330 - val_loss: -0.3981\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -2.1726 - val_loss: -0.3980\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.2087 - val_loss: -0.3978\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.2210 - val_loss: -0.3976\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.2107 - val_loss: -0.3974\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -2.2632 - val_loss: -0.3973\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.2626 - val_loss: -0.3971\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.1438 - val_loss: -0.3968\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.2250 - val_loss: -0.3966\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.3005 - val_loss: -0.3964\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.2830 - val_loss: -0.3961\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.3757 - val_loss: -0.3959\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.3309 - val_loss: -0.3957\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.3428 - val_loss: -0.3955\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.3816 - val_loss: -0.3952\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.2811 - val_loss: -0.3950\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -2.3655 - val_loss: -0.3948\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.3540 - val_loss: -0.3946\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.3806 - val_loss: -0.3944\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -2.4427 - val_loss: -0.3942\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: -2.3751 - val_loss: -0.3940\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.4608 - val_loss: -0.3938\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.3431 - val_loss: -0.3936\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.3952 - val_loss: -0.3934\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.3689 - val_loss: -0.3932\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.4355 - val_loss: -0.3930\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -2.4225 - val_loss: -0.3928\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00073: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and predict\n",
    "1. Get the rolling window data from of T-250\n",
    "2. scale and split data to prepare for neural network input, Xtrain, Xtest, ytrain, ytest, Xpred\n",
    "3. Train the model with Xtrain,Xtest,ytrain,ytest\n",
    "4. Predict weight using Xpred\n",
    "5. Save weight for that T\n",
    "6. Repeat 1 - 5 for every T(rebal date)\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=50, verbose=2, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# we will start the prediction from year 2020\n",
    "pred_dates = [d for d in REBAL_DATES if d.year == 2020]\n",
    "\n",
    "def get_window(df,pred_date):\n",
    "    temp = df.copy()\n",
    "    pred_date_ind = np.where(temp.index == pred_date)[0][0]\n",
    "    window = temp.iloc[((pred_date_ind+1)-WINDOW):(pred_date_ind+1)]\n",
    "    return window\n",
    "\n",
    "def create_input(df):\n",
    "    data = df.copy()\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    transformer = scaler.fit(data)\n",
    "    scaled_data = pd.DataFrame(transformer.transform(data), columns=data.columns)\n",
    "    test_data = scaled_data.iloc[:100]\n",
    "    train_data = scaled_data.iloc[100:]\n",
    "    \n",
    "    n_test_sample = 100 - LOOKBACK - HORIZON\n",
    "    n_train_sample = 100 - LOOKBACK - HORIZON\n",
    "    \n",
    "    assets = scaled_data.columns.levels[0].tolist()\n",
    "    channels = scaled_data.columns.levels[1].tolist()\n",
    "    \n",
    "    n_channels = len(data.columns.levels[1])\n",
    "    channel = scaled_data.columns.levels[1][0]\n",
    "    for index, asset in enumerate(assets):\n",
    "        if (index == 0):\n",
    "            test_temp = test_data[asset, channel].values.reshape(-1,1)\n",
    "            train_temp = train_data[asset, channel].values.reshape(-1,1)\n",
    "            full_temp = scaled_data[asset, channel].values.reshape(-1,1)\n",
    "        else:\n",
    "            test_temp = np.concatenate((test_temp, test_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            train_temp = np.concatenate((train_temp, train_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            full_temp = np.concatenate((full_temp, scaled_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            \n",
    "    data_block_test = test_temp.reshape(n_channels,test_temp.shape[0],test_temp.shape[1])\n",
    "    data_block_train = train_temp.reshape(n_channels,train_temp.shape[0],train_temp.shape[1])\n",
    "    data_block = full_temp.reshape(n_channels,full_temp.shape[0],full_temp.shape[1])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, X_pred = [], [], [], [], []\n",
    "    \n",
    "    for i in range(n_train_sample):\n",
    "        X_train.append(data_block_train[:,i:(i+LOOKBACK),:])\n",
    "        y_train.append(data_block_train[:,(i+LOOKBACK):(i + LOOKBACK + HORIZON),:])\n",
    "        \n",
    "    for i in range(n_train_sample):\n",
    "        X_test.append(data_block_test[:,i:(i+LOOKBACK),:])\n",
    "        y_test.append(data_block_test[:,(i+LOOKBACK):(i + LOOKBACK + HORIZON),:])\n",
    "        \n",
    "    X_pred.append(data_block_test[:,-LOOKBACK:,:])\n",
    "        \n",
    "    return np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), np.array(X_pred)\n",
    "\n",
    "\"\"\"Here will start train and predict rolling window, getting weight for each rebalance date\"\"\"\n",
    "portfolio_weights = []\n",
    "for d in pred_dates:\n",
    "    data = get_window(df, d)\n",
    "    \n",
    "    # split 100 sample for train, 100 sample for test. Since we have 190 samples, some test data will overlap\n",
    "    X_train, y_train, X_test, y_test, X_pred = create_input(data)\n",
    "    print(f\"X_train shape = {X_train.shape}\")\n",
    "    print(f\"y_train shape = {y_train.shape}\")\n",
    "    print(f\"X_test shape = {X_test.shape}\")\n",
    "    print(f\"y_test shape = {y_test.shape}\")\n",
    "    print(f\"X_pred shape = {X_pred.shape}\")\n",
    "    \n",
    "    model = build_model(\n",
    "    n_assets=N_ASSETS,\n",
    "    input_shape = (N_FEATURES),\n",
    "    dropout=0.5\n",
    "    )\n",
    "\n",
    "    my_loss = make_my_loss([0.1, 0.2, 0.7])\n",
    "    model.compile(\n",
    "        loss= my_loss,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    X_train_in = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_in = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_in,\n",
    "        y_train,\n",
    "        validation_data=(X_test_in, y_test),\n",
    "        epochs=N_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[es]\n",
    "        )\n",
    "    \n",
    "    X_pred_in = X_pred.reshape(X_pred.shape[0], -1)\n",
    "    weights = model.predict(X_pred_in)\n",
    "    portfolio_weights.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6c59b9f-5e06-4374-b414-4b37bdc4c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "portfolio_weights = np.array(portfolio_weights)\n",
    "portfolio_weights = portfolio_weights.flatten().reshape(len(pred_dates),-1)\n",
    "weights_df = pd.DataFrame(portfolio_weights, columns=ASSETS, index=pred_dates)\n",
    "\n",
    "background_color = '#fbfbfb'\n",
    "fig = plt.figure(figsize=(12, 8), facecolor=background_color)\n",
    "stack_list = []\n",
    "for asset in ASSETS:\n",
    "    stack_list.append(weights_df[asset])\n",
    "        \n",
    "plt.stackplot(weights_df.index, stack_list, labels=ASSETS)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75e7722d-47a6-4ec2-926b-9e724cb3e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baseline model best weights per month based on weights that give the best Sharpe ratio\n",
    "\"\"\"\n",
    "\n",
    "def get_rand_weights(n_samples, n_assets):\n",
    "    port_weights = []\n",
    "  \n",
    "    for port in range(n_samples):\n",
    "        weights = np.random.random(n_assets)\n",
    "        weights = weights/np.sum(weights)\n",
    "        port_weights.append(weights)\n",
    "  \n",
    "    return port_weights\n",
    "\n",
    "def weights_best_sharpe(random_port_weights, unweighted_rets, data, rebal_date, rf=0.01):\n",
    "    indv_return = unweighted_rets\n",
    "    port_returns = []\n",
    "    port_volatility = []\n",
    "    df_dict = {}\n",
    "\n",
    "    for weights in random_port_weights:\n",
    "        # calculate the port return for that random weight\n",
    "        port_expr = indv_return.dot(weights)\n",
    "        port_returns.append(port_expr)\n",
    "\n",
    "        # calculate the monthly volatility\n",
    "        covariance = data.cov()\n",
    "        var = np.transpose(weights)@covariance@weights\n",
    "        sd = np.sqrt(var)\n",
    "        monthly_sd = sd*np.sqrt(HORIZON)\n",
    "        port_volatility.append(monthly_sd)\n",
    "\n",
    "    df_dict['returns'] = port_returns\n",
    "    df_dict['volatility'] = port_volatility\n",
    "    ret_vol = pd.DataFrame(df_dict)\n",
    "    monthly_weight = pd.DataFrame(random_port_weights, columns=ASSETS)\n",
    "    final_df = pd.concat([ret_vol,monthly_weight], axis=1)\n",
    "    \n",
    "      # find row with best sharpe ratio\n",
    "    optimal_portfolio = final_df.iloc[((final_df['returns'] - rf)/final_df['volatility']).idxmax()]\n",
    "    return np.array(optimal_portfolio)\n",
    "\n",
    "num_portfolios = 10000\n",
    "\n",
    "_,df_3 = load_prepare_data(ASSETS, '_train')\n",
    "_,df_4 = load_prepare_data(ASSETS, '_test')\n",
    "df_bl = pd.concat([df_1, df_2], axis=0)\n",
    "pred_dates = [d for d in REBAL_DATES if d.year == 2020]\n",
    "\n",
    "bl_weights = np.array([])\n",
    "for d in pred_dates:\n",
    "    data = get_window(df_bl, d)\n",
    "    returns = to_return(data, 'Adj Close')\n",
    "    \n",
    "    #calculate the unweighted individual return for past 250 days\n",
    "    first_last = pd.concat([pd.DataFrame(data.iloc[0]).T, pd.DataFrame(data.iloc[-1]).T])\n",
    "    indv_rets = first_last.pct_change().apply(lambda x: np.log(1+x)).mean().values\n",
    "    \n",
    "    random_port_weights = get_rand_weights(num_portfolios, N_ASSETS)\n",
    "    \n",
    "    bl_weights = np.append(bl_weights, weights_best_sharpe(random_port_weights, indv_rets, data, d, rf=0.01))\n",
    "\n",
    "# contruct dataframe with weights, and use pred_dates and index\n",
    "bl_weights_df = pd.DataFrame(bl_weights.reshape(-1, N_ASSETS + 2)[:,-N_ASSETS:], index=pred_dates, columns = ASSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e4f5d2b-655f-45d6-b83b-081107ba3ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "background_color = '#fbfbfb'\n",
    "fig = plt.figure(figsize=(12, 8), facecolor=background_color)\n",
    "stack_list = []\n",
    "for asset in ASSETS:\n",
    "    stack_list.append(bl_weights_df[asset])\n",
    "        \n",
    "plt.stackplot(bl_weights_df.index, stack_list, labels=ASSETS)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "091331ab-01be-4fe1-849b-cb2070281476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will see what the volatility and return the weights predicted/optimized using the data from T-250 will do to horizon, in this case T + 20.\n",
    "\"\"\"\n",
    "\n",
    "# create dataframe of unweighted price change (expected return) of each rebalance date\n",
    "_,df_1 = load_prepare_data(ASSETS, '_train')\n",
    "_,df_2 = load_prepare_data(ASSETS, '_test')\n",
    "df_price = pd.concat([df_1, df_2], axis=0)\n",
    "pred_dates = [d for d in REBAL_DATES if d.year == 2020]\n",
    "\n",
    "def result_new_weight(price_data, weights, rebal_dates):\n",
    "    \n",
    "    monthly_price = price_data.loc[rebal_dates]\n",
    "    monthly_return = monthly_price.pct_change().apply(lambda x: np.log(1+x)).iloc[1:]\n",
    "    \n",
    "    all_port_expr = []\n",
    "    all_port_volatility = []\n",
    "    df_dict = {}\n",
    "    \n",
    "    for index,d in enumerate(rebal_dates[1:]):\n",
    "        #calculate returns \n",
    "        port_expr = monthly_return.iloc[index].values.dot(weights.iloc[index].values)\n",
    "        all_port_expr.append(port_expr)\n",
    "        \n",
    "        #calculate returns\n",
    "        data = get_window(price_data, d)\n",
    "        data = to_return(data, 'Adj Close')\n",
    "        covariance = data.cov()\n",
    "        var = np.transpose(weights.iloc[index].values)@covariance@weights.iloc[index].values\n",
    "        sd = np.sqrt(var)\n",
    "        monthly_sd = sd*np.sqrt(HORIZON)\n",
    "        all_port_volatility.append(monthly_sd)\n",
    "    \n",
    "    df_dict['returns'] = all_port_expr\n",
    "    df_dict['volatility'] = all_port_volatility\n",
    "    ret_vol = pd.DataFrame(df_dict, index = rebal_dates[1:])\n",
    "    # monthly_weight = pd.DataFrame(random_port_weights, columns=ASSETS)\n",
    "    final_df = pd.concat([ret_vol,weights], axis=1)\n",
    "    return final_df\n",
    "\n",
    "def shift_month(original, shift=1):\n",
    "    temp = original.copy()\n",
    "    temp = temp.iloc[:-shift].set_index(temp.index[shift:])\n",
    "    return temp\n",
    "    \n",
    "result_df = result_new_weight(df_price, shift_month(weights_df), pred_dates)\n",
    "result_df_bl = result_new_weight(df_price, shift_month(bl_weights_df), pred_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45fe470d-0875-4d07-b610-12a2f8b94010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>returns</th>\n",
       "      <th>volatility</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>SHOP</th>\n",
       "      <th>HLFNX</th>\n",
       "      <th>VGT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>0.291919</td>\n",
       "      <td>0.080871</td>\n",
       "      <td>0.203771</td>\n",
       "      <td>0.438399</td>\n",
       "      <td>0.146856</td>\n",
       "      <td>0.187004</td>\n",
       "      <td>0.023970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>-0.029858</td>\n",
       "      <td>0.068890</td>\n",
       "      <td>0.248671</td>\n",
       "      <td>0.176389</td>\n",
       "      <td>0.139907</td>\n",
       "      <td>0.182989</td>\n",
       "      <td>0.252044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>-0.301549</td>\n",
       "      <td>0.121339</td>\n",
       "      <td>0.429298</td>\n",
       "      <td>0.338358</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>0.102192</td>\n",
       "      <td>0.112116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>0.164455</td>\n",
       "      <td>0.114460</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>0.957224</td>\n",
       "      <td>0.022665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>0.104308</td>\n",
       "      <td>0.116641</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>0.982053</td>\n",
       "      <td>0.007683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.121150</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.991701</td>\n",
       "      <td>0.003816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-03</th>\n",
       "      <td>0.043302</td>\n",
       "      <td>0.121269</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.988028</td>\n",
       "      <td>0.001847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>0.160940</td>\n",
       "      <td>0.116335</td>\n",
       "      <td>0.208952</td>\n",
       "      <td>0.141779</td>\n",
       "      <td>0.106184</td>\n",
       "      <td>0.289205</td>\n",
       "      <td>0.253880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>-0.075296</td>\n",
       "      <td>0.119796</td>\n",
       "      <td>0.301190</td>\n",
       "      <td>0.080692</td>\n",
       "      <td>0.171049</td>\n",
       "      <td>0.283321</td>\n",
       "      <td>0.163749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02</th>\n",
       "      <td>-0.080543</td>\n",
       "      <td>0.123345</td>\n",
       "      <td>0.316481</td>\n",
       "      <td>0.123873</td>\n",
       "      <td>0.139584</td>\n",
       "      <td>0.218862</td>\n",
       "      <td>0.201199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>0.164795</td>\n",
       "      <td>0.123309</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.968236</td>\n",
       "      <td>0.027637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             returns  volatility      AAPL      TSLA      SHOP     HLFNX  \\\n",
       "2020-02-03  0.291919    0.080871  0.203771  0.438399  0.146856  0.187004   \n",
       "2020-03-02 -0.029858    0.068890  0.248671  0.176389  0.139907  0.182989   \n",
       "2020-04-01 -0.301549    0.121339  0.429298  0.338358  0.018036  0.102192   \n",
       "2020-05-01  0.164455    0.114460  0.005087  0.009700  0.005325  0.957224   \n",
       "2020-06-01  0.104308    0.116641  0.001677  0.005411  0.003176  0.982053   \n",
       "2020-07-01  0.006679    0.121150  0.000966  0.001894  0.001623  0.991701   \n",
       "2020-08-03  0.043302    0.121269  0.000610  0.001634  0.007881  0.988028   \n",
       "2020-09-01  0.160940    0.116335  0.208952  0.141779  0.106184  0.289205   \n",
       "2020-10-01 -0.075296    0.119796  0.301190  0.080692  0.171049  0.283321   \n",
       "2020-11-02 -0.080543    0.123345  0.316481  0.123873  0.139584  0.218862   \n",
       "2020-12-01  0.164795    0.123309  0.000629  0.001549  0.001949  0.968236   \n",
       "\n",
       "                 VGT  \n",
       "2020-02-03  0.023970  \n",
       "2020-03-02  0.252044  \n",
       "2020-04-01  0.112116  \n",
       "2020-05-01  0.022665  \n",
       "2020-06-01  0.007683  \n",
       "2020-07-01  0.003816  \n",
       "2020-08-03  0.001847  \n",
       "2020-09-01  0.253880  \n",
       "2020-10-01  0.163749  \n",
       "2020-11-02  0.201199  \n",
       "2020-12-01  0.027637  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b0fbc03-7774-4f89-97c5-f1cfe70980ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>returns</th>\n",
       "      <th>volatility</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>SHOP</th>\n",
       "      <th>HLFNX</th>\n",
       "      <th>VGT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>0.033806</td>\n",
       "      <td>0.042997</td>\n",
       "      <td>0.098067</td>\n",
       "      <td>0.016173</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>0.845606</td>\n",
       "      <td>0.026248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>-0.051236</td>\n",
       "      <td>0.054190</td>\n",
       "      <td>0.054120</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>0.753806</td>\n",
       "      <td>0.066918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>-0.303435</td>\n",
       "      <td>0.103897</td>\n",
       "      <td>0.328975</td>\n",
       "      <td>0.039445</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.619050</td>\n",
       "      <td>0.007887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>0.230224</td>\n",
       "      <td>0.119212</td>\n",
       "      <td>0.757229</td>\n",
       "      <td>0.179502</td>\n",
       "      <td>0.047918</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.012194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>0.155376</td>\n",
       "      <td>0.126029</td>\n",
       "      <td>0.533819</td>\n",
       "      <td>0.332624</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.095185</td>\n",
       "      <td>0.034836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>0.082930</td>\n",
       "      <td>0.112662</td>\n",
       "      <td>0.522148</td>\n",
       "      <td>0.063010</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.397675</td>\n",
       "      <td>0.011434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-03</th>\n",
       "      <td>0.186332</td>\n",
       "      <td>0.119481</td>\n",
       "      <td>0.747673</td>\n",
       "      <td>0.162423</td>\n",
       "      <td>0.010697</td>\n",
       "      <td>0.020189</td>\n",
       "      <td>0.059018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>0.157729</td>\n",
       "      <td>0.111772</td>\n",
       "      <td>0.592969</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>0.294199</td>\n",
       "      <td>0.095578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>-0.079423</td>\n",
       "      <td>0.115913</td>\n",
       "      <td>0.461593</td>\n",
       "      <td>0.032535</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.451081</td>\n",
       "      <td>0.048204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02</th>\n",
       "      <td>-0.068269</td>\n",
       "      <td>0.119924</td>\n",
       "      <td>0.628763</td>\n",
       "      <td>0.019374</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.298304</td>\n",
       "      <td>0.050624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>0.139496</td>\n",
       "      <td>0.122221</td>\n",
       "      <td>0.679033</td>\n",
       "      <td>0.024332</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.248871</td>\n",
       "      <td>0.047216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             returns  volatility      AAPL      TSLA      SHOP     HLFNX  \\\n",
       "2020-02-03  0.033806    0.042997  0.098067  0.016173  0.013906  0.845606   \n",
       "2020-03-02 -0.051236    0.054190  0.054120  0.115982  0.009176  0.753806   \n",
       "2020-04-01 -0.303435    0.103897  0.328975  0.039445  0.004643  0.619050   \n",
       "2020-05-01  0.230224    0.119212  0.757229  0.179502  0.047918  0.003156   \n",
       "2020-06-01  0.155376    0.126029  0.533819  0.332624  0.003535  0.095185   \n",
       "2020-07-01  0.082930    0.112662  0.522148  0.063010  0.005733  0.397675   \n",
       "2020-08-03  0.186332    0.119481  0.747673  0.162423  0.010697  0.020189   \n",
       "2020-09-01  0.157729    0.111772  0.592969  0.007614  0.009640  0.294199   \n",
       "2020-10-01 -0.079423    0.115913  0.461593  0.032535  0.006588  0.451081   \n",
       "2020-11-02 -0.068269    0.119924  0.628763  0.019374  0.002936  0.298304   \n",
       "2020-12-01  0.139496    0.122221  0.679033  0.024332  0.000548  0.248871   \n",
       "\n",
       "                 VGT  \n",
       "2020-02-03  0.026248  \n",
       "2020-03-02  0.066918  \n",
       "2020-04-01  0.007887  \n",
       "2020-05-01  0.012194  \n",
       "2020-06-01  0.034836  \n",
       "2020-07-01  0.011434  \n",
       "2020-08-03  0.059018  \n",
       "2020-09-01  0.095578  \n",
       "2020-10-01  0.048204  \n",
       "2020-11-02  0.050624  \n",
       "2020-12-01  0.047216  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e65ee-928c-4200-bc61-4a4887ffae46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
