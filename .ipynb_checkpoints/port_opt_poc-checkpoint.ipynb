{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6528bf7d-6352-48a7-9e9d-a40af3edc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "# plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a814ec-5b4c-45f5-9150-949957c778cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '~/deepdow_poc'\n",
    "raw_data = root_path + '/raw_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9a2bcb-32ae-4984-8feb-a53add39d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will load the dataset for each assets, filter only the adj close column to use as our feature and concat the assets into same dataframe\n",
    "\"\"\"\n",
    "\n",
    "def load_prepare_data(assets_list, suffix, col=['Adj Close']): #, 'Volume']):\n",
    "    assets_dict = {}\n",
    "    temp_list = []\n",
    "    \n",
    "    if suffix != None:\n",
    "        for asset in assets_list:\n",
    "            df_temp = pd.read_csv(raw_data + asset + suffix +'.csv')\n",
    "            df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "            df_temp.set_index('Date', inplace=True)\n",
    "            df_temp = pd.DataFrame(df_temp[col])\n",
    "\n",
    "            assets_dict[asset] = df_temp\n",
    "            temp_list.append(df_temp)\n",
    "    else:\n",
    "        for asset in assets_list:\n",
    "            df_temp = pd.read_csv(raw_data + asset +'.csv')\n",
    "            df_temp['Date'] = pd.to_datetime(df_temp['Date'])\n",
    "            df_temp.set_index('Date', inplace=True)\n",
    "            df_temp = pd.DataFrame(df_temp[col])\n",
    "\n",
    "            assets_dict[asset] = df_temp\n",
    "            temp_list.append(df_temp)\n",
    "            \n",
    "    df = pd.concat(temp_list, keys=assets_list, axis=1)\n",
    "    return assets_dict, df\n",
    "\n",
    "\n",
    "ASSETS = ['AAPL', 'TSLA', 'SHOP', 'HLFNX','VGT']\n",
    "_,df_1 = load_prepare_data(ASSETS, '_train')\n",
    "_,df_2 = load_prepare_data(ASSETS, '_test')\n",
    "df = pd.concat([df_1, df_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623c0df0-2291-431e-9ed9-457d125f281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert the data of the asset price to daily returns\n",
    "\"\"\"\n",
    "\n",
    "def to_return(data, col, log=True):\n",
    "    df = data.copy()\n",
    "    for asset in df.columns.levels[0]:\n",
    "        if log:\n",
    "            df[asset, 'Adj Close'] = df[asset, 'Adj Close'].pct_change().apply(lambda x: np.log(1+x))\n",
    "        else:\n",
    "            df[asset, 'Adj Close'] = df[asset, 'Adj Close'].pct_change()\n",
    "            \n",
    "    return df.iloc[1:]\n",
    "\n",
    "df_1 = to_return(df_1, 'Adj Close')\n",
    "df_2 = to_return(df_2, 'Adj Close')\n",
    "df = to_return(df, 'Adj Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b53b4f4-7227-4a46-b06a-945bfdbb7294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>SHOP</th>\n",
       "      <th>HLFNX</th>\n",
       "      <th>VGT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>-0.104924</td>\n",
       "      <td>-0.031978</td>\n",
       "      <td>-0.058433</td>\n",
       "      <td>-0.020795</td>\n",
       "      <td>-0.050685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>0.041803</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>0.061771</td>\n",
       "      <td>0.035463</td>\n",
       "      <td>0.042565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.009570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>0.016839</td>\n",
       "      <td>0.009438</td>\n",
       "      <td>0.021629</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.012405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>-0.007000</td>\n",
       "      <td>0.008769</td>\n",
       "      <td>-0.063956</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>-0.009144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-24</th>\n",
       "      <td>0.007683</td>\n",
       "      <td>0.024150</td>\n",
       "      <td>0.022745</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.006625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>0.035141</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>-0.066163</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.007003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-29</th>\n",
       "      <td>-0.013404</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.021177</td>\n",
       "      <td>-0.006415</td>\n",
       "      <td>-0.006635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30</th>\n",
       "      <td>-0.008563</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>-0.007376</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.001331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>502 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      TSLA      SHOP     HLFNX       VGT\n",
       "           Adj Close Adj Close Adj Close Adj Close Adj Close\n",
       "Date                                                        \n",
       "2019-01-03 -0.104924 -0.031978 -0.058433 -0.020795 -0.050685\n",
       "2019-01-04  0.041803  0.056094  0.061771  0.035463  0.042565\n",
       "2019-01-07 -0.002228  0.052935  0.044830  0.006221  0.011111\n",
       "2019-01-08  0.018884  0.001164  0.007246  0.003096  0.009570\n",
       "2019-01-09  0.016839  0.009438  0.021629  0.008209  0.012405\n",
       "...              ...       ...       ...       ...       ...\n",
       "2020-12-23 -0.007000  0.008769 -0.063956  0.002863 -0.009144\n",
       "2020-12-24  0.007683  0.024150  0.022745  0.004635  0.006625\n",
       "2020-12-28  0.035141  0.002897 -0.066163  0.001422  0.007003\n",
       "2020-12-29 -0.013404  0.003459  0.021177 -0.006415 -0.006635\n",
       "2020-12-30 -0.008563  0.042321 -0.007376  0.004637  0.001331\n",
       "\n",
       "[502 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13299300-c31c-48f7-ac16-a9477159637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Establish constants\n",
    "1. Rebalance freq = BMS first business day of month\n",
    "2. Lookback = number of historical data in unit of days that \n",
    "   deep learning use to learn to predict weight for the horizon\n",
    "3. Horizon = days that user hold before rebalance. Corresponds to the Rebalance freq. \n",
    "    20 days = n_business_day in 1 month\n",
    "\"\"\"\n",
    "\n",
    "LOOKBACK = 40\n",
    "HORIZON = 20\n",
    "WINDOW = 250\n",
    "N_SAMPLES = WINDOW - LOOKBACK - HORIZON\n",
    "N_ASSETS = len(df.columns.levels[0])\n",
    "N_FEATURES = LOOKBACK * N_ASSETS\n",
    "\n",
    "\"\"\"\n",
    "first day of the month in the historical data\n",
    "go through each element in list, if month changes, that is the first day of month, assuming time is ordered in the list\n",
    "\"\"\"\n",
    "def get_first_date(datelist):\n",
    "    rebal_dates = []\n",
    "    for index,d in enumerate(df.index):\n",
    "        if index==0:\n",
    "            rebal_dates.append(d)\n",
    "            current_month = d.month\n",
    "        else:\n",
    "            if d.month != current_month:\n",
    "                rebal_dates.append(d)\n",
    "                current_month = d.month\n",
    "            else:\n",
    "                continue\n",
    "    return rebal_dates\n",
    "\n",
    "REBAL_DATES = get_first_date(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a8125c-6159-4e1a-a3e1-5c48ae96acce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns.levels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4faf5ea9-504a-48d3-b1eb-451d51ce7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train and optimize.\n",
    "Training and optimize will be done to 1 rolling window to predict the weight to use for the horizon.\n",
    "Rolling window will start from T(rebalance date) and go backwards\n",
    "\"\"\"\n",
    "\n",
    "# first will define the loss functions\n",
    "import tensorflow as tf\n",
    "\n",
    "def portfolio_returns(weights, y):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    --------\n",
    "    weights = the predicted values output from neural network. (n_samples, n_asset)\n",
    "    y = test data (n_samples, horizon, n_asset) use for loss calculation\n",
    "    \"\"\"\n",
    "    # exp to reverse log transform\n",
    "    y = tf.math.exp(y) - 1\n",
    "    n_samples = tf.shape(y)[0]\n",
    "    horizon = tf.shape(y)[1]\n",
    "    n_assets = tf.shape(y)[2]\n",
    "    \n",
    "    \"\"\" \n",
    "    reshape the weight into (n_samples, horizon, n_asset) dimension\n",
    "    first we have (n_samples, n_assets) weight output from neural network output\n",
    "    we want to repeat the vector horizon amount of time\n",
    "    \"\"\"\n",
    "    weights_ = tf.tile(tf.reshape(weights, [n_samples, 1, n_assets]), (1, horizon, 1))\n",
    "    \n",
    "    \"\"\"\n",
    "    rebalance : now will always be false (same for all time steps in the horizon)\n",
    "        If True, each timestep the weights are adjusted to be equal to be equal to the original ones. Note that\n",
    "        this assumes that we tinker with the portfolio. If False, the portfolio evolves untouched.\n",
    "    \"\"\"\n",
    "    rebalance=False\n",
    "    \n",
    "    if not rebalance:\n",
    "        weights_unscaled = tf.math.cumprod((1 + y),1)[:, :-1, :] * weights_[:, 1:, :]\n",
    "        \n",
    "        output_list = []\n",
    "        output_list.append(weights_[:, 0, :])\n",
    "        \n",
    "        remaining = weights_unscaled / tf.math.reduce_sum(weights_unscaled, 2, keepdims=True)\n",
    "        rows = remaining.get_shape()\n",
    "        for row in range(rows[1]):\n",
    "            output_list.append(remaining[:, row,:])\n",
    "        \n",
    "        new_weights = tf.stack(output_list, axis=1)\n",
    "\n",
    "    out = tf.math.reduce_sum((y * new_weights),-1)\n",
    "    \n",
    "    # shape (n_samples, horizon) representing per timestep portfolio returns\n",
    "    return out\n",
    "\n",
    "def MeanReturns(y, weights):#, rebalance=False):\n",
    "    \"\"\"Negative mean returns\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])#, rebalance=rebalance)\n",
    "    \n",
    "    \"\"\"Tensor of shape `(n_samples,)` representing the per sample negative mean returns.\"\"\"\n",
    "    # neg_mean = tf.math.exp(prets)\n",
    "    mean = tf.math.reduce_mean(prets, 1)\n",
    "    neg_mean = mean * -1\n",
    "    return neg_mean\n",
    "\n",
    "def SharpeRatio(y, weights, rf=0.01, eps=1e-4):\n",
    "    \"\"\"Negative Sharpe ratio\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])#, rebalance=rebalance)\n",
    "    \n",
    "    \"\"\"Tensor of shape `(n_samples,)` representing the per sample negative mean returns.\"\"\"\n",
    "    sharpe = ((tf.math.reduce_mean(prets, 1) - rf)/(tf.math.reduce_std(prets, 1) + eps))\n",
    "    neg_sharpe = sharpe * -1\n",
    "    return neg_sharpe\n",
    "\n",
    "def LargestWeight(y, weights):\n",
    "    \"\"\"Penalize low diversity, or over powering weight in 1 asset, lower = better\"\"\"\n",
    "    return tf.reduce_max(weights, 1)[0]\n",
    "\n",
    "def StandardDeviation(y, weights):\n",
    "    \"\"\"Returns std dev of portfolio return of the horizon, lower = better\"\"\"\n",
    "    prets = portfolio_returns(weights, y[:, 0, ...])\n",
    "    return tf.math.reduce_std(prets, 1)\n",
    "\n",
    "def make_my_loss(alpha):\n",
    "    def my_loss(y, weights):\n",
    "        \"\"\"Combination of different loss functions for the final loss calculation\"\"\"\n",
    "        return (alpha[0] * MeanReturns(y, weights)\n",
    "                + alpha[1] * SharpeRatio(y, weights)\n",
    "                + alpha[2] * StandardDeviation(y, weights)\n",
    "               )\n",
    "    return my_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b46bf7b-9893-43e5-81bf-d42420250a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the model archtecture\n",
    "-----------------------------\n",
    "This is a simple feedfoward model that the input is the flattened array of (n_samples, n_features)\n",
    "n_features = n_assets * horizon\n",
    "\n",
    "The output is softmax of dimension n_assets. Softmax --> sum of all weight = 1\n",
    "\"\"\"\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Softmax\n",
    "from tensorflow.keras.activations import softmax\n",
    "\n",
    "def build_model(n_assets, input_shape, dropout):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    x = Dropout(dropout)(x)\n",
    "    weights = Dense(n_assets, activation=\"linear\")(x)\n",
    "    outputs = Softmax()(weights)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7921a069-5130-48b4-b2ea-a01332699e20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 15:30:54.758497: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-01 15:30:54.900879: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 598ms/step - loss: -0.6470 - val_loss: -1.0933\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.6572 - val_loss: -1.0955\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6502 - val_loss: -1.0973\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6635 - val_loss: -1.0990\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6660 - val_loss: -1.1006\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6828 - val_loss: -1.1021\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.6564 - val_loss: -1.1033\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6750 - val_loss: -1.1045\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.6672 - val_loss: -1.1055\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6946 - val_loss: -1.1062\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6734 - val_loss: -1.1065\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.6844 - val_loss: -1.1071\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6654 - val_loss: -1.1078\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6792 - val_loss: -1.1082\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.7049 - val_loss: -1.1085\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 157ms/step - loss: -0.6963 - val_loss: -1.1086\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6998 - val_loss: -1.1089\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.6913 - val_loss: -1.1092\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6875 - val_loss: -1.1093\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.7312 - val_loss: -1.1093\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.7132 - val_loss: -1.1090\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.7064 - val_loss: -1.1084\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.7008 - val_loss: -1.1079\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.7099 - val_loss: -1.1076\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.7212 - val_loss: -1.1076\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.7152 - val_loss: -1.1076\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.7184 - val_loss: -1.1076\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.7417 - val_loss: -1.1076\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.7234 - val_loss: -1.1072\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.7157 - val_loss: -1.1067\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.7386 - val_loss: -1.1060\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 149ms/step - loss: -0.7480 - val_loss: -1.1053\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.7539 - val_loss: -1.1045\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7316 - val_loss: -1.1036\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.7607 - val_loss: -1.1026\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.7543 - val_loss: -1.1017\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.7580 - val_loss: -1.1004\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.7680 - val_loss: -1.0992\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -0.7577 - val_loss: -1.0978\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -0.7747 - val_loss: -1.0962\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.7593 - val_loss: -1.0946\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.7621 - val_loss: -1.0928\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.7645 - val_loss: -1.0911\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.7679 - val_loss: -1.0892\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.7959 - val_loss: -1.0872\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.7988 - val_loss: -1.0852\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.7947 - val_loss: -1.0829\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8026 - val_loss: -1.0804\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.8027 - val_loss: -1.0779\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.8085 - val_loss: -1.0754\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.8072 - val_loss: -1.0728\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.8260 - val_loss: -1.0703\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.8281 - val_loss: -1.0679\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.8113 - val_loss: -1.0654\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.7920 - val_loss: -1.0629\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.8383 - val_loss: -1.0604\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.8468 - val_loss: -1.0581\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.8380 - val_loss: -1.0558\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.8432 - val_loss: -1.0535\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.8691 - val_loss: -1.0510\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.8389 - val_loss: -1.0485\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.8557 - val_loss: -1.0460\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.8700 - val_loss: -1.0436\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.8450 - val_loss: -1.0410\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.8396 - val_loss: -1.0383\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.8443 - val_loss: -1.0356\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.8781 - val_loss: -1.0329\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00067: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 500ms/step - loss: -0.6360 - val_loss: -0.6945\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.6179 - val_loss: -0.6946\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.6307 - val_loss: -0.6946\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 152ms/step - loss: -0.6324 - val_loss: -0.6946\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.6174 - val_loss: -0.6945\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.6443 - val_loss: -0.6944\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6561 - val_loss: -0.6944\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.6404 - val_loss: -0.6942\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.6516 - val_loss: -0.6941\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.6460 - val_loss: -0.6939\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.6431 - val_loss: -0.6935\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.6476 - val_loss: -0.6933\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.6779 - val_loss: -0.6932\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6403 - val_loss: -0.6932\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.6644 - val_loss: -0.6933\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6728 - val_loss: -0.6936\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.6663 - val_loss: -0.6940\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6754 - val_loss: -0.6947\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.6854 - val_loss: -0.6955\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -0.6499 - val_loss: -0.6962\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.6730 - val_loss: -0.6969\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.6749 - val_loss: -0.6977\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.6782 - val_loss: -0.6987\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -0.6835 - val_loss: -0.6998\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.6944 - val_loss: -0.7011\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6829 - val_loss: -0.7025\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.6856 - val_loss: -0.7040\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 147ms/step - loss: -0.6722 - val_loss: -0.7053\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6926 - val_loss: -0.7068\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.6952 - val_loss: -0.7083\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6926 - val_loss: -0.7097\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.7116 - val_loss: -0.7112\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.7014 - val_loss: -0.7127\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 142ms/step - loss: -0.6901 - val_loss: -0.7142\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6666 - val_loss: -0.7154\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6965 - val_loss: -0.7164\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.7149 - val_loss: -0.7175\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.7061 - val_loss: -0.7185\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 153ms/step - loss: -0.7420 - val_loss: -0.7194\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.7192 - val_loss: -0.7204\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.7302 - val_loss: -0.7215\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.7370 - val_loss: -0.7223\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.7154 - val_loss: -0.7231\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.7217 - val_loss: -0.7239\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.7324 - val_loss: -0.7247\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.7272 - val_loss: -0.7255\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.7260 - val_loss: -0.7261\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.7213 - val_loss: -0.7267\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.7277 - val_loss: -0.7273\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.7548 - val_loss: -0.7277\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.7356 - val_loss: -0.7278\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.7484 - val_loss: -0.7278\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.7417 - val_loss: -0.7275\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.7392 - val_loss: -0.7270\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.7503 - val_loss: -0.7262\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -0.77 - 0s 107ms/step - loss: -0.7697 - val_loss: -0.7253\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7494 - val_loss: -0.7242\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.7484 - val_loss: -0.7229\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.7591 - val_loss: -0.7214\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.7634 - val_loss: -0.7200\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.7624 - val_loss: -0.7185\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.7806 - val_loss: -0.7170\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.7627 - val_loss: -0.7153\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7592 - val_loss: -0.7136\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.7692 - val_loss: -0.7120\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.7723 - val_loss: -0.7104\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.7812 - val_loss: -0.7087\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.7713 - val_loss: -0.7068\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.7748 - val_loss: -0.7050\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.7825 - val_loss: -0.7030\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.7733 - val_loss: -0.7011\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.7950 - val_loss: -0.6993\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.7743 - val_loss: -0.6975\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.8035 - val_loss: -0.6957\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7923 - val_loss: -0.6939\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.7973 - val_loss: -0.6922\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.7939 - val_loss: -0.6906\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.7925 - val_loss: -0.6888\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.8063 - val_loss: -0.6871\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.7953 - val_loss: -0.6853\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.7857 - val_loss: -0.6835\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.7941 - val_loss: -0.6814\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.8097 - val_loss: -0.6795\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.8192 - val_loss: -0.6780\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.8297 - val_loss: -0.6768\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.8149 - val_loss: -0.6760\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.7915 - val_loss: -0.6754\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.8278 - val_loss: -0.6750\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.8148 - val_loss: -0.6747\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.8274 - val_loss: -0.6747\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.8114 - val_loss: -0.6748\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.8407 - val_loss: -0.6750\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.8219 - val_loss: -0.6753\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.8238 - val_loss: -0.6753\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.8187 - val_loss: -0.6751\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.8237 - val_loss: -0.6750\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.8252 - val_loss: -0.6748\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.8154 - val_loss: -0.6745\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.8496 - val_loss: -0.6744\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00099: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 517ms/step - loss: -1.1611 - val_loss: -0.8848\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.1005 - val_loss: -0.8882\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.0982 - val_loss: -0.8912\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.1442 - val_loss: -0.8939\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.1653 - val_loss: -0.8967\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.1745 - val_loss: -0.8996\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -1.1711 - val_loss: -0.9025\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.1285 - val_loss: -0.9052\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.1477 - val_loss: -0.9080\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.1672 - val_loss: -0.9107\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.1352 - val_loss: -0.9135\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.1596 - val_loss: -0.9162\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.1443 - val_loss: -0.9191\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.2017 - val_loss: -0.9222\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.1594 - val_loss: -0.9254\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.2257 - val_loss: -0.9287\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.2126 - val_loss: -0.9320\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.1926 - val_loss: -0.9352\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.2205 - val_loss: -0.9386\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.1961 - val_loss: -0.9420\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 146ms/step - loss: -1.1835 - val_loss: -0.9451\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.2095 - val_loss: -0.9481\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.2418 - val_loss: -0.9511\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.2121 - val_loss: -0.9543\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.2293 - val_loss: -0.9575\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.2195 - val_loss: -0.9607\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.2361 - val_loss: -0.9640\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.1953 - val_loss: -0.9672\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.2344 - val_loss: -0.9703\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.2192 - val_loss: -0.9734\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2310 - val_loss: -0.9763\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.2243 - val_loss: -0.9792\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.2383 - val_loss: -0.9821\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.2709 - val_loss: -0.9852\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.2577 - val_loss: -0.9878\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.2667 - val_loss: -0.9906\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.2255 - val_loss: -0.9936\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2582 - val_loss: -0.9965\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.2226 - val_loss: -0.9990\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.2346 - val_loss: -1.0012\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3074 - val_loss: -1.0036\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.2693 - val_loss: -1.0059\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.2724 - val_loss: -1.0082\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.2409 - val_loss: -1.0103\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.2720 - val_loss: -1.0126\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.2772 - val_loss: -1.0148\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.2647 - val_loss: -1.0172\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.2684 - val_loss: -1.0201\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.2804 - val_loss: -1.0230\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.3054 - val_loss: -1.0258\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.2675 - val_loss: -1.0288\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.2874 - val_loss: -1.0319\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.2631 - val_loss: -1.0351\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.3011 - val_loss: -1.0380\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.2551 - val_loss: -1.0409\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.3051 - val_loss: -1.0437\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.2645 - val_loss: -1.0462\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.2533 - val_loss: -1.0483\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.3156 - val_loss: -1.0506\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.2998 - val_loss: -1.0528\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.3304 - val_loss: -1.0553\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.2846 - val_loss: -1.0576\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.2781 - val_loss: -1.0596\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.3164 - val_loss: -1.0617\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.3140 - val_loss: -1.0641\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.2725 - val_loss: -1.0666\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.2934 - val_loss: -1.0688\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.2830 - val_loss: -1.0709\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.3023 - val_loss: -1.0729\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3014 - val_loss: -1.0743\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.3565 - val_loss: -1.0753\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3217 - val_loss: -1.0764\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3276 - val_loss: -1.0771\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.2988 - val_loss: -1.0779\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.3266 - val_loss: -1.0792\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.2977 - val_loss: -1.0804\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.3323 - val_loss: -1.0814\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3040 - val_loss: -1.0818\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3227 - val_loss: -1.0820\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.3109 - val_loss: -1.0825\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.3188 - val_loss: -1.0833\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.3059 - val_loss: -1.0838\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.2800 - val_loss: -1.0843\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.2893 - val_loss: -1.0847\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.3134 - val_loss: -1.0852\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3561 - val_loss: -1.0855\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.2629 - val_loss: -1.0861\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3052 - val_loss: -1.0868\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3349 - val_loss: -1.0877\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3041 - val_loss: -1.0877\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3515 - val_loss: -1.0876\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3032 - val_loss: -1.0879\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.3091 - val_loss: -1.0882\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.3444 - val_loss: -1.0885\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.3278 - val_loss: -1.0886\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.3163 - val_loss: -1.0887\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3352 - val_loss: -1.0898\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3243 - val_loss: -1.0911\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.2795 - val_loss: -1.0923\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.3113 - val_loss: -1.0931\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3001 - val_loss: -1.0934\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.2942 - val_loss: -1.0937\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3027 - val_loss: -1.0944\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.3667 - val_loss: -1.0946\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3070 - val_loss: -1.0946\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.3425 - val_loss: -1.0947\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.3621 - val_loss: -1.0954\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.3157 - val_loss: -1.0957\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3670 - val_loss: -1.0958\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.3452 - val_loss: -1.0958\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.3372 - val_loss: -1.0957\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3478 - val_loss: -1.0957\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.3437 - val_loss: -1.0950\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.3649 - val_loss: -1.0942\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.3874 - val_loss: -1.0938\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3312 - val_loss: -1.0928\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3199 - val_loss: -1.0916\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3353 - val_loss: -1.0906\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 131ms/step - loss: -1.3983 - val_loss: -1.0900\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.2854 - val_loss: -1.0894\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3936 - val_loss: -1.0885\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.3261 - val_loss: -1.0873\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.2913 - val_loss: -1.0860\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3605 - val_loss: -1.0851\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.3180 - val_loss: -1.0847\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3604 - val_loss: -1.0849\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.3252 - val_loss: -1.0854\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.3717 - val_loss: -1.0858\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.3105 - val_loss: -1.0861\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.3310 - val_loss: -1.0862\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.3327 - val_loss: -1.0866\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.3161 - val_loss: -1.0869\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3517 - val_loss: -1.0872\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.3690 - val_loss: -1.0872\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.3117 - val_loss: -1.0872\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3469 - val_loss: -1.0872\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3389 - val_loss: -1.0873\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3226 - val_loss: -1.0874\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.3758 - val_loss: -1.0878\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.3625 - val_loss: -1.0889\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.3992 - val_loss: -1.0903\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.3387 - val_loss: -1.0914\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.4046 - val_loss: -1.0919\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3393 - val_loss: -1.0925\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.3338 - val_loss: -1.0935\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3562 - val_loss: -1.0945\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3445 - val_loss: -1.0954\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.3006 - val_loss: -1.0961\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.3832 - val_loss: -1.0964\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.3362 - val_loss: -1.0963\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.3243 - val_loss: -1.0960\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3665 - val_loss: -1.0949\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3267 - val_loss: -1.0934\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.3623 - val_loss: -1.0921\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.3457 - val_loss: -1.0911\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.3380 - val_loss: -1.0904\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.3643 - val_loss: -1.0900\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.3449 - val_loss: -1.0897\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00158: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_3 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 508ms/step - loss: -2.3223 - val_loss: -1.9930\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -2.4702 - val_loss: -2.0063\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -2.5897 - val_loss: -2.0197\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -2.4518 - val_loss: -2.0330\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.5682 - val_loss: -2.0465\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.5266 - val_loss: -2.0601\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.5190 - val_loss: -2.0738\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.6354 - val_loss: -2.0874\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.5169 - val_loss: -2.1010\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.5994 - val_loss: -2.1143\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.7577 - val_loss: -2.1275\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -2.7408 - val_loss: -2.1408\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -2.7421 - val_loss: -2.1539\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.7024 - val_loss: -2.1669\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -2.6930 - val_loss: -2.1797\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.7535 - val_loss: -2.1923\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -2.8045 - val_loss: -2.2046\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.8224 - val_loss: -2.2170\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.8584 - val_loss: -2.2292\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.8329 - val_loss: -2.2413\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.8303 - val_loss: -2.2532\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.7683 - val_loss: -2.2650\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.9435 - val_loss: -2.2766\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.7591 - val_loss: -2.2879\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.9377 - val_loss: -2.2988\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -3.0977 - val_loss: -2.3092\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.0171 - val_loss: -2.3194\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.8798 - val_loss: -2.3296\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -3.0650 - val_loss: -2.3395\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.0240 - val_loss: -2.3494\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.0060 - val_loss: -2.3590\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.9906 - val_loss: -2.3688\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.9845 - val_loss: -2.3783\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -3.0366 - val_loss: -2.3876\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -3.2512 - val_loss: -2.3966\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -3.1429 - val_loss: -2.4056\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -3.0319 - val_loss: -2.4144\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.0485 - val_loss: -2.4229\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.1329 - val_loss: -2.4312\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.9812 - val_loss: -2.4391\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -3.1567 - val_loss: -2.4470\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -3.1921 - val_loss: -2.4549\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -3.2428 - val_loss: -2.4625\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -3.1790 - val_loss: -2.4700\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -3.1538 - val_loss: -2.4775\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -3.1602 - val_loss: -2.4847\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -3.2641 - val_loss: -2.4917\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -3.3740 - val_loss: -2.4986\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -3.2502 - val_loss: -2.5053\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -3.47 - 0s 102ms/step - loss: -3.4572 - val_loss: -2.5119\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -3.3686 - val_loss: -2.5183\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -3.4092 - val_loss: -2.5247\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.3299 - val_loss: -2.5310\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.3595 - val_loss: -2.5372\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.5152 - val_loss: -2.5434\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -3.5585 - val_loss: -2.5494\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -3.4472 - val_loss: -2.5552\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -3.4687 - val_loss: -2.5609\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.4294 - val_loss: -2.5667\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.5192 - val_loss: -2.5725\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.4582 - val_loss: -2.5782\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -3.4259 - val_loss: -2.5839\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -3.5646 - val_loss: -2.5895\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.5940 - val_loss: -2.5952\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -3.5161 - val_loss: -2.6007\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -3.5682 - val_loss: -2.6063\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.6472 - val_loss: -2.6119\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.5097 - val_loss: -2.6175\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -3.6712 - val_loss: -2.6231\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -3.5853 - val_loss: -2.6287\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -3.6363 - val_loss: -2.6343\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.6458 - val_loss: -2.6401\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.5336 - val_loss: -2.6460\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.5659 - val_loss: -2.6520\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.6815 - val_loss: -2.6580\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 147ms/step - loss: -3.7267 - val_loss: -2.6641\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -3.7114 - val_loss: -2.6702\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -3.7138 - val_loss: -2.6767\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -3.7225 - val_loss: -2.6831\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -3.7130 - val_loss: -2.6895\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -3.7475 - val_loss: -2.6959\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -3.7266 - val_loss: -2.7026\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -3.7932 - val_loss: -2.7094\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -3.8103 - val_loss: -2.7165\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -3.7736 - val_loss: -2.7236\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -3.7680 - val_loss: -2.7309\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -3.7984 - val_loss: -2.7384\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -3.7951 - val_loss: -2.7461\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -3.7737 - val_loss: -2.7537\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -3.9078 - val_loss: -2.7616\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -3.8449 - val_loss: -2.7696\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -3.9174 - val_loss: -2.7776\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -3.9704 - val_loss: -2.7857\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -3.8824 - val_loss: -2.7940\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -3.9127 - val_loss: -2.8023\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -3.8977 - val_loss: -2.8109\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -3.9826 - val_loss: -2.8195\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -3.8509 - val_loss: -2.8280\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.0285 - val_loss: -2.8367\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -4.0089 - val_loss: -2.8454\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -3.9332 - val_loss: -2.8540\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.0769 - val_loss: -2.8627\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -4.0040 - val_loss: -2.8712\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -4.1056 - val_loss: -2.8795\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -4.0142 - val_loss: -2.8877\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.1068 - val_loss: -2.8961\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -4.0637 - val_loss: -2.9044\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.1018 - val_loss: -2.9128\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.2332 - val_loss: -2.9211\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.2210 - val_loss: -2.9294\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -4.1641 - val_loss: -2.9374\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -4.0997 - val_loss: -2.9453\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.1588 - val_loss: -2.9530\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 153ms/step - loss: -4.1653 - val_loss: -2.9607\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.2152 - val_loss: -2.9681\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.1524 - val_loss: -2.9754\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.1823 - val_loss: -2.9823\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.2691 - val_loss: -2.9891\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.2743 - val_loss: -2.9955\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -4.1664 - val_loss: -3.0017\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 145ms/step - loss: -4.2338 - val_loss: -3.0077\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -4.2738 - val_loss: -3.0134\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.1826 - val_loss: -3.0189\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.3394 - val_loss: -3.0242\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -4.2375 - val_loss: -3.0291\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.2923 - val_loss: -3.0340\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.2642 - val_loss: -3.0388\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.2453 - val_loss: -3.0433\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.3787 - val_loss: -3.0477\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.3250 - val_loss: -3.0520\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.3411 - val_loss: -3.0562\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.3840 - val_loss: -3.0602\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -4.3635 - val_loss: -3.0640\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.3150 - val_loss: -3.0676\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.3402 - val_loss: -3.0711\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.3658 - val_loss: -3.0745\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -4.3742 - val_loss: -3.0776\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -4.3861 - val_loss: -3.0805\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.4483 - val_loss: -3.0831\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -4.4040 - val_loss: -3.0857\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -4.4306 - val_loss: -3.0881\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.3562 - val_loss: -3.0904\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.3861 - val_loss: -3.0925\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.4757 - val_loss: -3.0946\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.4445 - val_loss: -3.0965\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.4980 - val_loss: -3.0983\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.4030 - val_loss: -3.1001\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.4032 - val_loss: -3.1017\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -4.4731 - val_loss: -3.1032\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.5184 - val_loss: -3.1046\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.4413 - val_loss: -3.1060\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.5169 - val_loss: -3.1072\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.5086 - val_loss: -3.1084\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.5402 - val_loss: -3.1095\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.4886 - val_loss: -3.1105\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.5013 - val_loss: -3.1115\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -4.5049 - val_loss: -3.1124\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -4.4309 - val_loss: -3.1133\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -4.4731 - val_loss: -3.1141\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -4.4957 - val_loss: -3.1150\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -4.5717 - val_loss: -3.1158\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.4891 - val_loss: -3.1165\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -4.5479 - val_loss: -3.1171\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.4828 - val_loss: -3.1177\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -4.5106 - val_loss: -3.1183\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.5367 - val_loss: -3.1189\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -4.5449 - val_loss: -3.1195\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6002 - val_loss: -3.1200\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.5498 - val_loss: -3.1205\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -4.5425 - val_loss: -3.1210\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.5881 - val_loss: -3.1214\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -4.5239 - val_loss: -3.1219\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.5840 - val_loss: -3.1223\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -4.5558 - val_loss: -3.1227\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.5871 - val_loss: -3.1230\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -4.5765 - val_loss: -3.1234\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.5205 - val_loss: -3.1237\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -4.5695 - val_loss: -3.1241\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.5513 - val_loss: -3.1244\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -4.5783 - val_loss: -3.1247\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.5844 - val_loss: -3.1250\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -4.5750 - val_loss: -3.1253\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -4.6198 - val_loss: -3.1255\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.6174 - val_loss: -3.1258\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 153ms/step - loss: -4.5997 - val_loss: -3.1260\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.5933 - val_loss: -3.1263\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.5946 - val_loss: -3.1265\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.5879 - val_loss: -3.1267\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -4.5773 - val_loss: -3.1269\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -4.6265 - val_loss: -3.1271\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -4.6250 - val_loss: -3.1273\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -4.5361 - val_loss: -3.1275\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.5875 - val_loss: -3.1276\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.6061 - val_loss: -3.1278\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -4.5102 - val_loss: -3.1279\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -4.6406 - val_loss: -3.1281\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.5765 - val_loss: -3.1282\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6042 - val_loss: -3.1284\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.6273 - val_loss: -3.1285\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -4.5908 - val_loss: -3.1286\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -4.6036 - val_loss: -3.1287\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -4.5626 - val_loss: -3.1288\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -4.5872 - val_loss: -3.1290\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -4.5657 - val_loss: -3.1291\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.6211 - val_loss: -3.1292\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6079 - val_loss: -3.1293\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -4.5735 - val_loss: -3.1294\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -4.5958 - val_loss: -3.1295\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -4.5939 - val_loss: -3.1296\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -4.5959 - val_loss: -3.1297\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -4.5645 - val_loss: -3.1298\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -4.5955 - val_loss: -3.1299\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.5977 - val_loss: -3.1299\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6224 - val_loss: -3.1300\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6396 - val_loss: -3.1301\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6215 - val_loss: -3.1302\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -4.5980 - val_loss: -3.1303\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.6265 - val_loss: -3.1303\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -4.6392 - val_loss: -3.1304\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -4.5925 - val_loss: -3.1304\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.6050 - val_loss: -3.1305\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.6421 - val_loss: -3.1306\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -4.5870 - val_loss: -3.1306\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.6228 - val_loss: -3.1306\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6096 - val_loss: -3.1307\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -4.6106 - val_loss: -3.1307\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -4.6048 - val_loss: -3.1308\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6281 - val_loss: -3.1308\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6127 - val_loss: -3.1308\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -4.6117 - val_loss: -3.1309\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -4.6234 - val_loss: -3.1309\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -4.6209 - val_loss: -3.1309\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.6488 - val_loss: -3.1310\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -4.6390 - val_loss: -3.1310\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.6460 - val_loss: -3.1310\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6147 - val_loss: -3.1310\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -4.6491 - val_loss: -3.1311\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6382 - val_loss: -3.1311\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6313 - val_loss: -3.1311\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -4.5939 - val_loss: -3.1312\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6469 - val_loss: -3.1312\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -4.6101 - val_loss: -3.1312\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -4.6267 - val_loss: -3.1312\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -4.6226 - val_loss: -3.1312\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -4.6006 - val_loss: -3.1313\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -4.6217 - val_loss: -3.1313\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -4.6497 - val_loss: -3.1313\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6303 - val_loss: -3.1313\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6334 - val_loss: -3.1313\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6420 - val_loss: -3.1314\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -4.6024 - val_loss: -3.1314\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6295 - val_loss: -3.1314\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.5947 - val_loss: -3.1314\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6302 - val_loss: -3.1314\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6649 - val_loss: -3.1314\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6490 - val_loss: -3.1314\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -4.6486 - val_loss: -3.1314\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6459 - val_loss: -3.1314\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -4.6408 - val_loss: -3.1314\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -4.6549 - val_loss: -3.1314\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 152ms/step - loss: -4.6499 - val_loss: -3.1315\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -4.6283 - val_loss: -3.1315\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.6272 - val_loss: -3.1315\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -4.6379 - val_loss: -3.1315\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -4.6563 - val_loss: -3.1315\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -4.6628 - val_loss: -3.1315\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.6468 - val_loss: -3.1315\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6234 - val_loss: -3.1315\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -4.6553 - val_loss: -3.1315\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6579 - val_loss: -3.1315\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -4.6623 - val_loss: -3.1315\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -4.6272 - val_loss: -3.1315\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -4.6276 - val_loss: -3.1316\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -4.6517 - val_loss: -3.1316\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -4.6271 - val_loss: -3.1316\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -4.6452 - val_loss: -3.1316\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -4.6350 - val_loss: -3.1316\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00277: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 486ms/step - loss: -2.5794 - val_loss: -1.7625\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.6288 - val_loss: -1.7619\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.5871 - val_loss: -1.7614\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.6173 - val_loss: -1.7607\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.6498 - val_loss: -1.7601\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.6627 - val_loss: -1.7596\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.6421 - val_loss: -1.7590\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.6783 - val_loss: -1.7585\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -2.6353 - val_loss: -1.7580\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.6653 - val_loss: -1.7575\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.6521 - val_loss: -1.7569\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.5877 - val_loss: -1.7563\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.6706 - val_loss: -1.7557\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.6823 - val_loss: -1.7552\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.6757 - val_loss: -1.7546\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.6692 - val_loss: -1.7542\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.6797 - val_loss: -1.7538\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -2.6637 - val_loss: -1.7534\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -2.6694 - val_loss: -1.7530\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -2.6537 - val_loss: -1.7525\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -2.6376 - val_loss: -1.7520\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.6679 - val_loss: -1.7516\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.6709 - val_loss: -1.7513\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -2.6700 - val_loss: -1.7510\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -2.6970 - val_loss: -1.7508\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.6964 - val_loss: -1.7507\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.6844 - val_loss: -1.7506\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.6945 - val_loss: -1.7505\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.7056 - val_loss: -1.7504\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -2.7027 - val_loss: -1.7504\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.6922 - val_loss: -1.7504\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.7214 - val_loss: -1.7505\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.6791 - val_loss: -1.7506\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.7217 - val_loss: -1.7508\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.6956 - val_loss: -1.7510\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.6735 - val_loss: -1.7512\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -2.6985 - val_loss: -1.7516\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.6932 - val_loss: -1.7518\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.6971 - val_loss: -1.7519\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -2.6800 - val_loss: -1.7521\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.6822 - val_loss: -1.7521\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.7183 - val_loss: -1.7523\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.7074 - val_loss: -1.7525\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.7113 - val_loss: -1.7527\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.6706 - val_loss: -1.7528\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.7001 - val_loss: -1.7529\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.7117 - val_loss: -1.7531\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -2.7174 - val_loss: -1.7534\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -2.6998 - val_loss: -1.7537\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -2.6980 - val_loss: -1.7541\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.7350 - val_loss: -1.7546\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_5 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 484ms/step - loss: -1.0729 - val_loss: -1.5965\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.1079 - val_loss: -1.5929\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.1114 - val_loss: -1.5891\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.0948 - val_loss: -1.5855\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.1057 - val_loss: -1.5817\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.1623 - val_loss: -1.5780\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.1431 - val_loss: -1.5746\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.0804 - val_loss: -1.5708\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.1508 - val_loss: -1.5671\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.1651 - val_loss: -1.5636\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.1083 - val_loss: -1.5601\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.1348 - val_loss: -1.5567\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.1486 - val_loss: -1.5533\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.2189 - val_loss: -1.5500\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.1358 - val_loss: -1.5468\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.1066 - val_loss: -1.5439\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.1525 - val_loss: -1.5409\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.1349 - val_loss: -1.5381\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.1895 - val_loss: -1.5355\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.1858 - val_loss: -1.5329\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.2067 - val_loss: -1.5308\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.2118 - val_loss: -1.5291\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2180 - val_loss: -1.5276\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.2256 - val_loss: -1.5261\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.2140 - val_loss: -1.5248\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.2195 - val_loss: -1.5237\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.1997 - val_loss: -1.5226\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.2490 - val_loss: -1.5216\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.2032 - val_loss: -1.5211\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.2284 - val_loss: -1.5210\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.2243 - val_loss: -1.5209\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.2428 - val_loss: -1.5209\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.2319 - val_loss: -1.5213\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.1686 - val_loss: -1.5214\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.2835 - val_loss: -1.5217\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.2700 - val_loss: -1.5229\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.2435 - val_loss: -1.5249\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.2437 - val_loss: -1.5267\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.2169 - val_loss: -1.5278\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.2850 - val_loss: -1.5299\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.2856 - val_loss: -1.5331\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.2812 - val_loss: -1.5370\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3111 - val_loss: -1.5421\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.3260 - val_loss: -1.5483\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3104 - val_loss: -1.5547\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.3003 - val_loss: -1.5616\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3612 - val_loss: -1.5692\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.3384 - val_loss: -1.5772\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.3250 - val_loss: -1.5861\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3930 - val_loss: -1.5951\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.3599 - val_loss: -1.6042\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.3539 - val_loss: -1.6137\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.3182 - val_loss: -1.6236\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.3850 - val_loss: -1.6342\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3685 - val_loss: -1.6452\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3341 - val_loss: -1.6559\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.3623 - val_loss: -1.6670\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3855 - val_loss: -1.6790\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.3626 - val_loss: -1.6912\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3715 - val_loss: -1.7043\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.4439 - val_loss: -1.7183\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.4238 - val_loss: -1.7333\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.4325 - val_loss: -1.7484\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -1.4119 - val_loss: -1.7639\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4218 - val_loss: -1.7794\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.4642 - val_loss: -1.7951\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.4211 - val_loss: -1.8108\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4579 - val_loss: -1.8265\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3794 - val_loss: -1.8421\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.5033 - val_loss: -1.8580\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4588 - val_loss: -1.8747\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.5046 - val_loss: -1.8918\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.5342 - val_loss: -1.9088\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.5432 - val_loss: -1.9258\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.5426 - val_loss: -1.9428\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.5699 - val_loss: -1.9597\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.5412 - val_loss: -1.9764\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.4983 - val_loss: -1.9927\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.5884 - val_loss: -2.0085\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.5598 - val_loss: -2.0239\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.5107 - val_loss: -2.0394\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.6581 - val_loss: -2.0546\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.6192 - val_loss: -2.0692\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.6272 - val_loss: -2.0838\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6171 - val_loss: -2.0986\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6187 - val_loss: -2.1135\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.5544 - val_loss: -2.1280\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.6420 - val_loss: -2.1420\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6564 - val_loss: -2.1557\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6803 - val_loss: -2.1694\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.7294 - val_loss: -2.1827\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 148ms/step - loss: -1.7242 - val_loss: -2.1955\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6975 - val_loss: -2.2079\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.7139 - val_loss: -2.2198\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6905 - val_loss: -2.2315\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7427 - val_loss: -2.2431\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.6623 - val_loss: -2.2544\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7423 - val_loss: -2.2655\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7399 - val_loss: -2.2760\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7339 - val_loss: -2.2859\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7287 - val_loss: -2.2952\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.7925 - val_loss: -2.3043\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7928 - val_loss: -2.3130\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.7923 - val_loss: -2.3211\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.7628 - val_loss: -2.3290\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.7873 - val_loss: -2.3367\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8076 - val_loss: -2.3442\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.7966 - val_loss: -2.3512\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8242 - val_loss: -2.3577\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8262 - val_loss: -2.3641\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8454 - val_loss: -2.3703\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8158 - val_loss: -2.3762\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8163 - val_loss: -2.3817\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7928 - val_loss: -2.3869\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9093 - val_loss: -2.3918\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8499 - val_loss: -2.3966\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8694 - val_loss: -2.4011\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8815 - val_loss: -2.4055\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.9191 - val_loss: -2.4095\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8906 - val_loss: -2.4133\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.9240 - val_loss: -2.4168\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.9180 - val_loss: -2.4201\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9264 - val_loss: -2.4231\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8946 - val_loss: -2.4260\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.9169 - val_loss: -2.4288\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.9044 - val_loss: -2.4314\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.9517 - val_loss: -2.4339\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.9415 - val_loss: -2.4361\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.9043 - val_loss: -2.4382\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.9362 - val_loss: -2.4402\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.9383 - val_loss: -2.4421\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.9172 - val_loss: -2.4439\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.9750 - val_loss: -2.4455\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.9183 - val_loss: -2.4471\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.9263 - val_loss: -2.4486\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.9483 - val_loss: -2.4501\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.9519 - val_loss: -2.4514\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.9499 - val_loss: -2.4527\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.9396 - val_loss: -2.4540\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.9401 - val_loss: -2.4552\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.9488 - val_loss: -2.4563\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.9504 - val_loss: -2.4575\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.9487 - val_loss: -2.4587\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -1.9766 - val_loss: -2.4598\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.9589 - val_loss: -2.4609\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -1.9637 - val_loss: -2.4620\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.9584 - val_loss: -2.4630\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.9908 - val_loss: -2.4640\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.9711 - val_loss: -2.4649\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.9982 - val_loss: -2.4658\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.9769 - val_loss: -2.4666\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.9943 - val_loss: -2.4674\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.9449 - val_loss: -2.4682\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.9904 - val_loss: -2.4689\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.9747 - val_loss: -2.4696\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.9856 - val_loss: -2.4703\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.9991 - val_loss: -2.4709\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -2.0063 - val_loss: -2.4714\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -2.0092 - val_loss: -2.4720\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0033 - val_loss: -2.4725\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -2.0076 - val_loss: -2.4730\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.9978 - val_loss: -2.4734\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.9945 - val_loss: -2.4739\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0010 - val_loss: -2.4743\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.0011 - val_loss: -2.4747\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -2.0097 - val_loss: -2.4751\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -2.0097 - val_loss: -2.4755\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.0056 - val_loss: -2.4758\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -2.0135 - val_loss: -2.4762\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -2.0251 - val_loss: -2.4765\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.9931 - val_loss: -2.4768\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0004 - val_loss: -2.4772\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -2.0126 - val_loss: -2.4775\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0122 - val_loss: -2.4778\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0099 - val_loss: -2.4781\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0131 - val_loss: -2.4785\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0052 - val_loss: -2.4788\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0087 - val_loss: -2.4791\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.9929 - val_loss: -2.4794\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0189 - val_loss: -2.4797\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0332 - val_loss: -2.4800\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0101 - val_loss: -2.4802\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0270 - val_loss: -2.4805\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0158 - val_loss: -2.4808\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -2.0311 - val_loss: -2.4810\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -2.0229 - val_loss: -2.4812\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0187 - val_loss: -2.4815\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -2.0235 - val_loss: -2.4817\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0283 - val_loss: -2.4819\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.0228 - val_loss: -2.4821\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -2.0195 - val_loss: -2.4823\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0383 - val_loss: -2.4825\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.0185 - val_loss: -2.4827\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -2.0165 - val_loss: -2.4829\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0393 - val_loss: -2.4831\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0222 - val_loss: -2.4833\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0319 - val_loss: -2.4834\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -2.0290 - val_loss: -2.4836\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.9977 - val_loss: -2.4838\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0210 - val_loss: -2.4839\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0308 - val_loss: -2.4841\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0035 - val_loss: -2.4842\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0292 - val_loss: -2.4844\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.0327 - val_loss: -2.4846\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -2.0298 - val_loss: -2.4847\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0227 - val_loss: -2.4849\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0397 - val_loss: -2.4850\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -2.0361 - val_loss: -2.4852\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -2.0145 - val_loss: -2.4853\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0309 - val_loss: -2.4854\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0240 - val_loss: -2.4856\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0418 - val_loss: -2.4857\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0370 - val_loss: -2.4859\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -2.0387 - val_loss: -2.4860\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0262 - val_loss: -2.4861\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.0283 - val_loss: -2.4862\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.0318 - val_loss: -2.4863\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0324 - val_loss: -2.4864\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0334 - val_loss: -2.4866\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0435 - val_loss: -2.4867\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -2.0476 - val_loss: -2.4868\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -2.0252 - val_loss: -2.4869\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.0463 - val_loss: -2.4870\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0381 - val_loss: -2.4871\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0350 - val_loss: -2.4872\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -2.0408 - val_loss: -2.4873\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0432 - val_loss: -2.4874\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0331 - val_loss: -2.4874\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0385 - val_loss: -2.4875\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0478 - val_loss: -2.4876\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0429 - val_loss: -2.4877\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -2.0421 - val_loss: -2.4877\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.0422 - val_loss: -2.4878\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 132ms/step - loss: -2.0368 - val_loss: -2.4879\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0499 - val_loss: -2.4880\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -2.0470 - val_loss: -2.4880\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0400 - val_loss: -2.4881\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0397 - val_loss: -2.4882\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0428 - val_loss: -2.4882\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0483 - val_loss: -2.4883\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0253 - val_loss: -2.4883\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0444 - val_loss: -2.4884\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -2.0428 - val_loss: -2.4885\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -2.0471 - val_loss: -2.4885\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -2.0408 - val_loss: -2.4886\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0420 - val_loss: -2.4887\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0275 - val_loss: -2.4887\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -2.0476 - val_loss: -2.4888\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0466 - val_loss: -2.4888\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0445 - val_loss: -2.4889\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0525 - val_loss: -2.4890\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0387 - val_loss: -2.4890\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0469 - val_loss: -2.4891\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0431 - val_loss: -2.4891\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0387 - val_loss: -2.4892\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0351 - val_loss: -2.4892\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0433 - val_loss: -2.4893\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0433 - val_loss: -2.4893\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0476 - val_loss: -2.4894\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0440 - val_loss: -2.4894\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0507 - val_loss: -2.4895\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0415 - val_loss: -2.4895\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -2.0447 - val_loss: -2.4896\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -2.0512 - val_loss: -2.4896\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0370 - val_loss: -2.4896\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0520 - val_loss: -2.4897\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0532 - val_loss: -2.4897\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -2.0464 - val_loss: -2.4898\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.0518 - val_loss: -2.4898\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0511 - val_loss: -2.4898\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0516 - val_loss: -2.4899\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -2.0474 - val_loss: -2.4899\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -2.0505 - val_loss: -2.4899\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0544 - val_loss: -2.4900\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0532 - val_loss: -2.4900\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0324 - val_loss: -2.4900\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0462 - val_loss: -2.4901\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -2.0523 - val_loss: -2.4901\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0507 - val_loss: -2.4901\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0512 - val_loss: -2.4902\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0426 - val_loss: -2.4902\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0509 - val_loss: -2.4902\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: -2.0457 - val_loss: -2.4903\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.0458 - val_loss: -2.4903\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -2.0491 - val_loss: -2.4903\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0548 - val_loss: -2.4904\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0496 - val_loss: -2.4904\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0498 - val_loss: -2.4904\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0476 - val_loss: -2.4905\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.0498 - val_loss: -2.4905\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.0495 - val_loss: -2.4905\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0471 - val_loss: -2.4906\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0500 - val_loss: -2.4906\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0562 - val_loss: -2.4906\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0460 - val_loss: -2.4907\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -2.0493 - val_loss: -2.4907\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0507 - val_loss: -2.4907\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0538 - val_loss: -2.4907\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0475 - val_loss: -2.4908\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0559 - val_loss: -2.4908\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.0531 - val_loss: -2.4908\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0511 - val_loss: -2.4909\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -2.0539 - val_loss: -2.4909\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0535 - val_loss: -2.4909\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0481 - val_loss: -2.4909\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0517 - val_loss: -2.4910\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -2.0535 - val_loss: -2.4910\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0534 - val_loss: -2.4910\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0545 - val_loss: -2.4910\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0537 - val_loss: -2.4911\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0501 - val_loss: -2.4911\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0546 - val_loss: -2.4911\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0561 - val_loss: -2.4911\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0551 - val_loss: -2.4911\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0497 - val_loss: -2.4912\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0557 - val_loss: -2.4912\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0558 - val_loss: -2.4912\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0485 - val_loss: -2.4912\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0488 - val_loss: -2.4912\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0524 - val_loss: -2.4913\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0571 - val_loss: -2.4913\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0525 - val_loss: -2.4913\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0525 - val_loss: -2.4913\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0429 - val_loss: -2.4914\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0538 - val_loss: -2.4914\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0485 - val_loss: -2.4914\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0452 - val_loss: -2.4914\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0527 - val_loss: -2.4915\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0490 - val_loss: -2.4915\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0536 - val_loss: -2.4915\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0565 - val_loss: -2.4915\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0563 - val_loss: -2.4916\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -2.0539 - val_loss: -2.4916\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -2.0576 - val_loss: -2.4916\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: -2.0572 - val_loss: -2.4916\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0494 - val_loss: -2.4916\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0496 - val_loss: -2.4916\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0540 - val_loss: -2.4917\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0555 - val_loss: -2.4917\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -2.0546 - val_loss: -2.4917\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.0463 - val_loss: -2.4917\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0550 - val_loss: -2.4917\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0518 - val_loss: -2.4918\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0531 - val_loss: -2.4918\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0556 - val_loss: -2.4918\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0547 - val_loss: -2.4918\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -2.0526 - val_loss: -2.4918\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0554 - val_loss: -2.4918\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0509 - val_loss: -2.4919\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 136ms/step - loss: -2.0536 - val_loss: -2.4919\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0543 - val_loss: -2.4919\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.0552 - val_loss: -2.4919\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0571 - val_loss: -2.4919\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0539 - val_loss: -2.4920\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0563 - val_loss: -2.4920\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -2.0464 - val_loss: -2.4920\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -2.0522 - val_loss: -2.4920\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0601 - val_loss: -2.4920\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -2.0551 - val_loss: -2.4920\n",
      "Epoch 360/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0556 - val_loss: -2.4921\n",
      "Epoch 361/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0493 - val_loss: -2.4921\n",
      "Epoch 362/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0561 - val_loss: -2.4921\n",
      "Epoch 363/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -2.0537 - val_loss: -2.4921\n",
      "Epoch 364/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -2.0567 - val_loss: -2.4921\n",
      "Epoch 365/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0554 - val_loss: -2.4921\n",
      "Epoch 366/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0571 - val_loss: -2.4922\n",
      "Epoch 367/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -2.0592 - val_loss: -2.4922\n",
      "Epoch 368/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0531 - val_loss: -2.4922\n",
      "Epoch 369/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -2.0516 - val_loss: -2.4922\n",
      "Epoch 370/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0591 - val_loss: -2.4922\n",
      "Epoch 371/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0550 - val_loss: -2.4922\n",
      "Epoch 372/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -2.0548 - val_loss: -2.4922\n",
      "Epoch 373/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -2.0561 - val_loss: -2.4922\n",
      "Epoch 374/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -2.0576 - val_loss: -2.4923\n",
      "Epoch 375/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -2.0569 - val_loss: -2.4923\n",
      "Epoch 376/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -2.0581 - val_loss: -2.4923\n",
      "Epoch 377/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -2.0533 - val_loss: -2.4923\n",
      "Epoch 378/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -2.0571 - val_loss: -2.4923\n",
      "Epoch 379/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -2.0582 - val_loss: -2.4923\n",
      "Epoch 380/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0563 - val_loss: -2.4923\n",
      "Epoch 381/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -2.0581 - val_loss: -2.4923\n",
      "Epoch 382/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -2.0565 - val_loss: -2.4924\n",
      "Epoch 383/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -2.0565 - val_loss: -2.4924\n",
      "Epoch 384/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0574 - val_loss: -2.4924\n",
      "Epoch 385/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -2.0558 - val_loss: -2.4924\n",
      "Epoch 386/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -2.0502 - val_loss: -2.4924\n",
      "Epoch 387/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -2.0581 - val_loss: -2.4924\n",
      "Epoch 388/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -2.0578 - val_loss: -2.4924\n",
      "Epoch 389/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -2.0600 - val_loss: -2.4924\n",
      "Epoch 390/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0569 - val_loss: -2.4924\n",
      "Epoch 391/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -2.0600 - val_loss: -2.4925\n",
      "Epoch 392/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -2.0570 - val_loss: -2.4925\n",
      "Epoch 393/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -2.0593 - val_loss: -2.4925\n",
      "Epoch 394/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -2.0557 - val_loss: -2.4925\n",
      "Epoch 395/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0598 - val_loss: -2.4925\n",
      "Epoch 396/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -2.0604 - val_loss: -2.4925\n",
      "Epoch 397/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0562 - val_loss: -2.4925\n",
      "Epoch 398/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -2.0516 - val_loss: -2.4925\n",
      "Epoch 399/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -2.0556 - val_loss: -2.4925\n",
      "Epoch 400/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -2.0581 - val_loss: -2.4926\n",
      "Epoch 401/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0572 - val_loss: -2.4926\n",
      "Epoch 402/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -2.0521 - val_loss: -2.4926\n",
      "Epoch 403/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -2.0575 - val_loss: -2.4926\n",
      "Epoch 404/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -2.0601 - val_loss: -2.4926\n",
      "Epoch 405/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -2.0573 - val_loss: -2.4926\n",
      "Epoch 406/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -2.0514 - val_loss: -2.4926\n",
      "Epoch 407/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -2.0567 - val_loss: -2.4926\n",
      "Epoch 408/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -2.0563 - val_loss: -2.4927\n",
      "Epoch 409/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -2.0585 - val_loss: -2.4927\n",
      "Epoch 410/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -2.0580 - val_loss: -2.4927\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00410: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_6 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 881ms/step - loss: -0.4802 - val_loss: -2.5272\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.4615 - val_loss: -2.5298\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.4558 - val_loss: -2.5326\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4573 - val_loss: -2.5353\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4791 - val_loss: -2.5385\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.4696 - val_loss: -2.5420\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -0.4697 - val_loss: -2.5460\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.4739 - val_loss: -2.5505\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.4897 - val_loss: -2.5554\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.4749 - val_loss: -2.5603\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.4549 - val_loss: -2.5653\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.4572 - val_loss: -2.5705\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.4853 - val_loss: -2.5758\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.4793 - val_loss: -2.5809\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.4970 - val_loss: -2.5858\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.4835 - val_loss: -2.5908\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4923 - val_loss: -2.5955\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.4908 - val_loss: -2.5998\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.4955 - val_loss: -2.6034\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.4976 - val_loss: -2.6083\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.4921 - val_loss: -2.6131\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5107 - val_loss: -2.6179\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 138ms/step - loss: -0.4848 - val_loss: -2.6226\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.4772 - val_loss: -2.6275\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.4882 - val_loss: -2.6336\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.4714 - val_loss: -2.6405\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5129 - val_loss: -2.6476\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5016 - val_loss: -2.6549\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5096 - val_loss: -2.6613\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5079 - val_loss: -2.6671\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.4823 - val_loss: -2.6724\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5027 - val_loss: -2.6779\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.4981 - val_loss: -2.6842\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5099 - val_loss: -2.6916\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5168 - val_loss: -2.6996\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5215 - val_loss: -2.7072\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.4694 - val_loss: -2.7141\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5107 - val_loss: -2.7207\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5071 - val_loss: -2.7271\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5122 - val_loss: -2.7333\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5044 - val_loss: -2.7394\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5225 - val_loss: -2.7453\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5025 - val_loss: -2.7512\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5025 - val_loss: -2.7568\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5096 - val_loss: -2.7613\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5365 - val_loss: -2.7660\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5277 - val_loss: -2.7711\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5210 - val_loss: -2.7766\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5156 - val_loss: -2.7815\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5264 - val_loss: -2.7858\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.5330 - val_loss: -2.7897\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5292 - val_loss: -2.7932\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -0.5388 - val_loss: -2.7962\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.4976 - val_loss: -2.7996\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5127 - val_loss: -2.8036\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5303 - val_loss: -2.8081\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5389 - val_loss: -2.8133\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5109 - val_loss: -2.8185\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5217 - val_loss: -2.8233\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5148 - val_loss: -2.8276\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5306 - val_loss: -2.8313\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5328 - val_loss: -2.8349\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5271 - val_loss: -2.8387\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5220 - val_loss: -2.8421\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5306 - val_loss: -2.8453\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5254 - val_loss: -2.8481\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5210 - val_loss: -2.8505\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5137 - val_loss: -2.8530\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5189 - val_loss: -2.8559\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5097 - val_loss: -2.8590\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5348 - val_loss: -2.8621\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5320 - val_loss: -2.8658\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5429 - val_loss: -2.8694\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5304 - val_loss: -2.8729\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5236 - val_loss: -2.8760\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5384 - val_loss: -2.8788\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5454 - val_loss: -2.8808\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5216 - val_loss: -2.8830\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5412 - val_loss: -2.8855\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5423 - val_loss: -2.8881\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5172 - val_loss: -2.8910\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5282 - val_loss: -2.8940\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5395 - val_loss: -2.8973\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5361 - val_loss: -2.9005\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5335 - val_loss: -2.9034\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5289 - val_loss: -2.9069\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5316 - val_loss: -2.9107\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5459 - val_loss: -2.9142\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5405 - val_loss: -2.9173\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5359 - val_loss: -2.9201\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5477 - val_loss: -2.9225\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5318 - val_loss: -2.9248\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.5357 - val_loss: -2.9270\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.5427 - val_loss: -2.9286\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.5383 - val_loss: -2.9298\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5415 - val_loss: -2.9316\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5362 - val_loss: -2.9340\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5498 - val_loss: -2.9356\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5299 - val_loss: -2.9374\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5378 - val_loss: -2.9392\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5526 - val_loss: -2.9409\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5377 - val_loss: -2.9427\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5435 - val_loss: -2.9449\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5433 - val_loss: -2.9473\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5472 - val_loss: -2.9494\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5559 - val_loss: -2.9509\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5429 - val_loss: -2.9525\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5538 - val_loss: -2.9540\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5434 - val_loss: -2.9556\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5473 - val_loss: -2.9569\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5445 - val_loss: -2.9581\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5237 - val_loss: -2.9594\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5521 - val_loss: -2.9606\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5487 - val_loss: -2.9619\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5478 - val_loss: -2.9631\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5494 - val_loss: -2.9643\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5493 - val_loss: -2.9654\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5457 - val_loss: -2.9665\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5494 - val_loss: -2.9675\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5503 - val_loss: -2.9684\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5327 - val_loss: -2.9696\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5372 - val_loss: -2.9710\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5437 - val_loss: -2.9724\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -0.5334 - val_loss: -2.9741\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5415 - val_loss: -2.9759\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5546 - val_loss: -2.9777\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5439 - val_loss: -2.9793\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5458 - val_loss: -2.9807\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5342 - val_loss: -2.9819\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.5466 - val_loss: -2.9831\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5330 - val_loss: -2.9843\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5372 - val_loss: -2.9857\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.5477 - val_loss: -2.9872\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5475 - val_loss: -2.9887\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.5434 - val_loss: -2.9904\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5490 - val_loss: -2.9918\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5423 - val_loss: -2.9931\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5481 - val_loss: -2.9944\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.5568 - val_loss: -2.9954\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5464 - val_loss: -2.9963\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5443 - val_loss: -2.9972\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.5487 - val_loss: -2.9983\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5524 - val_loss: -2.9994\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5472 - val_loss: -3.0005\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5531 - val_loss: -3.0014\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5467 - val_loss: -3.0022\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5548 - val_loss: -3.0030\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5561 - val_loss: -3.0037\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.5536 - val_loss: -3.0043\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5462 - val_loss: -3.0049\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5508 - val_loss: -3.0054\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5581 - val_loss: -3.0058\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5478 - val_loss: -3.0062\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5502 - val_loss: -3.0066\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5540 - val_loss: -3.0070\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5479 - val_loss: -3.0073\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5517 - val_loss: -3.0077\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5479 - val_loss: -3.0082\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5499 - val_loss: -3.0087\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5482 - val_loss: -3.0092\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5436 - val_loss: -3.0097\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5562 - val_loss: -3.0101\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.5496 - val_loss: -3.0105\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5449 - val_loss: -3.0111\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.5467 - val_loss: -3.0116\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -0.5515 - val_loss: -3.0121\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5527 - val_loss: -3.0126\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5436 - val_loss: -3.0132\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.5511 - val_loss: -3.0138\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5514 - val_loss: -3.0144\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.5524 - val_loss: -3.0149\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5508 - val_loss: -3.0154\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -0.5496 - val_loss: -3.0159\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5517 - val_loss: -3.0163\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5502 - val_loss: -3.0167\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5557 - val_loss: -3.0170\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5467 - val_loss: -3.0173\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5492 - val_loss: -3.0177\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5452 - val_loss: -3.0182\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5602 - val_loss: -3.0187\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5453 - val_loss: -3.0190\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.5555 - val_loss: -3.0192\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5590 - val_loss: -3.0193\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.5497 - val_loss: -3.0195\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.5513 - val_loss: -3.0197\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5541 - val_loss: -3.0199\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5542 - val_loss: -3.0202\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5537 - val_loss: -3.0206\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5526 - val_loss: -3.0210\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5509 - val_loss: -3.0214\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5504 - val_loss: -3.0218\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5602 - val_loss: -3.0220\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5479 - val_loss: -3.0223\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5512 - val_loss: -3.0224\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5521 - val_loss: -3.0226\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5497 - val_loss: -3.0228\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.5568 - val_loss: -3.0229\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5493 - val_loss: -3.0231\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5583 - val_loss: -3.0232\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5564 - val_loss: -3.0234\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5527 - val_loss: -3.0236\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5518 - val_loss: -3.0238\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5503 - val_loss: -3.0241\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5560 - val_loss: -3.0243\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5591 - val_loss: -3.0245\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5520 - val_loss: -3.0247\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5542 - val_loss: -3.0248\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5526 - val_loss: -3.0249\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5544 - val_loss: -3.0250\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5557 - val_loss: -3.0251\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.5585 - val_loss: -3.0251\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5470 - val_loss: -3.0253\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5541 - val_loss: -3.0255\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.5573 - val_loss: -3.0256\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -0.5568 - val_loss: -3.0257\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5553 - val_loss: -3.0257\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5565 - val_loss: -3.0257\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5476 - val_loss: -3.0257\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5560 - val_loss: -3.0257\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5518 - val_loss: -3.0258\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5517 - val_loss: -3.0258\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.5537 - val_loss: -3.0260\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5475 - val_loss: -3.0263\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5548 - val_loss: -3.0265\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5545 - val_loss: -3.0267\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.5544 - val_loss: -3.0269\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5545 - val_loss: -3.0270\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5591 - val_loss: -3.0271\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5470 - val_loss: -3.0272\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5583 - val_loss: -3.0273\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5510 - val_loss: -3.0273\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5581 - val_loss: -3.0274\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5451 - val_loss: -3.0276\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5563 - val_loss: -3.0277\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5533 - val_loss: -3.0278\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5537 - val_loss: -3.0279\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5513 - val_loss: -3.0280\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.5548 - val_loss: -3.0281\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5531 - val_loss: -3.0284\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -0.5552 - val_loss: -3.0286\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.5498 - val_loss: -3.0289\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.5578 - val_loss: -3.0291\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.5535 - val_loss: -3.0294\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.5553 - val_loss: -3.0297\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5539 - val_loss: -3.0299\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5534 - val_loss: -3.0301\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5558 - val_loss: -3.0303\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5543 - val_loss: -3.0304\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5523 - val_loss: -3.0306\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5547 - val_loss: -3.0309\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5556 - val_loss: -3.0312\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5552 - val_loss: -3.0315\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5575 - val_loss: -3.0317\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.5564 - val_loss: -3.0318\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5599 - val_loss: -3.0318\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.5525 - val_loss: -3.0318\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5573 - val_loss: -3.0318\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5558 - val_loss: -3.0317\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.5528 - val_loss: -3.0318\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.5569 - val_loss: -3.0317\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5521 - val_loss: -3.0318\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.5572 - val_loss: -3.0319\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5553 - val_loss: -3.0320\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -0.5524 - val_loss: -3.0321\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5568 - val_loss: -3.0322\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5537 - val_loss: -3.0323\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5548 - val_loss: -3.0324\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -0.5521 - val_loss: -3.0325\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5569 - val_loss: -3.0325\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5537 - val_loss: -3.0326\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5602 - val_loss: -3.0326\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5589 - val_loss: -3.0325\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5552 - val_loss: -3.0325\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5554 - val_loss: -3.0324\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5563 - val_loss: -3.0324\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5555 - val_loss: -3.0323\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5543 - val_loss: -3.0323\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.5546 - val_loss: -3.0323\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5543 - val_loss: -3.0323\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5559 - val_loss: -3.0322\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.5560 - val_loss: -3.0322\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5516 - val_loss: -3.0323\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5534 - val_loss: -3.0326\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5555 - val_loss: -3.0329\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5464 - val_loss: -3.0331\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 152ms/step - loss: -0.5601 - val_loss: -3.0333\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.5554 - val_loss: -3.0333\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -0.5537 - val_loss: -3.0334\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.5499 - val_loss: -3.0335\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.5589 - val_loss: -3.0335\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5543 - val_loss: -3.0335\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5570 - val_loss: -3.0334\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.5570 - val_loss: -3.0334\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5593 - val_loss: -3.0333\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.5555 - val_loss: -3.0332\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.5584 - val_loss: -3.0330\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -0.5537 - val_loss: -3.0329\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5494 - val_loss: -3.0328\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.5528 - val_loss: -3.0329\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5593 - val_loss: -3.0330\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5571 - val_loss: -3.0330\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5549 - val_loss: -3.0331\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5593 - val_loss: -3.0330\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.5557 - val_loss: -3.0329\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.5576 - val_loss: -3.0328\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.5578 - val_loss: -3.0327\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.5553 - val_loss: -3.0325\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5588 - val_loss: -3.0323\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5541 - val_loss: -3.0322\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5586 - val_loss: -3.0320\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.5554 - val_loss: -3.0318\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -0.5537 - val_loss: -3.0317\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.5588 - val_loss: -3.0315\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.5591 - val_loss: -3.0312\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -0.5566 - val_loss: -3.0310\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.5544 - val_loss: -3.0308\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -0.5567 - val_loss: -3.0308\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.5595 - val_loss: -3.0307\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5562 - val_loss: -3.0306\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5553 - val_loss: -3.0305\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.5586 - val_loss: -3.0304\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.5579 - val_loss: -3.0303\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5560 - val_loss: -3.0304\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5572 - val_loss: -3.0304\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -0.5563 - val_loss: -3.0304\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -0.5563 - val_loss: -3.0304\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.5554 - val_loss: -3.0304\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.5546 - val_loss: -3.0305\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5550 - val_loss: -3.0307\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5532 - val_loss: -3.0309\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.5561 - val_loss: -3.0311\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.5568 - val_loss: -3.0313\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.5500 - val_loss: -3.0315\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -0.5558 - val_loss: -3.0318\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.5596 - val_loss: -3.0321\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00335: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_7 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 476ms/step - loss: -0.5954 - val_loss: -2.2567\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -0.6082 - val_loss: -2.2790\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.5935 - val_loss: -2.2993\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 144ms/step - loss: -0.6061 - val_loss: -2.3195\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6008 - val_loss: -2.3408\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.5961 - val_loss: -2.3627\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -0.6125 - val_loss: -2.3848\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.6017 - val_loss: -2.4064\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.6074 - val_loss: -2.4273\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6011 - val_loss: -2.4477\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -0.6090 - val_loss: -2.4674\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.6217 - val_loss: -2.4863\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6117 - val_loss: -2.5049\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6329 - val_loss: -2.5230\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.5991 - val_loss: -2.5405\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.6177 - val_loss: -2.5568\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.6220 - val_loss: -2.5716\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6057 - val_loss: -2.5866\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6395 - val_loss: -2.6010\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.6088 - val_loss: -2.6151\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6388 - val_loss: -2.6295\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6274 - val_loss: -2.6435\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.6090 - val_loss: -2.6560\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6202 - val_loss: -2.6666\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6404 - val_loss: -2.6761\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6093 - val_loss: -2.6853\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.6162 - val_loss: -2.6939\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6124 - val_loss: -2.7028\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -0.6326 - val_loss: -2.7111\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6250 - val_loss: -2.7189\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.6493 - val_loss: -2.7248\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -0.6251 - val_loss: -2.7306\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.6041 - val_loss: -2.7372\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.6106 - val_loss: -2.7450\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6307 - val_loss: -2.7533\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6336 - val_loss: -2.7606\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6108 - val_loss: -2.7682\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6214 - val_loss: -2.7756\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -0.6294 - val_loss: -2.7823\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.6269 - val_loss: -2.7880\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6336 - val_loss: -2.7940\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.6234 - val_loss: -2.8001\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.6254 - val_loss: -2.8066\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.6437 - val_loss: -2.8128\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -0.6149 - val_loss: -2.8183\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.6214 - val_loss: -2.8235\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6262 - val_loss: -2.8279\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -0.6270 - val_loss: -2.8320\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6163 - val_loss: -2.8362\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6276 - val_loss: -2.8397\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6395 - val_loss: -2.8420\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.6247 - val_loss: -2.8446\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6313 - val_loss: -2.8470\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6281 - val_loss: -2.8494\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -0.6203 - val_loss: -2.8526\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -0.61 - 0s 104ms/step - loss: -0.6348 - val_loss: -2.8557\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -0.6411 - val_loss: -2.8584\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.6398 - val_loss: -2.8609\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6434 - val_loss: -2.8632\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6182 - val_loss: -2.8661\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6255 - val_loss: -2.8698\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -0.6380 - val_loss: -2.8733\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6368 - val_loss: -2.8766\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6422 - val_loss: -2.8794\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6359 - val_loss: -2.8809\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.6459 - val_loss: -2.8820\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -0.6281 - val_loss: -2.8833\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -0.6429 - val_loss: -2.8848\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.6460 - val_loss: -2.8864\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -0.6310 - val_loss: -2.8875\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.6406 - val_loss: -2.8880\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 139ms/step - loss: -0.6452 - val_loss: -2.8883\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6291 - val_loss: -2.8887\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.6499 - val_loss: -2.8888\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.6413 - val_loss: -2.8887\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -0.6296 - val_loss: -2.8889\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -0.6371 - val_loss: -2.8894\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.6452 - val_loss: -2.8906\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.6486 - val_loss: -2.8919\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.6400 - val_loss: -2.8934\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -0.6408 - val_loss: -2.8951\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6366 - val_loss: -2.8965\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -0.6351 - val_loss: -2.8972\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6289 - val_loss: -2.8978\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -0.6317 - val_loss: -2.8986\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.6217 - val_loss: -2.8988\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -0.6426 - val_loss: -2.8989\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -0.6380 - val_loss: -2.8996\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -0.6432 - val_loss: -2.9011\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6372 - val_loss: -2.9023\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6342 - val_loss: -2.9035\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -0.6370 - val_loss: -2.9054\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6255 - val_loss: -2.9070\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.6560 - val_loss: -2.9084\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6539 - val_loss: -2.9094\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6333 - val_loss: -2.9100\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -0.6253 - val_loss: -2.9114\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: -0.6449 - val_loss: -2.9136\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.6429 - val_loss: -2.9165\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -0.6297 - val_loss: -2.9194\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -0.6509 - val_loss: -2.9216\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -0.6462 - val_loss: -2.9238\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -0.6344 - val_loss: -2.9255\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6424 - val_loss: -2.9266\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -0.6439 - val_loss: -2.9270\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6408 - val_loss: -2.9268\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.6418 - val_loss: -2.9263\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -0.6512 - val_loss: -2.9261\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.6393 - val_loss: -2.9250\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -0.6440 - val_loss: -2.9237\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -0.6456 - val_loss: -2.9221\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.6541 - val_loss: -2.9204\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.6501 - val_loss: -2.9188\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -0.6411 - val_loss: -2.9171\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6414 - val_loss: -2.9147\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -0.6368 - val_loss: -2.9125\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.6383 - val_loss: -2.9102\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -0.6390 - val_loss: -2.9063\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -0.6535 - val_loss: -2.9023\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.6468 - val_loss: -2.9003\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -0.6442 - val_loss: -2.8990\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -0.6483 - val_loss: -2.8982\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -0.6424 - val_loss: -2.8978\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -0.6505 - val_loss: -2.8970\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6433 - val_loss: -2.8958\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6279 - val_loss: -2.8936\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6526 - val_loss: -2.8907\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6496 - val_loss: -2.8878\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6477 - val_loss: -2.8857\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -0.6385 - val_loss: -2.8839\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6464 - val_loss: -2.8811\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -0.6378 - val_loss: -2.8777\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -0.6508 - val_loss: -2.8743\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6534 - val_loss: -2.8707\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -0.6486 - val_loss: -2.8677\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6481 - val_loss: -2.8659\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -0.6516 - val_loss: -2.8649\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -0.6441 - val_loss: -2.8639\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -0.6433 - val_loss: -2.8626\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -0.6515 - val_loss: -2.8610\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -0.6439 - val_loss: -2.8603\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -0.6293 - val_loss: -2.8597\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6416 - val_loss: -2.8593\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -0.6405 - val_loss: -2.8604\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -0.6492 - val_loss: -2.8615\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -0.6503 - val_loss: -2.8621\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -0.6392 - val_loss: -2.8627\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -0.6458 - val_loss: -2.8638\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -0.6528 - val_loss: -2.8663\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -0.6531 - val_loss: -2.8688\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -0.6417 - val_loss: -2.8713\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -0.6492 - val_loss: -2.8725\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -0.6619 - val_loss: -2.8734\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -0.6531 - val_loss: -2.8722\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00154: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_8 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 472ms/step - loss: -1.3331 - val_loss: -2.7496\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3694 - val_loss: -2.7668\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.3028 - val_loss: -2.7838\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.3007 - val_loss: -2.8006\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.3547 - val_loss: -2.8170\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.3328 - val_loss: -2.8321\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.3475 - val_loss: -2.8452\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.3269 - val_loss: -2.8574\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.3327 - val_loss: -2.8694\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.3079 - val_loss: -2.8803\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.3739 - val_loss: -2.8895\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.3814 - val_loss: -2.8972\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.3500 - val_loss: -2.9040\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3649 - val_loss: -2.9100\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.3653 - val_loss: -2.9160\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3851 - val_loss: -2.9219\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.3639 - val_loss: -2.9274\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.3741 - val_loss: -2.9330\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.3692 - val_loss: -2.9373\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.3583 - val_loss: -2.9412\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.3984 - val_loss: -2.9436\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.3701 - val_loss: -2.9454\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.3729 - val_loss: -2.9467\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -1.32 - 0s 97ms/step - loss: -1.4064 - val_loss: -2.9479\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.3826 - val_loss: -2.9492\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.3935 - val_loss: -2.9501\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.4140 - val_loss: -2.9510\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.4030 - val_loss: -2.9529\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.3988 - val_loss: -2.9551\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.4061 - val_loss: -2.9569\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.4205 - val_loss: -2.9589\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4003 - val_loss: -2.9597\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.4178 - val_loss: -2.9605\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.4187 - val_loss: -2.9604\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: -1.3807 - val_loss: -2.9597\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.3906 - val_loss: -2.9596\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.3975 - val_loss: -2.9603\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4311 - val_loss: -2.9608\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.4229 - val_loss: -2.9609\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.3944 - val_loss: -2.9600\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.3938 - val_loss: -2.9580\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.3849 - val_loss: -2.9560\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.4471 - val_loss: -2.9541\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.3950 - val_loss: -2.9519\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.3970 - val_loss: -2.9496\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.3871 - val_loss: -2.9470\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.4086 - val_loss: -2.9434\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.4002 - val_loss: -2.9405\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.4108 - val_loss: -2.9375\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.4632 - val_loss: -2.9347\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.4248 - val_loss: -2.9323\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.4150 - val_loss: -2.9304\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.4585 - val_loss: -2.9291\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.3891 - val_loss: -2.9277\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4060 - val_loss: -2.9263\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.3770 - val_loss: -2.9240\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.4090 - val_loss: -2.9201\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4580 - val_loss: -2.9160\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4101 - val_loss: -2.9134\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4113 - val_loss: -2.9110\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.4287 - val_loss: -2.9092\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.4195 - val_loss: -2.9076\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.4170 - val_loss: -2.9057\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.4046 - val_loss: -2.9040\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.4261 - val_loss: -2.9024\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.4321 - val_loss: -2.9006\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.4381 - val_loss: -2.8988\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.4162 - val_loss: -2.8962\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.4384 - val_loss: -2.8939\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.4012 - val_loss: -2.8913\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4332 - val_loss: -2.8886\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.4402 - val_loss: -2.8854\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.4401 - val_loss: -2.8830\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.3978 - val_loss: -2.8814\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.4207 - val_loss: -2.8800\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.4159 - val_loss: -2.8785\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.4268 - val_loss: -2.8763\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.4427 - val_loss: -2.8738\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.4150 - val_loss: -2.8720\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4130 - val_loss: -2.8704\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.4375 - val_loss: -2.8686\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.4637 - val_loss: -2.8657\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.4395 - val_loss: -2.8627\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00083: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_9 (Softmax)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 478ms/step - loss: -1.5474 - val_loss: -1.9593\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.5938 - val_loss: -1.9626\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.5428 - val_loss: -1.9660\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.5938 - val_loss: -1.9695\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.5624 - val_loss: -1.9731\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5591 - val_loss: -1.9764\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.5941 - val_loss: -1.9794\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.5779 - val_loss: -1.9822\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.5636 - val_loss: -1.9849\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.6038 - val_loss: -1.9872\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.5651 - val_loss: -1.9897\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.5941 - val_loss: -1.9925\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6034 - val_loss: -1.9957\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.5845 - val_loss: -1.9991\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.5871 - val_loss: -2.0027\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.5826 - val_loss: -2.0063\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.5904 - val_loss: -2.0098\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.5871 - val_loss: -2.0128\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.5854 - val_loss: -2.0160\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.5845 - val_loss: -2.0196\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.5790 - val_loss: -2.0229\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6080 - val_loss: -2.0265\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.5930 - val_loss: -2.0303\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.6055 - val_loss: -2.0341\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6139 - val_loss: -2.0374\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.5827 - val_loss: -2.0403\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6252 - val_loss: -2.0426\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6335 - val_loss: -2.0443\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6188 - val_loss: -2.0466\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6159 - val_loss: -2.0495\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6119 - val_loss: -2.0527\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.6309 - val_loss: -2.0559\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.5922 - val_loss: -2.0586\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.6456 - val_loss: -2.0609\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.6065 - val_loss: -2.0636\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.6404 - val_loss: -2.0665\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6197 - val_loss: -2.0694\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6191 - val_loss: -2.0724\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6289 - val_loss: -2.0756\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.5951 - val_loss: -2.0787\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6213 - val_loss: -2.0819\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6409 - val_loss: -2.0849\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6040 - val_loss: -2.0870\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6516 - val_loss: -2.0893\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.6297 - val_loss: -2.0920\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6403 - val_loss: -2.0947\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6538 - val_loss: -2.0975\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6188 - val_loss: -2.0995\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.6400 - val_loss: -2.1013\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6583 - val_loss: -2.1024\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6529 - val_loss: -2.1029\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6320 - val_loss: -2.1036\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.6597 - val_loss: -2.1038\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6591 - val_loss: -2.1038\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6474 - val_loss: -2.1044\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6401 - val_loss: -2.1051\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.6472 - val_loss: -2.1056\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6206 - val_loss: -2.1066\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6730 - val_loss: -2.1078\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.6632 - val_loss: -2.1087\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.6676 - val_loss: -2.1092\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.6303 - val_loss: -2.1098\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6081 - val_loss: -2.1102\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6609 - val_loss: -2.1104\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.6188 - val_loss: -2.1106\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.6423 - val_loss: -2.1109\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6644 - val_loss: -2.1110\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6428 - val_loss: -2.1114\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6337 - val_loss: -2.1119\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6643 - val_loss: -2.1124\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.6610 - val_loss: -2.1131\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6415 - val_loss: -2.1138\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6784 - val_loss: -2.1143\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6470 - val_loss: -2.1144\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.6525 - val_loss: -2.1142\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.6344 - val_loss: -2.1145\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6406 - val_loss: -2.1153\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6441 - val_loss: -2.1161\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6605 - val_loss: -2.1165\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.6584 - val_loss: -2.1172\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.6577 - val_loss: -2.1180\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6640 - val_loss: -2.1188\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6259 - val_loss: -2.1200\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.6512 - val_loss: -2.1214\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6411 - val_loss: -2.1233\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6476 - val_loss: -2.1254\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.6778 - val_loss: -2.1273\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6531 - val_loss: -2.1293\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.6640 - val_loss: -2.1312\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.6584 - val_loss: -2.1327\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6220 - val_loss: -2.1347\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6616 - val_loss: -2.1368\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6446 - val_loss: -2.1392\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6671 - val_loss: -2.1408\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6635 - val_loss: -2.1422\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -1.6709 - val_loss: -2.1434\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.6500 - val_loss: -2.1443\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6689 - val_loss: -2.1450\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6422 - val_loss: -2.1458\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.6617 - val_loss: -2.1468\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.6583 - val_loss: -2.1478\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6779 - val_loss: -2.1488\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6350 - val_loss: -2.1497\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6567 - val_loss: -2.1501\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6513 - val_loss: -2.1504\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6585 - val_loss: -2.1504\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.6606 - val_loss: -2.1507\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6316 - val_loss: -2.1507\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6726 - val_loss: -2.1509\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.6634 - val_loss: -2.1506\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.6640 - val_loss: -2.1503\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.6647 - val_loss: -2.1500\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6376 - val_loss: -2.1495\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.6657 - val_loss: -2.1489\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 129ms/step - loss: -1.6790 - val_loss: -2.1476\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.6586 - val_loss: -2.1463\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6730 - val_loss: -2.1449\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6697 - val_loss: -2.1431\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.6796 - val_loss: -2.1416\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6550 - val_loss: -2.1402\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.6498 - val_loss: -2.1385\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -1.6474 - val_loss: -2.1369\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6381 - val_loss: -2.1358\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.5956 - val_loss: -2.1353\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6832 - val_loss: -2.1351\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 137ms/step - loss: -1.6601 - val_loss: -2.1353\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6187 - val_loss: -2.1359\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6497 - val_loss: -2.1361\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6446 - val_loss: -2.1364\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6678 - val_loss: -2.1365\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6552 - val_loss: -2.1370\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6424 - val_loss: -2.1373\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6535 - val_loss: -2.1380\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6455 - val_loss: -2.1388\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.6434 - val_loss: -2.1393\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6591 - val_loss: -2.1395\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6814 - val_loss: -2.1396\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6299 - val_loss: -2.1401\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6568 - val_loss: -2.1408\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.6514 - val_loss: -2.1412\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.6741 - val_loss: -2.1419\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6588 - val_loss: -2.1435\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6349 - val_loss: -2.1449\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6470 - val_loss: -2.1464\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6351 - val_loss: -2.1479\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6199 - val_loss: -2.1492\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6686 - val_loss: -2.1505\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.6873 - val_loss: -2.1520\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6535 - val_loss: -2.1536\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.6622 - val_loss: -2.1553\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6677 - val_loss: -2.1566\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.6693 - val_loss: -2.1573\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6881 - val_loss: -2.1576\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.6384 - val_loss: -2.1576\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.6471 - val_loss: -2.1580\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 145ms/step - loss: -1.6427 - val_loss: -2.1596\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.6620 - val_loss: -2.1608\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 143ms/step - loss: -1.6754 - val_loss: -2.1615\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.6468 - val_loss: -2.1625\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.6327 - val_loss: -2.1631\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6437 - val_loss: -2.1631\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6560 - val_loss: -2.1630\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6885 - val_loss: -2.1630\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6632 - val_loss: -2.1631\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6662 - val_loss: -2.1634\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.6002 - val_loss: -2.1627\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6703 - val_loss: -2.1614\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.6399 - val_loss: -2.1599\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6769 - val_loss: -2.1582\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.6312 - val_loss: -2.1575\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6677 - val_loss: -2.1572\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.6425 - val_loss: -2.1578\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6891 - val_loss: -2.1591\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6854 - val_loss: -2.1603\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.6257 - val_loss: -2.1609\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6778 - val_loss: -2.1607\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.6460 - val_loss: -2.1600\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.6466 - val_loss: -2.1596\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6405 - val_loss: -2.1594\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6549 - val_loss: -2.1597\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6568 - val_loss: -2.1606\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6335 - val_loss: -2.1617\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6827 - val_loss: -2.1622\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6692 - val_loss: -2.1623\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.6902 - val_loss: -2.1622\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.6413 - val_loss: -2.1623\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6619 - val_loss: -2.1628\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.6369 - val_loss: -2.1639\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.6632 - val_loss: -2.1647\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6417 - val_loss: -2.1652\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 125ms/step - loss: -1.6669 - val_loss: -2.1657\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.6963 - val_loss: -2.1666\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6494 - val_loss: -2.1672\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.6633 - val_loss: -2.1676\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 140ms/step - loss: -1.6384 - val_loss: -2.1678\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6701 - val_loss: -2.1674\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6650 - val_loss: -2.1665\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6766 - val_loss: -2.1654\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6901 - val_loss: -2.1636\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6762 - val_loss: -2.1625\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6426 - val_loss: -2.1625\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.6785 - val_loss: -2.1628\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6488 - val_loss: -2.1626\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.6745 - val_loss: -2.1620\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.6590 - val_loss: -2.1612\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6749 - val_loss: -2.1608\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6643 - val_loss: -2.1597\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6546 - val_loss: -2.1587\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6683 - val_loss: -2.1579\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6684 - val_loss: -2.1574\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.6489 - val_loss: -2.1574\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.6419 - val_loss: -2.1582\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6742 - val_loss: -2.1592\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6273 - val_loss: -2.1602\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.6423 - val_loss: -2.1609\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.6709 - val_loss: -2.1614\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.6610 - val_loss: -2.1619\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.6673 - val_loss: -2.1621\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6651 - val_loss: -2.1612\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6758 - val_loss: -2.1598\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6544 - val_loss: -2.1581\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6614 - val_loss: -2.1568\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6661 - val_loss: -2.1565\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.6360 - val_loss: -2.1566\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6568 - val_loss: -2.1564\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6118 - val_loss: -2.1561\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 134ms/step - loss: -1.6567 - val_loss: -2.1559\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.6817 - val_loss: -2.1557\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 117ms/step - loss: -1.6879 - val_loss: -2.1553\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.6321 - val_loss: -2.1555\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.6772 - val_loss: -2.1560\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6623 - val_loss: -2.1566\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.6481 - val_loss: -2.1571\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6690 - val_loss: -2.1574\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.6772 - val_loss: -2.1583\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6947 - val_loss: -2.1596\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.6630 - val_loss: -2.1605\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6540 - val_loss: -2.1605\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.6585 - val_loss: -2.1601\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6398 - val_loss: -2.1600\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.6408 - val_loss: -2.1606\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 140ms/step - loss: -1.6405 - val_loss: -2.1616\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6937 - val_loss: -2.1626\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.6577 - val_loss: -2.1641\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00244: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_10 (Softmax)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 3s 866ms/step - loss: -1.3308 - val_loss: -0.7729\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.3618 - val_loss: -0.7790\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -1.3534 - val_loss: -0.7852\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.3248 - val_loss: -0.7914\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.3840 - val_loss: -0.7976\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.4044 - val_loss: -0.8038\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.3742 - val_loss: -0.8101\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.3882 - val_loss: -0.8163\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.4167 - val_loss: -0.8225\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.3911 - val_loss: -0.8288\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.3735 - val_loss: -0.8349\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.4395 - val_loss: -0.8409\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.4464 - val_loss: -0.8466\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.3861 - val_loss: -0.8523\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.4145 - val_loss: -0.8580\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.4569 - val_loss: -0.8636\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.4338 - val_loss: -0.8691\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.4359 - val_loss: -0.8745\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.4477 - val_loss: -0.8798\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.4562 - val_loss: -0.8849\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.4112 - val_loss: -0.8900\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.4189 - val_loss: -0.8952\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.4665 - val_loss: -0.9005\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.4695 - val_loss: -0.9059\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.4869 - val_loss: -0.9111\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.4210 - val_loss: -0.9163\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.4874 - val_loss: -0.9212\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.5168 - val_loss: -0.9261\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.4654 - val_loss: -0.9307\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5272 - val_loss: -0.9351\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.4813 - val_loss: -0.9394\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.4789 - val_loss: -0.9436\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.5243 - val_loss: -0.9478\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.5010 - val_loss: -0.9520\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4773 - val_loss: -0.9562\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.5425 - val_loss: -0.9604\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.5001 - val_loss: -0.9645\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.5095 - val_loss: -0.9687\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.5013 - val_loss: -0.9728\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.5754 - val_loss: -0.9768\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.5359 - val_loss: -0.9806\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.5540 - val_loss: -0.9843\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.5460 - val_loss: -0.9882\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 162ms/step - loss: -1.5107 - val_loss: -0.9919\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.5334 - val_loss: -0.9957\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.5444 - val_loss: -0.9994\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.5095 - val_loss: -1.0031\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5441 - val_loss: -1.0066\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.5412 - val_loss: -1.0099\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5306 - val_loss: -1.0131\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.5067 - val_loss: -1.0162\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.5375 - val_loss: -1.0194\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.6100 - val_loss: -1.0225\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.5781 - val_loss: -1.0255\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.5408 - val_loss: -1.0286\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.5728 - val_loss: -1.0319\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.5969 - val_loss: -1.0353\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.5928 - val_loss: -1.0385\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.6228 - val_loss: -1.0415\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.5864 - val_loss: -1.0445\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.5423 - val_loss: -1.0476\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6190 - val_loss: -1.0510\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.5951 - val_loss: -1.0544\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.5979 - val_loss: -1.0579\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5608 - val_loss: -1.0614\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.5940 - val_loss: -1.0649\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.5953 - val_loss: -1.0683\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.6370 - val_loss: -1.0719\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.5898 - val_loss: -1.0756\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.5718 - val_loss: -1.0791\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.6722 - val_loss: -1.0826\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6385 - val_loss: -1.0861\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6487 - val_loss: -1.0898\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.6274 - val_loss: -1.0935\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.6230 - val_loss: -1.0974\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.6460 - val_loss: -1.1017\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6538 - val_loss: -1.1058\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.6323 - val_loss: -1.1099\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.6416 - val_loss: -1.1139\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.6204 - val_loss: -1.1179\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.6197 - val_loss: -1.1221\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 82ms/step - loss: -1.6303 - val_loss: -1.1263\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.6965 - val_loss: -1.1307\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.6833 - val_loss: -1.1352\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.5974 - val_loss: -1.1397\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.6893 - val_loss: -1.1442\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 133ms/step - loss: -1.6922 - val_loss: -1.1490\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: -1.6592 - val_loss: -1.1534\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7026 - val_loss: -1.1577\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7113 - val_loss: -1.1621\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7049 - val_loss: -1.1665\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.6866 - val_loss: -1.1709\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7073 - val_loss: -1.1754\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6733 - val_loss: -1.1797\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7012 - val_loss: -1.1838\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7325 - val_loss: -1.1878\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.6901 - val_loss: -1.1919\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7097 - val_loss: -1.1963\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.6871 - val_loss: -1.2006\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.6840 - val_loss: -1.2050\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7202 - val_loss: -1.2092\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.7548 - val_loss: -1.2133\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7500 - val_loss: -1.2175\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.6836 - val_loss: -1.2215\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7494 - val_loss: -1.2254\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.7445 - val_loss: -1.2287\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7136 - val_loss: -1.2321\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7352 - val_loss: -1.2358\n",
      "Epoch 109/1000\n",
      "2/2 [==============================] - 0s 142ms/step - loss: -1.7613 - val_loss: -1.2394\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7490 - val_loss: -1.2426\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7743 - val_loss: -1.2458\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7588 - val_loss: -1.2491\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7563 - val_loss: -1.2524\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6928 - val_loss: -1.2555\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7328 - val_loss: -1.2585\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8044 - val_loss: -1.2614\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.6943 - val_loss: -1.2642\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7809 - val_loss: -1.2669\n",
      "Epoch 119/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.7573 - val_loss: -1.2692\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.7489 - val_loss: -1.2715\n",
      "Epoch 121/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7812 - val_loss: -1.2736\n",
      "Epoch 122/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7412 - val_loss: -1.2759\n",
      "Epoch 123/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7153 - val_loss: -1.2783\n",
      "Epoch 124/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.7339 - val_loss: -1.2808\n",
      "Epoch 125/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7420 - val_loss: -1.2832\n",
      "Epoch 126/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.7503 - val_loss: -1.2858\n",
      "Epoch 127/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.7235 - val_loss: -1.2884\n",
      "Epoch 128/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7628 - val_loss: -1.2911\n",
      "Epoch 129/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7538 - val_loss: -1.2937\n",
      "Epoch 130/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7240 - val_loss: -1.2962\n",
      "Epoch 131/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.7708 - val_loss: -1.2984\n",
      "Epoch 132/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.7854 - val_loss: -1.3005\n",
      "Epoch 133/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7734 - val_loss: -1.3024\n",
      "Epoch 134/1000\n",
      "2/2 [==============================] - 0s 118ms/step - loss: -1.7811 - val_loss: -1.3044\n",
      "Epoch 135/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.8365 - val_loss: -1.3061\n",
      "Epoch 136/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7638 - val_loss: -1.3077\n",
      "Epoch 137/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8091 - val_loss: -1.3093\n",
      "Epoch 138/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.7806 - val_loss: -1.3107\n",
      "Epoch 139/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7574 - val_loss: -1.3123\n",
      "Epoch 140/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7932 - val_loss: -1.3137\n",
      "Epoch 141/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7461 - val_loss: -1.3152\n",
      "Epoch 142/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.7850 - val_loss: -1.3166\n",
      "Epoch 143/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7712 - val_loss: -1.3180\n",
      "Epoch 144/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7744 - val_loss: -1.3195\n",
      "Epoch 145/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.7857 - val_loss: -1.3209\n",
      "Epoch 146/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.7333 - val_loss: -1.3224\n",
      "Epoch 147/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7973 - val_loss: -1.3240\n",
      "Epoch 148/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7911 - val_loss: -1.3254\n",
      "Epoch 149/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.7681 - val_loss: -1.3269\n",
      "Epoch 150/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7868 - val_loss: -1.3283\n",
      "Epoch 151/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7821 - val_loss: -1.3296\n",
      "Epoch 152/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.7923 - val_loss: -1.3308\n",
      "Epoch 153/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.7756 - val_loss: -1.3319\n",
      "Epoch 154/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.7681 - val_loss: -1.3331\n",
      "Epoch 155/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: -1.7930 - val_loss: -1.3342\n",
      "Epoch 156/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8032 - val_loss: -1.3352\n",
      "Epoch 157/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7983 - val_loss: -1.3362\n",
      "Epoch 158/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7678 - val_loss: -1.3373\n",
      "Epoch 159/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.7827 - val_loss: -1.3384\n",
      "Epoch 160/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7411 - val_loss: -1.3396\n",
      "Epoch 161/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7923 - val_loss: -1.3407\n",
      "Epoch 162/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7909 - val_loss: -1.3418\n",
      "Epoch 163/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7925 - val_loss: -1.3427\n",
      "Epoch 164/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7648 - val_loss: -1.3435\n",
      "Epoch 165/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7751 - val_loss: -1.3444\n",
      "Epoch 166/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.7962 - val_loss: -1.3453\n",
      "Epoch 167/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.7972 - val_loss: -1.3462\n",
      "Epoch 168/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.7940 - val_loss: -1.3470\n",
      "Epoch 169/1000\n",
      "2/2 [==============================] - 0s 147ms/step - loss: -1.7898 - val_loss: -1.3477\n",
      "Epoch 170/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8002 - val_loss: -1.3483\n",
      "Epoch 171/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7611 - val_loss: -1.3490\n",
      "Epoch 172/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7916 - val_loss: -1.3497\n",
      "Epoch 173/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.7777 - val_loss: -1.3505\n",
      "Epoch 174/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.7882 - val_loss: -1.3514\n",
      "Epoch 175/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8037 - val_loss: -1.3522\n",
      "Epoch 176/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8071 - val_loss: -1.3530\n",
      "Epoch 177/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.7981 - val_loss: -1.3537\n",
      "Epoch 178/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7911 - val_loss: -1.3545\n",
      "Epoch 179/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7874 - val_loss: -1.3551\n",
      "Epoch 180/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7762 - val_loss: -1.3558\n",
      "Epoch 181/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.7892 - val_loss: -1.3565\n",
      "Epoch 182/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7907 - val_loss: -1.3571\n",
      "Epoch 183/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8134 - val_loss: -1.3576\n",
      "Epoch 184/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7652 - val_loss: -1.3583\n",
      "Epoch 185/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.7803 - val_loss: -1.3590\n",
      "Epoch 186/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.8047 - val_loss: -1.3598\n",
      "Epoch 187/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8077 - val_loss: -1.3605\n",
      "Epoch 188/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7973 - val_loss: -1.3612\n",
      "Epoch 189/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.8130 - val_loss: -1.3617\n",
      "Epoch 190/1000\n",
      "2/2 [==============================] - 0s 112ms/step - loss: -1.8110 - val_loss: -1.3622\n",
      "Epoch 191/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.7941 - val_loss: -1.3627\n",
      "Epoch 192/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7771 - val_loss: -1.3632\n",
      "Epoch 193/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.7812 - val_loss: -1.3637\n",
      "Epoch 194/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8009 - val_loss: -1.3642\n",
      "Epoch 195/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7837 - val_loss: -1.3647\n",
      "Epoch 196/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7860 - val_loss: -1.3653\n",
      "Epoch 197/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.7991 - val_loss: -1.3660\n",
      "Epoch 198/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8140 - val_loss: -1.3666\n",
      "Epoch 199/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.7880 - val_loss: -1.3671\n",
      "Epoch 200/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7791 - val_loss: -1.3677\n",
      "Epoch 201/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8050 - val_loss: -1.3682\n",
      "Epoch 202/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8052 - val_loss: -1.3687\n",
      "Epoch 203/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8233 - val_loss: -1.3692\n",
      "Epoch 204/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7765 - val_loss: -1.3696\n",
      "Epoch 205/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7912 - val_loss: -1.3699\n",
      "Epoch 206/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8010 - val_loss: -1.3702\n",
      "Epoch 207/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8011 - val_loss: -1.3705\n",
      "Epoch 208/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.7928 - val_loss: -1.3707\n",
      "Epoch 209/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.7903 - val_loss: -1.3711\n",
      "Epoch 210/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8204 - val_loss: -1.3714\n",
      "Epoch 211/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.7994 - val_loss: -1.3716\n",
      "Epoch 212/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.7972 - val_loss: -1.3719\n",
      "Epoch 213/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7980 - val_loss: -1.3722\n",
      "Epoch 214/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8054 - val_loss: -1.3724\n",
      "Epoch 215/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7800 - val_loss: -1.3727\n",
      "Epoch 216/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7885 - val_loss: -1.3730\n",
      "Epoch 217/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.7999 - val_loss: -1.3733\n",
      "Epoch 218/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7883 - val_loss: -1.3736\n",
      "Epoch 219/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8140 - val_loss: -1.3738\n",
      "Epoch 220/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.8143 - val_loss: -1.3740\n",
      "Epoch 221/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8191 - val_loss: -1.3742\n",
      "Epoch 222/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.7910 - val_loss: -1.3744\n",
      "Epoch 223/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: -1.8052 - val_loss: -1.3747\n",
      "Epoch 224/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8169 - val_loss: -1.3748\n",
      "Epoch 225/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8212 - val_loss: -1.3750\n",
      "Epoch 226/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7964 - val_loss: -1.3751\n",
      "Epoch 227/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8092 - val_loss: -1.3753\n",
      "Epoch 228/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.7908 - val_loss: -1.3754\n",
      "Epoch 229/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8109 - val_loss: -1.3756\n",
      "Epoch 230/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8031 - val_loss: -1.3758\n",
      "Epoch 231/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8134 - val_loss: -1.3759\n",
      "Epoch 232/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.7831 - val_loss: -1.3761\n",
      "Epoch 233/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.8087 - val_loss: -1.3763\n",
      "Epoch 234/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: -1.7913 - val_loss: -1.3766\n",
      "Epoch 235/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8020 - val_loss: -1.3769\n",
      "Epoch 236/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8211 - val_loss: -1.3772\n",
      "Epoch 237/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8051 - val_loss: -1.3775\n",
      "Epoch 238/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.7768 - val_loss: -1.3776\n",
      "Epoch 239/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: -1.8293 - val_loss: -1.3778\n",
      "Epoch 240/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8199 - val_loss: -1.3780\n",
      "Epoch 241/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8086 - val_loss: -1.3781\n",
      "Epoch 242/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8140 - val_loss: -1.3781\n",
      "Epoch 243/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.7895 - val_loss: -1.3782\n",
      "Epoch 244/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.7982 - val_loss: -1.3783\n",
      "Epoch 245/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8118 - val_loss: -1.3784\n",
      "Epoch 246/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7820 - val_loss: -1.3785\n",
      "Epoch 247/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7944 - val_loss: -1.3786\n",
      "Epoch 248/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8037 - val_loss: -1.3787\n",
      "Epoch 249/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8033 - val_loss: -1.3788\n",
      "Epoch 250/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8046 - val_loss: -1.3789\n",
      "Epoch 251/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: -1.8076 - val_loss: -1.3789\n",
      "Epoch 252/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8017 - val_loss: -1.3790\n",
      "Epoch 253/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7890 - val_loss: -1.3791\n",
      "Epoch 254/1000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: -1.8116 - val_loss: -1.3792\n",
      "Epoch 255/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.8113 - val_loss: -1.3793\n",
      "Epoch 256/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8012 - val_loss: -1.3794\n",
      "Epoch 257/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7899 - val_loss: -1.3795\n",
      "Epoch 258/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8033 - val_loss: -1.3795\n",
      "Epoch 259/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7830 - val_loss: -1.3797\n",
      "Epoch 260/1000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: -1.8027 - val_loss: -1.3799\n",
      "Epoch 261/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8030 - val_loss: -1.3802\n",
      "Epoch 262/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7951 - val_loss: -1.3804\n",
      "Epoch 263/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8077 - val_loss: -1.3805\n",
      "Epoch 264/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.7973 - val_loss: -1.3806\n",
      "Epoch 265/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8174 - val_loss: -1.3807\n",
      "Epoch 266/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7977 - val_loss: -1.3808\n",
      "Epoch 267/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8150 - val_loss: -1.3809\n",
      "Epoch 268/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7737 - val_loss: -1.3810\n",
      "Epoch 269/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8061 - val_loss: -1.3810\n",
      "Epoch 270/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7955 - val_loss: -1.3811\n",
      "Epoch 271/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: -1.8136 - val_loss: -1.3812\n",
      "Epoch 272/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.7908 - val_loss: -1.3813\n",
      "Epoch 273/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8021 - val_loss: -1.3814\n",
      "Epoch 274/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.7926 - val_loss: -1.3816\n",
      "Epoch 275/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8026 - val_loss: -1.3818\n",
      "Epoch 276/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7951 - val_loss: -1.3820\n",
      "Epoch 277/1000\n",
      "2/2 [==============================] - 0s 123ms/step - loss: -1.8088 - val_loss: -1.3822\n",
      "Epoch 278/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8113 - val_loss: -1.3823\n",
      "Epoch 279/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8042 - val_loss: -1.3824\n",
      "Epoch 280/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8158 - val_loss: -1.3824\n",
      "Epoch 281/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: -1.8126 - val_loss: -1.3824\n",
      "Epoch 282/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.7924 - val_loss: -1.3825\n",
      "Epoch 283/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8000 - val_loss: -1.3825\n",
      "Epoch 284/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7972 - val_loss: -1.3826\n",
      "Epoch 285/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8177 - val_loss: -1.3826\n",
      "Epoch 286/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8015 - val_loss: -1.3826\n",
      "Epoch 287/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8033 - val_loss: -1.3826\n",
      "Epoch 288/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7959 - val_loss: -1.3827\n",
      "Epoch 289/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8170 - val_loss: -1.3827\n",
      "Epoch 290/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8066 - val_loss: -1.3828\n",
      "Epoch 291/1000\n",
      "2/2 [==============================] - 0s 113ms/step - loss: -1.8053 - val_loss: -1.3829\n",
      "Epoch 292/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7995 - val_loss: -1.3831\n",
      "Epoch 293/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8045 - val_loss: -1.3832\n",
      "Epoch 294/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8148 - val_loss: -1.3833\n",
      "Epoch 295/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8179 - val_loss: -1.3834\n",
      "Epoch 296/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8278 - val_loss: -1.3834\n",
      "Epoch 297/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8087 - val_loss: -1.3834\n",
      "Epoch 298/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7972 - val_loss: -1.3834\n",
      "Epoch 299/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8114 - val_loss: -1.3834\n",
      "Epoch 300/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.7905 - val_loss: -1.3834\n",
      "Epoch 301/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8065 - val_loss: -1.3835\n",
      "Epoch 302/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8123 - val_loss: -1.3835\n",
      "Epoch 303/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8087 - val_loss: -1.3836\n",
      "Epoch 304/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8198 - val_loss: -1.3836\n",
      "Epoch 305/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8166 - val_loss: -1.3836\n",
      "Epoch 306/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.7917 - val_loss: -1.3837\n",
      "Epoch 307/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8019 - val_loss: -1.3837\n",
      "Epoch 308/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8111 - val_loss: -1.3837\n",
      "Epoch 309/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8091 - val_loss: -1.3838\n",
      "Epoch 310/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7957 - val_loss: -1.3838\n",
      "Epoch 311/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8234 - val_loss: -1.3839\n",
      "Epoch 312/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8055 - val_loss: -1.3840\n",
      "Epoch 313/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8106 - val_loss: -1.3841\n",
      "Epoch 314/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8152 - val_loss: -1.3841\n",
      "Epoch 315/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8091 - val_loss: -1.3842\n",
      "Epoch 316/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8034 - val_loss: -1.3843\n",
      "Epoch 317/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8012 - val_loss: -1.3844\n",
      "Epoch 318/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8183 - val_loss: -1.3845\n",
      "Epoch 319/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8121 - val_loss: -1.3845\n",
      "Epoch 320/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8155 - val_loss: -1.3845\n",
      "Epoch 321/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8047 - val_loss: -1.3845\n",
      "Epoch 322/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: -1.8085 - val_loss: -1.3845\n",
      "Epoch 323/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8096 - val_loss: -1.3845\n",
      "Epoch 324/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8002 - val_loss: -1.3845\n",
      "Epoch 325/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8160 - val_loss: -1.3846\n",
      "Epoch 326/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8151 - val_loss: -1.3847\n",
      "Epoch 327/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8136 - val_loss: -1.3847\n",
      "Epoch 328/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8138 - val_loss: -1.3847\n",
      "Epoch 329/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8095 - val_loss: -1.3848\n",
      "Epoch 330/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8147 - val_loss: -1.3848\n",
      "Epoch 331/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8082 - val_loss: -1.3848\n",
      "Epoch 332/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8156 - val_loss: -1.3849\n",
      "Epoch 333/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.7958 - val_loss: -1.3849\n",
      "Epoch 334/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8070 - val_loss: -1.3850\n",
      "Epoch 335/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8045 - val_loss: -1.3851\n",
      "Epoch 336/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8188 - val_loss: -1.3852\n",
      "Epoch 337/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8161 - val_loss: -1.3852\n",
      "Epoch 338/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.7991 - val_loss: -1.3853\n",
      "Epoch 339/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8123 - val_loss: -1.3853\n",
      "Epoch 340/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.8072 - val_loss: -1.3853\n",
      "Epoch 341/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8103 - val_loss: -1.3853\n",
      "Epoch 342/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8101 - val_loss: -1.3853\n",
      "Epoch 343/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8045 - val_loss: -1.3852\n",
      "Epoch 344/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8082 - val_loss: -1.3852\n",
      "Epoch 345/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8002 - val_loss: -1.3852\n",
      "Epoch 346/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8130 - val_loss: -1.3851\n",
      "Epoch 347/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7985 - val_loss: -1.3851\n",
      "Epoch 348/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8089 - val_loss: -1.3851\n",
      "Epoch 349/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8006 - val_loss: -1.3852\n",
      "Epoch 350/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.8047 - val_loss: -1.3852\n",
      "Epoch 351/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8123 - val_loss: -1.3853\n",
      "Epoch 352/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8018 - val_loss: -1.3854\n",
      "Epoch 353/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8107 - val_loss: -1.3854\n",
      "Epoch 354/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8088 - val_loss: -1.3855\n",
      "Epoch 355/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7874 - val_loss: -1.3856\n",
      "Epoch 356/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7891 - val_loss: -1.3857\n",
      "Epoch 357/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7962 - val_loss: -1.3859\n",
      "Epoch 358/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8194 - val_loss: -1.3861\n",
      "Epoch 359/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.8087 - val_loss: -1.3862\n",
      "Epoch 360/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8075 - val_loss: -1.3862\n",
      "Epoch 361/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.8079 - val_loss: -1.3862\n",
      "Epoch 362/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.7993 - val_loss: -1.3863\n",
      "Epoch 363/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.7965 - val_loss: -1.3864\n",
      "Epoch 364/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8053 - val_loss: -1.3864\n",
      "Epoch 365/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8041 - val_loss: -1.3865\n",
      "Epoch 366/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8116 - val_loss: -1.3866\n",
      "Epoch 367/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8070 - val_loss: -1.3867\n",
      "Epoch 368/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.7936 - val_loss: -1.3868\n",
      "Epoch 369/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8086 - val_loss: -1.3870\n",
      "Epoch 370/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8153 - val_loss: -1.3871\n",
      "Epoch 371/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8089 - val_loss: -1.3871\n",
      "Epoch 372/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8145 - val_loss: -1.3872\n",
      "Epoch 373/1000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: -1.8100 - val_loss: -1.3872\n",
      "Epoch 374/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8125 - val_loss: -1.3872\n",
      "Epoch 375/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.8140 - val_loss: -1.3872\n",
      "Epoch 376/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8015 - val_loss: -1.3871\n",
      "Epoch 377/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8116 - val_loss: -1.3871\n",
      "Epoch 378/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: -1.8116 - val_loss: -1.3870\n",
      "Epoch 379/1000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: -1.8069 - val_loss: -1.3870\n",
      "Epoch 380/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8047 - val_loss: -1.3870\n",
      "Epoch 381/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.7944 - val_loss: -1.3871\n",
      "Epoch 382/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.8009 - val_loss: -1.3872\n",
      "Epoch 383/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8052 - val_loss: -1.3873\n",
      "Epoch 384/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8048 - val_loss: -1.3874\n",
      "Epoch 385/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.7946 - val_loss: -1.3875\n",
      "Epoch 386/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8144 - val_loss: -1.3876\n",
      "Epoch 387/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.8156 - val_loss: -1.3876\n",
      "Epoch 388/1000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: -1.7955 - val_loss: -1.3876\n",
      "Epoch 389/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8055 - val_loss: -1.3876\n",
      "Epoch 390/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: -1.8196 - val_loss: -1.3876\n",
      "Epoch 391/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8005 - val_loss: -1.3876\n",
      "Epoch 392/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: -1.8043 - val_loss: -1.3876\n",
      "Epoch 393/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8012 - val_loss: -1.3876\n",
      "Epoch 394/1000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: -1.8099 - val_loss: -1.3877\n",
      "Epoch 395/1000\n",
      "2/2 [==============================] - 0s 68ms/step - loss: -1.8094 - val_loss: -1.3877\n",
      "Epoch 396/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8090 - val_loss: -1.3877\n",
      "Epoch 397/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8110 - val_loss: -1.3876\n",
      "Epoch 398/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8052 - val_loss: -1.3877\n",
      "Epoch 399/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8131 - val_loss: -1.3876\n",
      "Epoch 400/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8080 - val_loss: -1.3876\n",
      "Epoch 401/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: -1.8010 - val_loss: -1.3876\n",
      "Epoch 402/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8086 - val_loss: -1.3875\n",
      "Epoch 403/1000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: -1.7853 - val_loss: -1.3875\n",
      "Epoch 404/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8029 - val_loss: -1.3876\n",
      "Epoch 405/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8086 - val_loss: -1.3878\n",
      "Epoch 406/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.7820 - val_loss: -1.3880\n",
      "Epoch 407/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.7925 - val_loss: -1.3882\n",
      "Epoch 408/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.7945 - val_loss: -1.3884\n",
      "Epoch 409/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8129 - val_loss: -1.3885\n",
      "Epoch 410/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8074 - val_loss: -1.3886\n",
      "Epoch 411/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.8183 - val_loss: -1.3887\n",
      "Epoch 412/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8033 - val_loss: -1.3887\n",
      "Epoch 413/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8096 - val_loss: -1.3888\n",
      "Epoch 414/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8034 - val_loss: -1.3889\n",
      "Epoch 415/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8126 - val_loss: -1.3889\n",
      "Epoch 416/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8105 - val_loss: -1.3890\n",
      "Epoch 417/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8097 - val_loss: -1.3890\n",
      "Epoch 418/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.7951 - val_loss: -1.3891\n",
      "Epoch 419/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8037 - val_loss: -1.3892\n",
      "Epoch 420/1000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: -1.8004 - val_loss: -1.3893\n",
      "Epoch 421/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8127 - val_loss: -1.3894\n",
      "Epoch 422/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8153 - val_loss: -1.3895\n",
      "Epoch 423/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8074 - val_loss: -1.3895\n",
      "Epoch 424/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8216 - val_loss: -1.3896\n",
      "Epoch 425/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8094 - val_loss: -1.3895\n",
      "Epoch 426/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8113 - val_loss: -1.3895\n",
      "Epoch 427/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: -1.8172 - val_loss: -1.3895\n",
      "Epoch 428/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8144 - val_loss: -1.3894\n",
      "Epoch 429/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.8083 - val_loss: -1.3894\n",
      "Epoch 430/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.7891 - val_loss: -1.3894\n",
      "Epoch 431/1000\n",
      "2/2 [==============================] - 0s 114ms/step - loss: -1.8041 - val_loss: -1.3894\n",
      "Epoch 432/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8042 - val_loss: -1.3894\n",
      "Epoch 433/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.8043 - val_loss: -1.3895\n",
      "Epoch 434/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8055 - val_loss: -1.3895\n",
      "Epoch 435/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8111 - val_loss: -1.3895\n",
      "Epoch 436/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: -1.8040 - val_loss: -1.3896\n",
      "Epoch 437/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7924 - val_loss: -1.3896\n",
      "Epoch 438/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8097 - val_loss: -1.3896\n",
      "Epoch 439/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8022 - val_loss: -1.3897\n",
      "Epoch 440/1000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: -1.8158 - val_loss: -1.3898\n",
      "Epoch 441/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8003 - val_loss: -1.3898\n",
      "Epoch 442/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8029 - val_loss: -1.3899\n",
      "Epoch 443/1000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: -1.8094 - val_loss: -1.3899\n",
      "Epoch 444/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: -1.8034 - val_loss: -1.3900\n",
      "Epoch 445/1000\n",
      "2/2 [==============================] - 0s 76ms/step - loss: -1.8104 - val_loss: -1.3901\n",
      "Epoch 446/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8151 - val_loss: -1.3901\n",
      "Epoch 447/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8197 - val_loss: -1.3901\n",
      "Epoch 448/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8119 - val_loss: -1.3901\n",
      "Epoch 449/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8090 - val_loss: -1.3901\n",
      "Epoch 450/1000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: -1.8138 - val_loss: -1.3901\n",
      "Epoch 451/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.8086 - val_loss: -1.3901\n",
      "Epoch 452/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8115 - val_loss: -1.3901\n",
      "Epoch 453/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.8150 - val_loss: -1.3901\n",
      "Epoch 454/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.8120 - val_loss: -1.3901\n",
      "Epoch 455/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8023 - val_loss: -1.3901\n",
      "Epoch 456/1000\n",
      "2/2 [==============================] - 0s 110ms/step - loss: -1.8170 - val_loss: -1.3900\n",
      "Epoch 457/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.8026 - val_loss: -1.3900\n",
      "Epoch 458/1000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: -1.7950 - val_loss: -1.3901\n",
      "Epoch 459/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8143 - val_loss: -1.3901\n",
      "Epoch 460/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.8147 - val_loss: -1.3902\n",
      "Epoch 461/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.8040 - val_loss: -1.3902\n",
      "Epoch 462/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.8041 - val_loss: -1.3903\n",
      "Epoch 463/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8134 - val_loss: -1.3904\n",
      "Epoch 464/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.8118 - val_loss: -1.3904\n",
      "Epoch 465/1000\n",
      "2/2 [==============================] - ETA: 0s - loss: -1.76 - 0s 109ms/step - loss: -1.7975 - val_loss: -1.3905\n",
      "Epoch 466/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8018 - val_loss: -1.3906\n",
      "Epoch 467/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: -1.8004 - val_loss: -1.3907\n",
      "Epoch 468/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8135 - val_loss: -1.3908\n",
      "Epoch 469/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: -1.8101 - val_loss: -1.3908\n",
      "Epoch 470/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8098 - val_loss: -1.3909\n",
      "Epoch 471/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8161 - val_loss: -1.3909\n",
      "Epoch 472/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8136 - val_loss: -1.3908\n",
      "Epoch 473/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.8163 - val_loss: -1.3907\n",
      "Epoch 474/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.7937 - val_loss: -1.3907\n",
      "Epoch 475/1000\n",
      "2/2 [==============================] - 0s 72ms/step - loss: -1.7986 - val_loss: -1.3907\n",
      "Epoch 476/1000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: -1.8150 - val_loss: -1.3907\n",
      "Epoch 477/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.8110 - val_loss: -1.3906\n",
      "Epoch 478/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.8210 - val_loss: -1.3905\n",
      "Epoch 479/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.8038 - val_loss: -1.3904\n",
      "Epoch 480/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.8064 - val_loss: -1.3903\n",
      "Epoch 481/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: -1.8052 - val_loss: -1.3902\n",
      "Epoch 482/1000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: -1.8236 - val_loss: -1.3901\n",
      "Epoch 483/1000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: -1.8159 - val_loss: -1.3900\n",
      "Epoch 484/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.8147 - val_loss: -1.3899\n",
      "Epoch 485/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.8001 - val_loss: -1.3898\n",
      "Epoch 486/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: -1.8118 - val_loss: -1.3897\n",
      "Epoch 487/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: -1.8128 - val_loss: -1.3896\n",
      "Epoch 488/1000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: -1.8081 - val_loss: -1.3896\n",
      "Epoch 489/1000\n",
      "2/2 [==============================] - 0s 78ms/step - loss: -1.8058 - val_loss: -1.3895\n",
      "Epoch 490/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.8058 - val_loss: -1.3894\n",
      "Epoch 491/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.7949 - val_loss: -1.3894\n",
      "Epoch 492/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8170 - val_loss: -1.3893\n",
      "Epoch 493/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8119 - val_loss: -1.3892\n",
      "Epoch 494/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8134 - val_loss: -1.3892\n",
      "Epoch 495/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.8137 - val_loss: -1.3891\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00495: early stopping\n",
      "X_train shape = (40, 1, 40, 5)\n",
      "y_train shape = (40, 1, 20, 5)\n",
      "X_test shape = (40, 1, 40, 5)\n",
      "y_test shape = (40, 1, 20, 5)\n",
      "X_pred shape = (1, 1, 40, 5)\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "softmax_11 (Softmax)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,005\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "2/2 [==============================] - 2s 546ms/step - loss: -1.2561 - val_loss: -0.4100\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.2234 - val_loss: -0.4101\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 102ms/step - loss: -1.1958 - val_loss: -0.4102\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.2204 - val_loss: -0.4103\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: -1.3088 - val_loss: -0.4104\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: -1.2594 - val_loss: -0.4105\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 79ms/step - loss: -1.3695 - val_loss: -0.4106\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: -1.2970 - val_loss: -0.4107\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: -1.2889 - val_loss: -0.4107\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: -1.3294 - val_loss: -0.4107\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.3456 - val_loss: -0.4107\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.3513 - val_loss: -0.4106\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 71ms/step - loss: -1.3650 - val_loss: -0.4106\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: -1.4009 - val_loss: -0.4106\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: -1.3403 - val_loss: -0.4106\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: -1.5126 - val_loss: -0.4105\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: -1.4571 - val_loss: -0.4105\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -1.4165 - val_loss: -0.4105\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: -1.4378 - val_loss: -0.4105\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.4352 - val_loss: -0.4104\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5783 - val_loss: -0.4104\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: -1.4299 - val_loss: -0.4104\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.5211 - val_loss: -0.4103\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.5228 - val_loss: -0.4102\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.5332 - val_loss: -0.4100\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 116ms/step - loss: -1.5219 - val_loss: -0.4099\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 115ms/step - loss: -1.5839 - val_loss: -0.4097\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.5682 - val_loss: -0.4095\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 103ms/step - loss: -1.6133 - val_loss: -0.4093\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.6737 - val_loss: -0.4091\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: -1.6970 - val_loss: -0.4090\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7408 - val_loss: -0.4089\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.7190 - val_loss: -0.4088\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: -1.7035 - val_loss: -0.4086\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.7963 - val_loss: -0.4085\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: -1.6858 - val_loss: -0.4083\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: -1.7293 - val_loss: -0.4081\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.7405 - val_loss: -0.4080\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 108ms/step - loss: -1.8178 - val_loss: -0.4078\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 138ms/step - loss: -1.7900 - val_loss: -0.4076\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7845 - val_loss: -0.4073\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: -1.8153 - val_loss: -0.4071\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: -1.7472 - val_loss: -0.4069\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: -1.8242 - val_loss: -0.4066\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 122ms/step - loss: -1.8549 - val_loss: -0.4064\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: -1.9137 - val_loss: -0.4062\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 73ms/step - loss: -1.9558 - val_loss: -0.4060\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: -1.9597 - val_loss: -0.4058\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 75ms/step - loss: -1.9607 - val_loss: -0.4056\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0251 - val_loss: -0.4054\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: -2.0749 - val_loss: -0.4052\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00051: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and predict\n",
    "1. Get the rolling window data from of T-250\n",
    "2. scale and split data to prepare for neural network input, Xtrain, Xtest, ytrain, ytest, Xpred\n",
    "3. Train the model with Xtrain,Xtest,ytrain,ytest\n",
    "4. Predict weight using Xpred\n",
    "5. Save weight for that T\n",
    "6. Repeat 1 - 5 for every T(rebal date)\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=50, verbose=2, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n",
    "N_EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# we will start the prediction from year 2020\n",
    "pred_dates = [d for d in REBAL_DATES if d.year == 2020]\n",
    "\n",
    "def get_window(df,pred_date):\n",
    "    temp = df.copy()\n",
    "    pred_date_ind = np.where(temp.index == pred_date)[0][0]\n",
    "    window = temp.iloc[((pred_date_ind+1)-WINDOW):(pred_date_ind+1)]\n",
    "    return window\n",
    "\n",
    "def create_input(df):\n",
    "    data = df.copy()\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    transformer = scaler.fit(data)\n",
    "    scaled_data = pd.DataFrame(transformer.transform(data), columns=data.columns)\n",
    "    test_data = scaled_data.iloc[:100]\n",
    "    train_data = scaled_data.iloc[100:]\n",
    "    \n",
    "    n_test_sample = 100 - LOOKBACK - HORIZON\n",
    "    n_train_sample = 100 - LOOKBACK - HORIZON\n",
    "    \n",
    "    assets = scaled_data.columns.levels[0].tolist()\n",
    "    channels = scaled_data.columns.levels[1].tolist()\n",
    "    \n",
    "    n_channels = len(data.columns.levels[1])\n",
    "    channel = scaled_data.columns.levels[1][0]\n",
    "    for index, asset in enumerate(assets):\n",
    "        if (index == 0):\n",
    "            test_temp = test_data[asset, channel].values.reshape(-1,1)\n",
    "            train_temp = train_data[asset, channel].values.reshape(-1,1)\n",
    "            full_temp = scaled_data[asset, channel].values.reshape(-1,1)\n",
    "        else:\n",
    "            test_temp = np.concatenate((test_temp, test_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            train_temp = np.concatenate((train_temp, train_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            full_temp = np.concatenate((full_temp, scaled_data[asset, channel].values.reshape(-1,1)), axis=1)\n",
    "            \n",
    "    data_block_test = test_temp.reshape(n_channels,test_temp.shape[0],test_temp.shape[1])\n",
    "    data_block_train = train_temp.reshape(n_channels,train_temp.shape[0],train_temp.shape[1])\n",
    "    data_block = full_temp.reshape(n_channels,full_temp.shape[0],full_temp.shape[1])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, X_pred = [], [], [], [], []\n",
    "    \n",
    "    for i in range(n_train_sample):\n",
    "        X_train.append(data_block_train[:,i:(i+LOOKBACK),:])\n",
    "        y_train.append(data_block_train[:,(i+LOOKBACK):(i + LOOKBACK + HORIZON),:])\n",
    "        \n",
    "    for i in range(n_train_sample):\n",
    "        X_test.append(data_block_test[:,i:(i+LOOKBACK),:])\n",
    "        y_test.append(data_block_test[:,(i+LOOKBACK):(i + LOOKBACK + HORIZON),:])\n",
    "        \n",
    "    X_pred.append(data_block_test[:,-LOOKBACK:,:])\n",
    "        \n",
    "    return np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test), np.array(X_pred)\n",
    "\n",
    "\"\"\"Here will start train and predict rolling window, getting weight for each rebalance date\"\"\"\n",
    "portfolio_weights = []\n",
    "for d in pred_dates:\n",
    "    data = get_window(df, d)\n",
    "    \n",
    "    # split 100 sample for train, 100 sample for test. Since we have 190 samples, some test data will overlap\n",
    "    X_train, y_train, X_test, y_test, X_pred = create_input(data)\n",
    "    print(f\"X_train shape = {X_train.shape}\")\n",
    "    print(f\"y_train shape = {y_train.shape}\")\n",
    "    print(f\"X_test shape = {X_test.shape}\")\n",
    "    print(f\"y_test shape = {y_test.shape}\")\n",
    "    print(f\"X_pred shape = {X_pred.shape}\")\n",
    "    \n",
    "    model = build_model(\n",
    "    n_assets=N_ASSETS,\n",
    "    input_shape = (N_FEATURES),\n",
    "    dropout=0.5\n",
    "    )\n",
    "\n",
    "    my_loss = make_my_loss([0.1, 0.2, 0.7])\n",
    "    model.compile(\n",
    "        loss= my_loss,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    X_train_in = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_in = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_in,\n",
    "        y_train,\n",
    "        validation_data=(X_test_in, y_test),\n",
    "        epochs=N_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[es]\n",
    "        )\n",
    "    \n",
    "    weights = model.predict(X_pred)\n",
    "    portfolio_weights.append(weights)\n",
    "\n",
    "weights_df = pd.DataFrame(weights, columns=assets, index=pred_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c59b9f-5e06-4374-b414-4b37bdc4c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), facecolor=background_color)\n",
    "stack_list = []\n",
    "for asset in assets:\n",
    "    stack_list.append(weights_df[asset])\n",
    "        \n",
    "plt.stackplot(weights_df.index, stack_list)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
